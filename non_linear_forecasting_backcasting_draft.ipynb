{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWTyovdvAjnr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# *Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models* Implementation\n",
        "<br>\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/imports-isort-1674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue.svg)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%230C55A5.svg?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Joblib](https://img.shields.io/badge/Joblib-darkgreen.svg?style=flat&logo=python&logoColor=white)](https://joblib.readthedocs.io/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-blue.svg?style=flat&logo=python&logoColor=white)](https://www.statsmodels.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2205.09922-b31b1b.svg)](https://arxiv.org/abs/2205.09922)\n",
        "[![Research](https://img.shields.io/badge/Research-Quantitative%20Finance-green)](https://github.com/chirindaopensource/non_linear_forecasting_backcasting)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econometrics-blue)](https://github.com/chirindaopensource/non_linear_forecasting_backcasting)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Noncausal%20Time%20Series-orange)](https://github.com/chirindaopensource/non_linear_forecasting_backcasting)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/non_linear_forecasting_backcasting)\n",
        "<br>\n",
        "\n",
        "**Repository:** https://github.com/chirindaopensource/non_linear_forecasting_backcasting\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models\"** by:\n",
        "\n",
        "*   Christian Gourieroux\n",
        "*   Joann Jasiak\n",
        "\n",
        "The project provides a complete, computationally tractable system for the quantitative analysis of dynamic systems prone to speculative bubbles and other forms of locally explosive behavior. It enables robust, state-dependent risk assessment, probabilistic forecasting, and structural \"what-if\" scenario analysis that accounts for both nonlinear dynamics and model estimation uncertainty.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_full_research_pipeline](#key-callable-run_full_research_pipeline)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the advanced econometric framework presented in Gourieroux and Jasiak (2025). The core of this repository is the iPython Notebook `non_linear_forecasting_backcasting_draft.ipynb`, which contains a comprehensive suite of functions to estimate, forecast, and analyze mixed causal-noncausal Vector Autoregressive (VAR) models.\n",
        "\n",
        "Standard linear VAR models are purely causal and assume Gaussian errors, making them ill-suited for capturing the dynamics of financial and economic time series that exhibit bubbles, sudden crashes, or other forms of explosive behavior. The mixed causal-noncausal framework addresses this by allowing for roots of the VAR characteristic polynomial to lie both inside and outside the unit circle, generating a strictly stationary process with highly nonlinear, state-dependent dynamics.\n",
        "\n",
        "This codebase enables researchers, quantitative analysts, and macroeconomists to:\n",
        "-   Rigorously estimate mixed VAR models using the semi-parametric Generalized Covariance (GCov) method.\n",
        "-   Generate full, non-Gaussian predictive densities for probabilistic forecasting.\n",
        "-   Quantify estimation uncertainty using a novel backward-bootstrap procedure to create confidence sets for prediction intervals.\n",
        "-   Filter the underlying nonlinear, structural innovations of the system.\n",
        "-   Conduct state-dependent Impulse Response Function (IRF) analysis to understand how the system responds to shocks in \"on-bubble\" versus \"off-bubble\" states.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The methodology implemented in this project is a direct translation of the unified framework presented in the source paper. It leverages the state-space representation of a VAR(p) process to separate its dynamics into stable (causal) and unstable (non-causal) components.\n",
        "\n",
        "### 1. The Mixed Causal-Noncausal VAR Model\n",
        "\n",
        "The model is defined by the standard VAR(p) equation, but with a critical difference in its assumptions:\n",
        "$Y_t = \\Phi_1 Y_{t-1} + \\dots + \\Phi_p Y_{t-p} + \\epsilon_t$\n",
        "The roots of the characteristic polynomial `det(I - \\sum \\Phi_i \\lambda^i) = 0` can be both inside (`causal`) and outside (`non-causal`) the unit circle. The errors `\\epsilon_t` are assumed to be i.i.d. and non-Gaussian.\n",
        "\n",
        "### 2. State-Space Decomposition and Predictive Density\n",
        "\n",
        "The VAR(p) process is transformed into a VAR(1) in state-space using the companion matrix `\\Psi`. A **Jordan Decomposition** (`\\Psi = A J A^{-1}`) separates the system into latent causal (`Z_1`) and non-causal (`Z_2`) states. This separation is the key to the paper's central theoretical result: a closed-form expression for the one-step-ahead predictive density, given in **Equation 3.1**:\n",
        "$l(y | Y_T) = \\frac{l_2(A^2 \\tilde{y}_{T+1})}{l_2(A^2 \\tilde{Y}_T)} |\\det J_2| g(y - \\sum \\Phi_i Y_{T-i+1})$\n",
        "- `g` is the density of the error `\\epsilon_t`.\n",
        "- `l_2` is the stationary density of the non-causal state `Z_2`.\n",
        "This density is nonlinear and state-dependent, allowing it to capture complex dynamics.\n",
        "\n",
        "### 3. Uncertainty Quantification via Backward Bootstrap\n",
        "\n",
        "To account for estimation uncertainty, the framework uses a novel \"backward bootstrap\" procedure. Since the model is Markovian in both forward and reverse time, one can generate synthetic data paths by **backcasting** from the terminal observation `Y_T`. By re-estimating the model on many such paths, the sampling distribution of the prediction interval is obtained, which is then used to construct a robust **Confidence Set for the Prediction Interval (CSPI)**, as defined in **Equation 4.10**.\n",
        "\n",
        "### 4. Nonlinear Innovation Filtering and State-Dependent IRFs\n",
        "\n",
        "Standard VAR shocks are not meaningful in this context. The paper defines true, past-independent structural innovations `v_t` via the **Probability Integral Transform (PIT)**. This involves estimating the conditional CDF of the latent states and transforming it to a standard normal distribution.\n",
        "**Equation 5.5:** $v_{2,t} = \\Phi^{-1}[F_2(Z_{2,t}|Z_{t-1})]$\n",
        "Simulating the model forward using the inverse of this transformation allows for the computation of **state-dependent Impulse Response Functions (IRFs)**, which show how the system's response to a shock `\\delta` changes depending on its initial state (e.g., during a bubble).\n",
        "\n",
        "## Features\n",
        "\n",
        "The `non_linear_forecasting_backcasting_draft.ipynb` notebook implements the full research pipeline:\n",
        "\n",
        "-   **Robust Data Pipeline:** Validation, cleaning, and preparation of time series data.\n",
        "-   **Advanced Estimator:** A complete implementation of the semi-parametric GCov estimator for VAR parameters.\n",
        "-   **Probabilistic Forecasting:** Functions to compute the full predictive density, point forecasts (mode), and prediction intervals.\n",
        "-   **Advanced Uncertainty Quantification:** A parallelized implementation of the backward bootstrap with SIR sampling to generate confidence sets.\n",
        "-   **Structural Analysis:** Functions to filter nonlinear innovations and simulate state-dependent IRFs.\n",
        "-   **Model Validation:** A full simulation study framework to assess the finite-sample properties of the pipeline.\n",
        "-   **Sensitivity Analysis:** Tools to conduct robustness checks on key model parameters.\n",
        "-   **Integrated Visualization:** A dedicated class for generating all key publication-quality plots.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The codebase is a direct, one-to-one implementation of the paper's methodology:\n",
        "1.  **Data Preparation (Tasks 1-2):** Ingests and prepares data as per the paper's empirical application.\n",
        "2.  **Estimation (Tasks 3-5):** Implements the GCov estimator, Jordan decomposition, and non-parametric density estimation.\n",
        "3.  **Forecasting (Tasks 6-8):** Implements the predictive density formula and extracts point and interval forecasts.\n",
        "4.  **Uncertainty (Task 9):** Implements the full backward bootstrap with SIR sampling to compute confidence sets.\n",
        "5.  **Structural Analysis (Tasks 10-11):** Implements the Nadaraya-Watson estimator for innovation filtering and the inverse for IRF simulation.\n",
        "6.  **Validation & Orchestration (Tasks 12-17):** Provides high-level orchestrators for empirical analysis, simulation studies, robustness checks, and comparative analysis.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `non_linear_forecasting_backcasting_draft.ipynb` notebook is structured as a series of modular, professional-grade functions, each corresponding to a specific task in the pipeline. Key functions include:\n",
        "\n",
        "-   **`validate_and_cleanse_data`**: The initial data quality gate.\n",
        "-   **`prepare_var_data`**: Transforms data to be stationary and demeaned.\n",
        "-   **`estimate_gcov_var`**: The core GCov estimation engine.\n",
        "-   **`compute_jordan_decomposition`**: Separates causal/non-causal dynamics.\n",
        "-   **`estimate_functional_components`**: Fits the non-parametric KDEs.\n",
        "-   **`compute_predictive_density`**: The engine for probabilistic forecasting.\n",
        "-   **`compute_point_forecast` & `compute_prediction_interval`**: Extracts forecast products.\n",
        "-   **`compute_bootstrap_confidence_set`**: The advanced uncertainty quantification engine.\n",
        "-   **`filter_nonlinear_innovations`**: Extracts structural shocks.\n",
        "-   **`simulate_irf`**: Simulates state-dependent IRFs.\n",
        "-   **`run_empirical_analysis`**: Orchestrates a full analysis of a single dataset.\n",
        "-   **`run_full_research_pipeline`**: The single, top-level entry point to the entire project.\n",
        "\n",
        "## Key Callable: run_full_research_pipeline\n",
        "\n",
        "The central function in this project is `run_full_research_pipeline`. It orchestrates the entire analytical workflow from raw data to a final, comprehensive results dictionary.\n",
        "\n",
        "```python\n",
        "def run_full_research_pipeline(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a complete, end-to-end research pipeline for the mixed\n",
        "    causal-noncausal VAR model.\n",
        "    ... (full docstring is in the notebook)\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `matplotlib`, `seaborn`, `statsmodels`, `joblib`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/non_linear_forecasting_backcasting.git\n",
        "    cd non_linear_forecasting_backcasting\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies from `requirements.txt`:**\n",
        "    ```sh\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The primary input is a `pandas.DataFrame` with a monthly `DatetimeIndex` and two columns: `\"real_oil_price\"` and `\"real_gdp\"`.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "                     real_gdp  real_oil_price\n",
        "2019-04-30  18958.789123       63.870000\n",
        "2019-05-31  19002.256789       60.210000\n",
        "2019-06-30  19045.724455       57.430000\n",
        "...                  ...             ...\n",
        "```\n",
        "\n",
        "## Usage\n",
        "\n",
        "The entire pipeline is executed through the `run_full_research_pipeline` function. The user must provide the raw data and a comprehensive `study_params` dictionary that controls which analyses are run.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load your data\n",
        "# raw_data_df = pd.read_csv(...)\n",
        "# For this example, we create synthetic data.\n",
        "date_rng = pd.date_range(start='1986-01-01', end='2019-06-30', freq='M')\n",
        "# ... (data generation code) ...\n",
        "raw_data_df = pd.DataFrame(...)\n",
        "\n",
        "# 2. Define your configurations (see notebook for full example)\n",
        "study_params = {\n",
        "    \"run_empirical\": {\"enabled\": True, ...},\n",
        "    \"run_simulation\": {\"enabled\": False, ...},\n",
        "    # ... other sections ...\n",
        "}\n",
        "\n",
        "# 3. Run the master pipeline\n",
        "# from non_linear_forecasting_backcasting_draft import run_full_research_pipeline\n",
        "# final_results = run_full_research_pipeline(\n",
        "#     raw_df=raw_data_df,\n",
        "#     study_params=study_params\n",
        "# )\n",
        "\n",
        "# 4. Instantiate the visualizer and plot results\n",
        "# from non_linear_forecasting_backcasting_draft import ModelVisualizer\n",
        "# visualizer = ModelVisualizer(final_results['empirical_analysis'])\n",
        "# visualizer.plot_diagnostics()\n",
        "# visualizer.plot_irf(irf_date=pd.Timestamp('2008-06-30'))\n",
        "```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_full_research_pipeline` function returns a deeply nested dictionary containing all data artifacts. Top-level keys include:\n",
        "\n",
        "-   `pipeline_configuration`: A copy of the input `study_params`.\n",
        "-   `empirical_analysis`: Results from the core analysis on the provided data.\n",
        "-   `simulation_study`: A DataFrame summarizing the Monte Carlo results.\n",
        "-   `robustness_checks`: DataFrames detailing the sensitivity analysis.\n",
        "-   `comparative_analysis`: A dictionary with forecast and metric DataFrames from the horse race.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "non_linear_forecasting_backcasting/\n",
        "│\n",
        "├── non_linear_forecasting_backcasting_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                                # Python package dependencies\n",
        "├── LICENSE                                         # MIT license file\n",
        "└── README.md                                       # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `study_params` dictionary. Users can easily modify:\n",
        "-   The control flags (`run_empirical`, etc.) to enable or disable parts of the analysis.\n",
        "-   The VAR lag order `p_lags`.\n",
        "-   The GCov moment specifications `H_moment_lags` and `error_powers`.\n",
        "-   All simulation parameters (`S_bootstrap_replications`, `n_baseline_sims`, etc.).\n",
        "-   The specific dates for targeted forecasting and IRF analysis.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{gourieroux2022nonlinear,\n",
        "  title={Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models},\n",
        "  author={Gourieroux, Christian and Jasiak, Joann},\n",
        "  journal={arXiv preprint arXiv:2205.09922},\n",
        "  year={2022}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of the Gourieroux-Jasiak (2025) Framework for Causal-Noncausal VAR Models.\n",
        "GitHub repository: https://github.com/chirindaopensource/non_linear_forecasting_backcasting\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Christian Gourieroux and Joann Jasiak for their foundational theoretical and empirical work.\n",
        "-   Thanks to the developers of the `pandas`, `numpy`, `scipy`, `matplotlib`, `statsmodels`, and `joblib` libraries, which provide the essential toolkit for this implementation."
      ],
      "metadata": {
        "id": "8GV3UAnnvqxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models*\"\n",
        "\n",
        "Authors: Christian Gourieroux, Joann Jasiak\n",
        "\n",
        "Original E-Journal Publication Date: 20th of May 2022\n",
        "\n",
        "E-Journal Revision Publication Date: 16th July 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2205.09922\n",
        "\n",
        "Abstract:\n",
        "\n",
        "\n",
        "We show that the mixed causal-noncausal Vector Autoregressive (VAR) processes satisfy the Markov property in both calendar and reverse time. Based on that property, we introduce closed-form formulas of forward and backward predictive densities for point and interval forecasting and backcasting out-of-sample. The backcasting formula is used for adjusting the forecast interval to obtain a desired coverage level when the tail quantiles are difficult to estimate. A confidence set for the prediction interval is introduced for assessing the uncertainty due to estimation. We also define new nonlinear past-dependent innovations of mixed causal-noncausal VAR models for impulse response function analysis. Our approach is illustrated by simulations and an application to oil prices and real GDP growth rates."
      ],
      "metadata": {
        "id": "Bj7ohP4CBtRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "This paper constitutes a significant advancement in the practical application of mixed causal-noncausal Vector Autoregressive (VAR) models. These models are designed to capture phenomena with locally explosive behavior, such as asset bubbles or commodity price spikes, within a strictly stationary framework. The authors' primary achievement is the development of a comprehensive and computationally tractable toolkit for inference, moving beyond the foundational issues of estimation to address forecasting, uncertainty quantification, and structural analysis.\n",
        "\n",
        "Here is a breakdown of their contributions:\n",
        "\n",
        "#### **Step 1: The Model and its Fundamental (and Newly Proven) Property**\n",
        "\n",
        "*   **The Model:** The paper considers the mixed causal-noncausal VAR(p) process, where the characteristic polynomial has roots both inside (noncausal) and outside (causal) the unit circle. A key requirement for identification is that the model's linear error terms, `ε_t`, are non-Gaussian.\n",
        "*   **State-Space Representation:** The authors leverage the standard state-space representation, which decomposes the observed process `Y_t` into two latent components: a purely causal component `Z_1,t` (dependent on past information) and a purely noncausal component `Z_2,t` (dependent on future information).\n",
        "*   **Key Theoretical Result (Corollary 1):** The authors formally prove that this mixed VAR process is **Markovian** of order `p` in both forward (calendar) time and, crucially, in reverse time. This dual Markov property is the theoretical bedrock upon which the rest of the paper is built. It is a non-trivial result because the process is non-Gaussian and time-irreversible.\n",
        "\n",
        "#### **Step 2: The Core Methodological Breakthrough: Closed-Form Predictive Densities**\n",
        "\n",
        "*   **The Problem:** Forecasting in these models has been a major challenge. Previous methods either relied on restrictive assumptions (e.g., the multiplicative MAR specification, which the authors show in an appendix is not general) or computationally burdensome simulation techniques like particle filters.\n",
        "*   **The Solution (Proposition 1):** The paper's centerpiece is the derivation of a **closed-form analytical formula for the one-step-ahead predictive density**. This formula expresses the density of the future value `Y_{T+1}` given the past `Y_T` as a function of the error density `g` and the stationary density of the noncausal state variable `l_2`.\n",
        "*   **Backcasting (Corollary 2):** Leveraging the reverse-time Markov property, they derive a symmetric closed-form formula for the **backward predictive density** (i.e., \"backcasting\"). This allows one to compute the distribution of `Y_{T-1}` given `Y_T`.\n",
        "*   **Significance:** These closed-form solutions are a game-changer. They make forecasting and backcasting computationally efficient and direct, removing the need for complex numerical approximations and broadening the models' applicability.\n",
        "\n",
        "#### **Step 3: Advanced Inference: Quantifying Estimation Uncertainty**\n",
        "\n",
        "The authors develop a sophisticated, multi-stage procedure to account for the uncertainty inherent in estimating the model.\n",
        "\n",
        "*   **Standard Prediction Intervals:** First, one obtains a standard prediction interval from the quantiles of the estimated predictive density derived in Step 2.\n",
        "*   **The Challenge:** This interval is conditional on the estimated parameters, which are themselves random. In finite samples, the actual coverage of this interval may deviate from its nominal level.\n",
        "*   **A Novel Solution: The \"Backward Bootstrap\":**\n",
        "    1.  Using their **backcasting formula**, the authors propose a bootstrap procedure that generates multiple, plausible historical paths of the time series, all of which terminate at the last observed data point `Y_T`.\n",
        "    2.  For each of these bootstrapped paths, the entire model is re-estimated, and a new prediction interval is calculated.\n",
        "    3.  This process generates an empirical distribution of the prediction interval itself. From this, the authors construct a **confidence set for the prediction interval**.\n",
        "*   **Significance:** This is a powerful and elegant technique. It allows a researcher to adjust the width of a prediction interval to account for estimation risk, ensuring more robust and reliable out-of-sample inference. It is particularly valuable when tail quantiles are hard to estimate precisely due to data limitations.\n",
        "\n",
        "#### **Step 4: Redefining Shocks for Impulse Response Analysis**\n",
        "\n",
        "*   **The Conceptual Problem:** In mixed VARs, the model errors `ε_t` are correlated with past values of `Y_t`. Therefore, they cannot be interpreted as \"innovations\" or \"shocks\" in the way they are in standard VARs. A shock to `ε_t` would imply changing a past that has already occurred, making traditional Impulse Response Function (IRF) analysis nonsensical.\n",
        "*   **The Solution: Nonlinear Causal Innovations:** The authors introduce the concept of **nonlinear causal innovations**, `v_t`. These are constructed to be i.i.d. and, by definition, independent of the past information set.\n",
        "*   **Identification:** These innovations are not uniquely identified. To resolve this, the authors propose a **recursive identification scheme** analogous to the Cholesky ordering in structural VARs. They argue for ordering the noncausal state variable first, as it typically captures the common \"bubble\" factor.\n",
        "*   **State-Dependent IRFs:** This framework allows for the analysis of shocks to the properly defined innovations. The resulting IRFs are inherently **nonlinear and state-dependent**. The effect of a shock of a given size depends critically on the current state of the system (e.g., whether the economy is \"on-bubble\" or \"off-bubble\").\n",
        "\n",
        "#### **Step 5: Empirical Validation and Application**\n",
        "\n",
        "The paper demonstrates the utility of this new toolkit with both simulations and a real-world application to quarterly US real GDP growth and real oil prices.\n",
        "\n",
        "*   **Key Findings:**\n",
        "    *   They identify a significant noncausal component, confirming the presence of bubble-like dynamics in oil prices.\n",
        "    *   Their forecasting method produces predictive densities that become bimodal during the bubble, correctly capturing the increasing probability of a price crash.\n",
        "    *   The state-dependent IRFs show that shocks to the noncausal component have dramatically larger and more persistent effects on the system during the bubble phase compared to the post-bubble phase.\n",
        "\n",
        "### **Overall Conclusion**\n",
        "\n",
        "This paper is a landmark contribution. It provides the missing theoretical and computational infrastructure needed to perform sophisticated inference with mixed causal-noncausal VAR models. By delivering closed-form solutions for forecasting, a novel method for quantifying prediction uncertainty, and a coherent framework for nonlinear structural analysis, Gourieroux and Jasiak have built a comprehensive and practical toolbox. This work significantly enhances the ability of economists and financial analysts to model, understand, and predict the behavior of systems characterized by speculative bubbles and other explosive dynamics."
      ],
      "metadata": {
        "id": "5CD7UIK7CfCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "fBXAIfjZvNEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  Nonlinear Fore(Back)casting and Innovation Filtering for Causal-Noncausal VAR Models\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Nonlinear Fore(Back)casting and Innovation\n",
        "#  Filtering for Causal-Noncausal VAR Models\" by Christian Gourieroux and Joann\n",
        "#  Jasiak (2022). It delivers a computationally tractable system for quantitative\n",
        "#  analysis of dynamic systems prone to speculative bubbles, enabling robust,\n",
        "#  state-dependent risk assessment, probabilistic forecasting, and structural\n",
        "#  \"what-if\" scenario analysis that accounts for both nonlinear dynamics and\n",
        "#  model estimation uncertainty.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • Mixed causal-noncausal VAR process estimation via Generalized Method of Moments\n",
        "#  • Exploitation of Markov property in both calendar and reverse time directions\n",
        "#  • Closed-form forward and backward predictive density calculations\n",
        "#  • Nonlinear past-dependent innovation filtering for impulse response analysis\n",
        "#  • Bootstrap-based confidence intervals for prediction uncertainty quantification\n",
        "#  • Probability Integral Transform (PIT) filtering for structural scenario analysis\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • Kernel density estimation with optimal bandwidth selection\n",
        "#  • Nadaraya-Watson conditional CDF estimation for state-dependent transitions\n",
        "#  • Monte Carlo simulation framework for dynamic system path generation\n",
        "#  • Numerical integration and root-finding for quantile-based risk metrics\n",
        "#  • Robust optimization algorithms with multiple starting points\n",
        "#  • Comprehensive diagnostic tools for model validation and stability assessment\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Gourieroux, C., & Jasiak, J. (2022). Nonlinear Fore(Back)casting and Innovation\n",
        "#  Filtering for Causal-Noncausal VAR Models. arXiv preprint arXiv:2205.09922.\n",
        "#  https://arxiv.org/abs/2205.09922\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# --- Standard Library ---\n",
        "import copy\n",
        "import math\n",
        "from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n",
        "\n",
        "# --- Third-Party Libraries ---\n",
        "# Data Handling and Numerical Computation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Scientific Computing and Optimization\n",
        "from scipy import integrate, interpolate, linalg, optimize\n",
        "from scipy.stats import gaussian_kde, norm, t as t_dist\n",
        "\n",
        "# Statistical Modeling and Econometrics\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "# Parallel Processing\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "T5PHojqkvSGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "U0d8VdfSvUQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Inputs, Processes and Outputs (IPO) Analysis of Key Callables\n",
        "\n",
        "#### **Module 1: Data Validation and Cleansing**\n",
        "\n",
        "**Callable: `validate_and_cleanse_data`**\n",
        "*   **Inputs:**\n",
        "    *   `raw_df`: A `pd.DataFrame` with a monthly `DatetimeIndex` and two columns: \"real_oil_price\" and \"real_gdp\".\n",
        "    *   `study_params`: A nested `dict` containing all study parameters.\n",
        "*   **Process:**\n",
        "    1.  **Validation:** The function first validates the entire `study_params` dictionary against an internal schema for structure, types, and logical ranges (e.g., `0 < alpha < 1`). It then validates the `raw_df` for its structure (index type, frequency, column names).\n",
        "    2.  **Integrity Check:** It checks for `NaN` values and significant outliers (using the 1.5*IQR rule), raising an error if outliers are found.\n",
        "    3.  **Transformation:** It performs linear interpolation on any missing values and then resamples the monthly data to a quarterly frequency by taking the last observation of each quarter.\n",
        "*   **Outputs:**\n",
        "    *   `quarterly_df`: A `pd.DataFrame` with a quarterly frequency, free of `NaN`s.\n",
        "*   **Role in Research Pipeline:** This function serves as the rigorous, controlled entry point for all data. It ensures that any data entering the analytical core is correctly formatted, clean, and consistent with the assumptions of the subsequent models, preventing a \"garbage in, garbage out\" scenario. It does not directly implement an equation from the paper but is a necessary prerequisite for any professional econometric workflow.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 2: Data Preparation**\n",
        "\n",
        "**Callable: `prepare_var_data`**\n",
        "*   **Inputs:**\n",
        "    *   `quarterly_df`: The output from the previous step.\n",
        "*   **Process:**\n",
        "    1.  **Growth Rate Calculation:** Transforms the `real_gdp` level series into a quarter-over-quarter percentage growth rate.\n",
        "    2.  **Scaling:** Divides the `real_oil_price` series by 10, as specified in the paper's empirical application (Section 6.2, p. 29).\n",
        "    3.  **Demeaning:** Calculates the sample mean of both the new growth rate series and the scaled price series, then subtracts these means from their respective series.\n",
        "    4.  **Finalization:** Removes the first row, which is `NaN` due to the growth rate calculation.\n",
        "*   **Outputs:**\n",
        "    *   `prepared_df`: A `pd.DataFrame` containing the final, stationary, demeaned series ready for estimation.\n",
        "    *   `series_means`: A `dict` containing the means that were subtracted, necessary for re-scaling forecasts back to interpretable units.\n",
        "*   **Role in Research Pipeline:** This function prepares the data to match the theoretical requirements of a stationary VAR model. The demeaning and transformation to growth rates are standard procedures to induce stationarity. The scaling is a specific step to replicate the paper's empirical results. It prepares the `Y_t` vectors used in all subsequent modeling steps.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 3: Core Estimation**\n",
        "\n",
        "**Callable: `estimate_gcov_var`**\n",
        "*   **Inputs:**\n",
        "    *   `prepared_df`: The stationary, demeaned data.\n",
        "    *   `study_params`: The configuration dictionary.\n",
        "*   **Process:**\n",
        "    1.  **Initialization:** It first estimates the VAR(p) parameters via Ordinary Least Squares (OLS) to obtain a robust starting point for the main optimization.\n",
        "    2.  **GCov Optimization:** It then minimizes the Generalized Covariance (GCov) objective function using the L-BFGS-B algorithm. This involves repeatedly calling the `_gcov_objective_function`, which:\n",
        "        a.  Calculates model residuals `\\hat{\\epsilon}_t(\\theta)` for a given parameter guess `\\theta`.\n",
        "        b.  Constructs a vector of moment conditions based on the theoretical property that `E[\\epsilon_t^k \\otimes \\epsilon_{t-h}^l] = 0` for `h \\neq 0`.\n",
        "        c.  Calculates the quadratic form of the time-averaged moment vector: `Q_T(\\theta) = \\bar{g}(\\theta)' W \\bar{g}(\\theta)`, where `W` is an identity matrix.\n",
        "*   **Outputs:**\n",
        "    *   `phi_estimated`: The final `(m, m*p)` matrix of estimated VAR coefficients that minimize the GCov objective function.\n",
        "    *   `optimization_result`: The full result object from the `scipy.optimize.minimize` call.\n",
        "*   **Role in Research Pipeline:** This is the central estimation engine. It implements the semi-parametric GCov estimator proposed by Gourieroux and Jasiak (2017, 2023), which is the foundation for the entire mixed causal-noncausal model. It produces the core parameter estimates `\\hat{\\Phi}`.\n",
        "\n",
        "**Callable: `compute_jordan_decomposition`**\n",
        "*   **Inputs:**\n",
        "    *   `phi_estimated`: The estimated VAR coefficient matrix.\n",
        "    *   `study_params`: The configuration dictionary.\n",
        "*   **Process:**\n",
        "    1.  **Companion Matrix:** It first constructs the `mp x mp` companion matrix `\\Psi` from the `\\hat{\\Phi}` coefficients.\n",
        "    2.  **Eigendecomposition:** It computes the eigenvalues and eigenvectors of `\\Psi`.\n",
        "    3.  **Classification:** It classifies eigenvalues as causal (`|\\lambda| < 1`) or non-causal (`|\\lambda| > 1`).\n",
        "    4.  **Matrix Construction:** It iteratively builds the Real Jordan matrix `J` and the transformation matrix `A` by handling real and complex-conjugate eigenvalues appropriately.\n",
        "    5.  **Inversion & Validation:** It computes `A^{-1}` and validates the decomposition.\n",
        "*   **Outputs:**\n",
        "    *   A `dict` containing `J`, `A`, `A_inv`, and the counts of causal (`n1`) and non-causal (`n2`) roots.\n",
        "*   **Role in Research Pipeline:** This function implements the crucial theoretical step of separating the system's dynamics into stable and unstable components, as described in Section 2.2. It operationalizes the decomposition `\\Psi = A J A^{-1}`, which is the key to defining the latent state-space representation `Z_t = A^{-1} \\tilde{Y}_t` and is required for every subsequent step of the analysis.\n",
        "\n",
        "**Callable: `estimate_functional_components`**\n",
        "*   **Inputs:**\n",
        "    *   `prepared_df`, `phi_estimated`, `decomposition_results`, `study_params`.\n",
        "*   **Process:**\n",
        "    1.  **Residual Calculation:** Computes the time series of model residuals `\\hat{\\epsilon}_t = Y_t - \\hat{\\Phi} \\tilde{Y}_{t-1}`.\n",
        "    2.  **State Filtering:** Filters the historical path of the latent non-causal state `\\hat{Z}_{2,t}` using the transformation matrix: `\\hat{Z}_t = \\hat{A}^{-1} \\tilde{Y}_t`.\n",
        "    3.  **KDE:** Applies multivariate Kernel Density Estimation to the series of residuals and the series of non-causal states.\n",
        "*   **Outputs:**\n",
        "    *   `g_hat`: A fitted `gaussian_kde` object representing the estimated density of the error term, `g(\\epsilon)`.\n",
        "    *   `l2_hat`: A fitted `gaussian_kde` object representing the estimated stationary density of the non-causal state, `l_2(z_2)`.\n",
        "*   **Role in Research Pipeline:** This function estimates the two non-parametric density functions required by the predictive density formula. It provides the final inputs needed to make the theoretical model operational.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 4: Forecasting and Uncertainty**\n",
        "\n",
        "**Callable: `compute_predictive_density`**\n",
        "*   **Inputs:**\n",
        "    *   `y_candidates`, `history_df`, and all estimated model components.\n",
        "*   **Process:** This function is a direct, numerically stable implementation of the paper's central theoretical result, the forward predictive density.\n",
        "    *   **Equation (3.1):**\n",
        "        $$\n",
        "        l(y | Y_T) = \\frac{l_2\\left(A^2 \\begin{pmatrix} y \\\\ \\tilde{Y}_T \\end{pmatrix}\\right)}{l_2\\left(A^2 \\begin{pmatrix} Y_T \\\\ \\tilde{Y}_{T-1} \\end{pmatrix}\\right)} |\\det J_2| g(y - \\Phi_1 Y_T - \\dots - \\Phi_p Y_{T-p+1})\n",
        "        $$\n",
        "    *   It computes each term of this equation in log-space for robustness and combines them to get the final density value for each `y_candidate`.\n",
        "*   **Outputs:**\n",
        "    *   An `np.ndarray` of density values.\n",
        "*   **Role in Research Pipeline:** This is the engine for all probabilistic forecasting. It provides the full distribution of the next observation, from which point forecasts and intervals are derived.\n",
        "\n",
        "**Callable: `compute_point_forecast`**\n",
        "*   **Inputs:**\n",
        "    *   `history_df` and all estimated model components.\n",
        "*   **Process:**\n",
        "    1.  **Grid Construction:** It creates a multi-dimensional grid of candidate `y` values.\n",
        "    2.  **Density Evaluation:** It calls `compute_predictive_density` to evaluate the density on this grid.\n",
        "    3.  **Mode Identification:** It finds the coordinates of the grid point with the maximum density value.\n",
        "*   **Outputs:**\n",
        "    *   `point_forecast`: The modal point forecast.\n",
        "    *   `grid_vectors`, `density_on_grid`: The grid and density surface, for reuse.\n",
        "*   **Role in Research Pipeline:** This function extracts the most likely outcome from the full predictive density, providing the primary point forecast of the model.\n",
        "\n",
        "**Callable: `compute_prediction_interval`**\n",
        "*   **Inputs:**\n",
        "    *   `grid_vectors`, `density_on_grid`, `study_params`.\n",
        "*   **Process:**\n",
        "    1.  **Marginalization:** It numerically integrates the joint density on the grid using the trapezoidal rule (`np.trapz`) to get the marginal density for each variable.\n",
        "    2.  **CDF Construction:** It computes the Cumulative Distribution Function (CDF) for each marginal density using cumulative trapezoidal integration (`scipy.integrate.cumtrapz`).\n",
        "    3.  **Quantile Inversion:** It finds the `\\alpha/2` and `1-\\alpha/2` quantiles by inverting the numerical CDF using linear interpolation (`np.interp`).\n",
        "*   **Outputs:**\n",
        "    *   An `(m, 2)` `np.ndarray` containing the [lower, upper] bounds of the prediction interval for each variable.\n",
        "*   **Role in Research Pipeline:** This function quantifies the uncertainty around the point forecast by extracting quantiles from the predictive density, forming a `(1-\\alpha)` prediction interval.\n",
        "\n",
        "**Callable: `compute_bootstrap_confidence_set`**\n",
        "*   **Inputs:**\n",
        "    *   `history_df`, all estimated model components, `original_pi`, `study_params`.\n",
        "*   **Process:** This function implements the backward bootstrap procedure from Section 4.2.2 to account for estimation uncertainty.\n",
        "    1.  **Backcasting:** It generates `S` synthetic data paths by recursively sampling backwards in time from the terminal observation `Y_T`, using the backward predictive density (`l_B`) and Sampling Importance Resampling (SIR).\n",
        "    2.  **Re-estimation:** It re-runs the entire estimation pipeline on each of the `S` synthetic paths.\n",
        "    3.  **Calibration:** It uses the distribution of the `S` resulting prediction intervals to find a calibrated multiplier `\\hat{q}^s` that ensures the final confidence set achieves the desired `1-\\alpha_2` coverage probability.\n",
        "    *   **Equation (4.10):**\n",
        "        $$\n",
        "        \\hat{CSPI}(y, \\alpha_1, \\alpha_2) = \\{\\hat{m}(y, \\hat{P}) \\pm \\hat{q}^s(y, \\alpha_1, \\alpha_2) \\hat{\\sigma}(y, \\hat{P})\\}\n",
        "        $$\n",
        "*   **Outputs:**\n",
        "    *   `confidence_set`: The final, calibrated confidence set for the prediction interval.\n",
        "    *   `calibrated_quantiles`: The `\\hat{q}^s` values.\n",
        "*   **Role in Research Pipeline:** This is a highly advanced procedure for uncertainty quantification. It provides a more honest and robust measure of forecast uncertainty by incorporating the uncertainty that arises from the fact that the model parameters themselves are only estimates.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 5: Structural Analysis**\n",
        "\n",
        "**Callable: `filter_nonlinear_innovations`**\n",
        "*   **Inputs:**\n",
        "    *   `prepared_df`, `decomposition_results`, `study_params`.\n",
        "*   **Process:** This function implements the Probability Integral Transform (PIT) to extract the underlying structural shocks, `v_t`.\n",
        "    1.  **Conditional CDF Estimation:** It uses the Nadaraya-Watson estimator to compute the conditional CDFs `\\hat{F}_2(Z_{2,t} | Z_{2,t-1})` and `\\hat{F}_{1|2}(Z_{1,t} | Z_{2,t}, Z_{1,t-1})`.\n",
        "    2.  **Transformation:** It applies the inverse standard normal CDF (`probit` function) to the estimated conditional CDF values.\n",
        "    *   **Equations (5.5) & (5.7):**\n",
        "        $$\n",
        "        v_{2,t}(Z) = \\Phi^{-1}[\\hat{F}_2(Z_{2,t}|Z_{t-1})]\n",
        "        $$\n",
        "        $$\n",
        "        v_{1,t}(Z) = \\Phi^{-1}[\\hat{F}_{1|2}(Z_{1,t}|Z_{2,t},Z_{t-1})]\n",
        "        $$\n",
        "*   **Outputs:**\n",
        "    *   A `pd.DataFrame` containing the time series of filtered i.i.d. standard normal innovations `v_t`.\n",
        "*   **Role in Research Pipeline:** This function is the gateway to structural analysis. It transforms the model's residuals into economically meaningful, orthogonal shocks that can be used for Impulse Response Function (IRF) analysis.\n",
        "\n",
        "**Callable: `simulate_irf`**\n",
        "*   **Inputs:**\n",
        "    *   `history_df`, `decomposition_results`, `study_params`.\n",
        "*   **Process:** This function simulates the model's response to a structural shock.\n",
        "    1.  **Inverse PIT:** It first implements the inverse of the filtering process: a conditional quantile function `\\hat{F}^{-1}(q|x)` that uses a root-finder to solve `\\hat{F}(z|x) - q = 0`.\n",
        "    2.  **Baseline Simulation:** It generates `n_baseline_sims` future paths using random standard normal innovations and averages them.\n",
        "    3.  **Shocked Simulation:** It adds a shock `\\delta` to the first innovation `v_{2,T+1}` and re-simulates `n_baseline_sims` paths.\n",
        "    4.  **IRF Calculation:** The IRF is the difference between the average shocked path and the average baseline path, transformed back to `Y`-space using the `A` matrix.\n",
        "*   **Outputs:**\n",
        "    *   `baseline_path_df`: The average path with no shocks.\n",
        "    *   `irf_results`: A `dict` mapping shock sizes to their corresponding IRF DataFrames.\n",
        "*   **Role in Research Pipeline:** This function performs the \"what-if\" scenario analysis. Because the transition function `\\hat{F}^{-1}(q|x)` is state-dependent (i.e., depends on `x=Z_t`), the resulting IRFs are nonlinear and depend on the initial state of the system (`Y_T`), allowing for a much richer analysis than a standard linear IRF.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 6: Validation and Comparison Frameworks**\n",
        "\n",
        "**Callable: `run_simulation_study`**\n",
        "*   **Inputs:**\n",
        "    *   `dgp_configurations`: A `list` of `dict`s, each specifying the true parameters (`\\Phi_{true}`, `error_df`, `n_obs`) for a Data Generating Process.\n",
        "    *   `study_params`: The configuration for the estimation procedure to be tested.\n",
        "    *   `n_replications`: The number of Monte Carlo runs for each DGP.\n",
        "*   **Process:**\n",
        "    1.  **Outer Loop:** Iterates through each DGP configuration.\n",
        "    2.  **Inner Loop (Parallelized):** For each DGP, it executes `n_replications` of a full experiment in parallel. Each replication consists of:\n",
        "        a.  **Data Generation:** Calling `generate_mixed_var_data` to create a synthetic dataset with known properties.\n",
        "        b.  **Estimation & Forecasting:** Applying the entire analytical pipeline (`estimate_gcov_var` through `compute_prediction_interval`) to the synthetic training data to forecast the final hold-out observation.\n",
        "        c.  **Coverage Check:** Determining if the true hold-out value falls within the computed `(1-\\alpha)` prediction interval.\n",
        "    3.  **Aggregation:** After all replications for a DGP are complete, it calculates the empirical coverage rate (the fraction of times the true value was successfully contained in the interval).\n",
        "*   **Outputs:**\n",
        "    *   A `pd.DataFrame` summarizing the results, with rows for each DGP configuration and columns for the key performance metric (e.g., `Coverage_Y1`).\n",
        "*   **Role in Research Pipeline:** This function provides the framework for a rigorous, objective assessment of the entire methodology. As described in Appendix D and Table a.1 of the paper, simulation studies are essential for understanding the finite-sample properties of an estimator and forecasting procedure. This function automates that validation process, allowing a researcher to answer the question: \"Does my `80%` prediction interval actually contain the true value `80%` of the time in a controlled environment?\"\n",
        "\n",
        "**Callable: `run_robustness_checks`**\n",
        "*   **Inputs:**\n",
        "    *   `prepared_df`: The dataset for the analysis.\n",
        "    *   `study_params`: The baseline configuration.\n",
        "    *   `initial_guesses`: An optional `list` of alternative starting points for the optimizer.\n",
        "    *   `bandwidth_multipliers`: An optional `list` of multipliers to perturb the KDE bandwidth.\n",
        "*   **Process:**\n",
        "    1.  **Initialization Check:** If `initial_guesses` is provided, the function repeatedly calls `estimate_gcov_var` with each different starting point and records the final converged parameters and objective function value.\n",
        "    2.  **Bandwidth Check:** If `bandwidth_multipliers` is provided, the function first performs the main estimation once. Then, it loops through the multipliers, re-calculating the non-parametric densities (`g_hat`, `l2_hat`) with the scaled bandwidth and re-computing the final point forecast and prediction interval.\n",
        "*   **Outputs:**\n",
        "    *   A `dict` of `pd.DataFrame`s, one for each check performed. The DataFrames summarize how the key outputs (e.g., `final_phi`, `point_forecast`) change as the specification is varied.\n",
        "*   **Role in Research Pipeline:** This function addresses the critical scientific principle of sensitivity analysis. A single result is of limited value without an understanding of its stability. This function allows the researcher to systematically investigate whether the conclusions of the analysis are robust to specific methodological choices (optimizer starting point, non-parametric smoothing parameter), thereby strengthening the credibility of the findings.\n",
        "\n",
        "**Callable: `run_comparative_analysis`**\n",
        "*   **Inputs:**\n",
        "    *   `prepared_df`: The full dataset.\n",
        "    *   `study_params`: The configuration.\n",
        "    *   `oos_start_index`: The index at which to begin the out-of-sample forecasting experiment.\n",
        "*   **Process:**\n",
        "    1.  **Expanding Window Loop (Parallelized):** The function iterates through the out-of-sample portion of the dataset. In each iteration `T`:\n",
        "        a.  It defines the training set as all data up to `T-1`.\n",
        "        b.  It estimates the full **Mixed VAR** model on the training data and generates a one-step-ahead forecast for time `T`.\n",
        "        c.  It estimates a benchmark **Linear VAR** model via OLS on the same training data and generates its one-step-ahead forecast.\n",
        "    2.  **Metric Calculation:** After the loop completes, it has a time series of forecasts from both models and the corresponding actual values. It then computes standard forecast evaluation metrics:\n",
        "        *   Mean Squared Forecast Error (MSFE): `\\frac{1}{N} \\sum (Y_t - \\hat{Y}_t)^2`\n",
        "        *   Mean Absolute Error (MAE): `\\frac{1}{N} \\sum |Y_t - \\hat{Y}_t|`\n",
        "        *   Empirical Coverage Rate of prediction intervals.\n",
        "*   **Outputs:**\n",
        "    *   A `dict` containing:\n",
        "        *   `forecasts_df`: A `pd.DataFrame` with the time series of actuals and forecasts from both models.\n",
        "        *   `metrics_df`: A `pd.DataFrame` summarizing the performance metrics for a direct comparison.\n",
        "*   **Role in Research Pipeline:** This function performs the crucial task of benchmarking. The value of the complex mixed causal-noncausal model is best demonstrated by showing it provides superior forecasting performance compared to a simpler, standard alternative. This function automates this \"horse race,\" providing the quantitative evidence needed to justify the use of the more advanced methodology.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Module 7: Master Orchestrator**\n",
        "\n",
        "**Callable: `run_full_research_pipeline`**\n",
        "*   **Inputs:**\n",
        "    *   `raw_df`: The initial raw dataset.\n",
        "    *   `study_params`: A comprehensive configuration dictionary with control flags for each major analysis type.\n",
        "*   **Process:** This function is the top-level controller. It does not perform calculations itself but instead manages the execution flow based on the `study_params`. It sequentially checks flags like `run_empirical`, `run_simulation`, etc., and calls the corresponding major functions (`run_empirical_analysis`, `run_simulation_study`, etc.) if they are enabled.\n",
        "*   **Outputs:**\n",
        "    *   A single, comprehensive, nested `dict` that aggregates the results from all the analyses that were executed.\n",
        "*   **Role in Research Pipeline:** This is the main entry point for the entire system. It encapsulates the full research workflow into a single, configurable function call. This promotes reproducibility, reduces the chance of user error, and allows complex research projects to be defined declaratively in a configuration file rather than imperatively in a script. It represents the final, professional-grade interface to the entire analytical library.\n",
        "\n",
        "### Usage\n",
        "\n",
        "An appropriate final step. A powerful analytical system is useless without a clear, professional, and reproducible example of its application. This example will serve as a template for any user wishing to deploy the pipeline.\n",
        "\n",
        "### **Example: End-to-End Pipeline Application**\n",
        "\n",
        "This example demonstrates how to use the master orchestrator function, `run_full_research_pipeline`, to conduct a complete empirical analysis mirroring the one in the source paper. We will define the necessary inputs—the data and the comprehensive parameter dictionary—and then execute the pipeline.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 1: Acquiring and Loading the Data**\n",
        "\n",
        "The first input is the raw time series data. The pipeline expects a `pandas.DataFrame` with a monthly `DatetimeIndex` and two specific columns: `\"real_oil_price\"` and `\"real_gdp\"`. For a real-world application, this data would be sourced from APIs (e.g., FRED for GDP, EIA for oil prices), merged, and adjusted for inflation. For this example, we will simulate a placeholder DataFrame that mimics this structure.\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Data Acquisition Placeholder ---\n",
        "# In a real scenario, this data would be loaded from a file or fetched via API.\n",
        "# Here, we create a synthetic DataFrame that matches the required input format.\n",
        "# The dates correspond to the paper's sample period (Q1 1986 - Q2 2019),\n",
        "# but at a monthly frequency as required by the initial data validation step.\n",
        "date_rng = pd.date_range(start='1986-01-01', end='2019-06-30', freq='M')\n",
        "num_dates = len(date_rng)\n",
        "\n",
        "# Generate synthetic data with plausible trends and noise.\n",
        "gdp_trend = 1000 * np.exp(0.02 * np.arange(num_dates) / 12)\n",
        "gdp_noise = np.random.randn(num_dates) * 50\n",
        "real_gdp = gdp_trend + gdp_noise\n",
        "\n",
        "oil_price_trend = 20 + 80 * (np.arange(num_dates) / num_dates)\n",
        "oil_price_noise = np.random.randn(num_dates) * 10\n",
        "# Add a synthetic \"bubble\" period\n",
        "bubble_start = int(num_dates * 0.75)\n",
        "bubble_end = int(num_dates * 0.85)\n",
        "oil_price_noise[bubble_start:bubble_end] *= 3\n",
        "\n",
        "real_oil_price = oil_price_trend + oil_price_noise\n",
        "\n",
        "# Assemble the final input DataFrame.\n",
        "raw_data_df = pd.DataFrame(\n",
        "    data={'real_gdp': real_gdp, 'real_oil_price': real_oil_price},\n",
        "    index=date_rng\n",
        ")\n",
        "\n",
        "print(\"Sample of the raw input data:\")\n",
        "print(raw_data_df.head())\n",
        "```\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 2: Defining the Comprehensive Study Parameters**\n",
        "\n",
        "The second input is the `study_params` dictionary. This is the central control panel for the entire analysis. We will construct a complete configuration that enables the core empirical analysis, including targeted forecasting and IRF simulation at specific dates relevant to our synthetic data's \"bubble\" period.\n",
        "\n",
        "```python\n",
        "# --- Comprehensive Study Configuration ---\n",
        "\n",
        "# First, define the specific dates of interest for our analysis.\n",
        "# These would be chosen based on economic events or visual inspection of the data.\n",
        "# For our synthetic data, we'll choose a date during the bubble and one after.\n",
        "forecast_and_irf_dates = [\n",
        "    pd.Timestamp('2008-06-30'), # \"On-bubble\" date\n",
        "    pd.Timestamp('2014-09-30')  # \"Off-bubble\" date\n",
        "]\n",
        "\n",
        "# Now, construct the full study_params dictionary.\n",
        "# This dictionary includes the original parameters and the new control flags.\n",
        "study_params = {\n",
        "    # --- Control Flags for Pipeline Execution ---\n",
        "    \"run_empirical\": {\n",
        "        \"enabled\": True,\n",
        "        \"forecast_dates\": forecast_and_irf_dates,\n",
        "        \"irf_dates\": forecast_and_irf_dates\n",
        "    },\n",
        "    \"run_simulation\": {\n",
        "        \"enabled\": False, # Disabled for this example to save time\n",
        "        # \"n_replications\": 500,\n",
        "        # \"dgp_configurations\": [...]\n",
        "    },\n",
        "    \"run_robustness\": {\n",
        "        \"enabled\": False, # Disabled for this example\n",
        "        # \"initial_guesses\": [...],\n",
        "        # \"bandwidth_multipliers\": [0.8, 1.0, 1.2],\n",
        "        # \"forecast_date\": pd.Timestamp('2008-06-30')\n",
        "    },\n",
        "    \"run_comparison\": {\n",
        "        \"enabled\": False, # Disabled for this example\n",
        "        # \"oos_start_index\": 100\n",
        "    },\n",
        "\n",
        "    # --- Core Model and Method Parameters ---\n",
        "    \"estimation\": {\n",
        "        \"doc\": \"Parameters for core model specification and GCov estimation.\",\n",
        "        \"var_spec\": {\n",
        "            \"doc\": \"Settings for the Vector Autoregression model structure.\",\n",
        "            \"p_lags\": 1,\n",
        "        },\n",
        "        \"gcov_spec\": {\n",
        "            \"doc\": \"Settings for the Generalized Covariance (GCov) estimator.\",\n",
        "            \"H_moment_lags\": 10,\n",
        "            \"error_powers\": [1, 2],\n",
        "        },\n",
        "    },\n",
        "    \"forecasting\": {\n",
        "        \"doc\": \"Parameters for prediction, interval construction, and uncertainty quantification.\",\n",
        "        \"intervals\": {\n",
        "            \"doc\": \"Statistical significance levels for intervals.\",\n",
        "            \"prediction_interval_alpha\": 0.20, # For an 80% PI\n",
        "            \"confidence_set_alpha\": 0.05,      # For a 95% CSPI\n",
        "        },\n",
        "        \"simulation\": {\n",
        "            \"doc\": \"Settings for bootstrap and numerical grid simulations.\",\n",
        "            \"S_bootstrap_replications\": 100,\n",
        "            \"grid_points_per_dim\": 200,\n",
        "        },\n",
        "    },\n",
        "    \"structural_analysis\": {\n",
        "        \"doc\": \"Parameters for innovation filtering and Impulse Response Function (IRF) analysis.\",\n",
        "        \"irf\": {\n",
        "            \"doc\": \"Settings for the IRF simulation experiment.\",\n",
        "            \"H_horizon\": 10,\n",
        "            \"delta_shock_sizes\": [-2.0, -1.0, 1.0, 2.0],\n",
        "            \"n_baseline_sims\": 10,\n",
        "        },\n",
        "    },\n",
        "    \"nonparametric_methods\": {\n",
        "        \"doc\": \"Configuration for all nonparametric estimation techniques.\",\n",
        "        \"kde\": {\n",
        "            \"doc\": \"Settings for Kernel Density Estimation.\",\n",
        "            \"kernel\": \"gaussian\",\n",
        "            \"bandwidth_method\": \"stdev_rule\",\n",
        "        },\n",
        "    },\n",
        "}\n",
        "```\n",
        "This configuration explicitly instructs the pipeline to run only the core empirical analysis and to generate forecasts and IRFs for the two specified dates. The other, more time-consuming analyses are disabled.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Step 3: Executing the Pipeline and Inspecting the Results**\n",
        "\n",
        "With the inputs prepared, the final step is to call the master orchestrator function. This single call will execute the entire configured workflow.\n",
        "\n",
        "```python\n",
        "# --- Execute the End-to-End Pipeline ---\n",
        "\n",
        "# This is the single entry point to the entire system.\n",
        "# It will perform all steps enabled in the study_params dictionary.\n",
        "# In this case: data prep, estimation, forecasting, IRF simulation, and visualization.\n",
        "\n",
        "# To run this, you would need to have all the previously defined functions\n",
        "# (from Task 1 to 17) in the same script or imported from a library.\n",
        "# For example:\n",
        "# from my_cn_var_library import run_full_research_pipeline\n",
        "\n",
        "# Assuming the function is in scope, the call is:\n",
        "# final_results = run_full_research_pipeline(\n",
        "#     raw_df=raw_data_df,\n",
        "#     study_params=study_params\n",
        "# )\n",
        "\n",
        "# --- Inspecting the Output ---\n",
        "# The `final_results` object is a deeply nested dictionary containing all\n",
        "# artifacts from the analysis. We can inspect its structure.\n",
        "\n",
        "# print(\"\\n--- Structure of the Final Results Dictionary ---\")\n",
        "# for key, value in final_results.items():\n",
        "#     print(f\"- {key}:\")\n",
        "#     if isinstance(value, dict):\n",
        "#         for subkey in value.keys():\n",
        "#             print(f\"  - {subkey}\")\n",
        "\n",
        "# Example of accessing a specific result:\n",
        "# Get the point forecast for the first specified date.\n",
        "# first_forecast_date = forecast_and_irf_dates[0]\n",
        "# point_forecast_on_bubble = final_results['empirical_analysis']['forecasts'][first_forecast_date]['point_forecast']\n",
        "\n",
        "# print(f\"\\nPoint forecast at {first_forecast_date.strftime('%Y-%m-%d')}:\")\n",
        "# print(point_forecast_on_bubble)\n",
        "\n",
        "# Example of accessing an IRF result:\n",
        "# Get the IRF DataFrame for a +2.0 shock from the second specified date.\n",
        "# second_irf_date = forecast_and_irf_dates[1]\n",
        "# irf_df = final_results['empirical_analysis']['irf_analysis'][second_irf_date]['irfs'][2.0]\n",
        "\n",
        "# print(f\"\\nIRF to a +2.0 shock from {second_irf_date.strftime('%Y-%m-%d')}:\")\n",
        "# print(irf_df.head())\n",
        "```\n",
        "\n",
        "This example demonstrates how to structure the inputs and how to execute the pipeline with a single command, and it shows how to navigate the comprehensive results dictionary to extract key findings.\n"
      ],
      "metadata": {
        "id": "NrNmDaiFvXNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Parameter Validation and Data Cleansing\n",
        "\n",
        "def _validate_study_params_recursive(\n",
        "    params: Dict[str, Any],\n",
        "    schema: Dict[str, Any],\n",
        "    path: str = \"\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Recursively validates the structure and types of a nested dictionary.\n",
        "\n",
        "    This is a helper function for validate_and_cleanse_data. It traverses\n",
        "    both the user-provided parameter dictionary and a schema dictionary\n",
        "    in parallel, ensuring all keys in the schema exist in the parameters\n",
        "    and that the corresponding values have the correct data type.\n",
        "\n",
        "    Args:\n",
        "        params (Dict[str, Any]): The user-provided parameter dictionary.\n",
        "        schema (Dict[str, Any]): The schema dictionary defining expected\n",
        "                                 structure and types.\n",
        "        path (str, optional): The current nested path for error reporting.\n",
        "                              Defaults to \"\".\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If a key is missing or a value has an incorrect type.\n",
        "    \"\"\"\n",
        "    # Iterate through all keys defined in the schema for the current level.\n",
        "    for key, expected_type in schema.items():\n",
        "        # Construct the full path for clear error messages.\n",
        "        current_path = f\"{path}.{key}\" if path else key\n",
        "\n",
        "        # --- Key Existence Check ---\n",
        "        # Ensure the required key is present in the user's parameters.\n",
        "        if key not in params:\n",
        "            # Raise an error if a key is missing.\n",
        "            raise ValueError(f\"Missing required parameter key: '{current_path}'\")\n",
        "\n",
        "        # Get the user-provided value for the current key.\n",
        "        value = params[key]\n",
        "\n",
        "        # --- Type Validation ---\n",
        "        # If the expected type is a dictionary, recurse.\n",
        "        if isinstance(expected_type, dict):\n",
        "            # The corresponding value in params must also be a dictionary.\n",
        "            if not isinstance(value, dict):\n",
        "                # Raise an error if the type is not a dictionary as expected.\n",
        "                raise ValueError(\n",
        "                    f\"Parameter '{current_path}' must be a dictionary.\"\n",
        "                )\n",
        "            # Perform recursive validation on the nested dictionary.\n",
        "            _validate_study_params_recursive(value, expected_type, current_path)\n",
        "        # If the expected type is a list, validate the list and its contents.\n",
        "        elif isinstance(expected_type, list):\n",
        "            # The value must be a list.\n",
        "            if not isinstance(value, list):\n",
        "                # Raise an error if the type is not a list.\n",
        "                raise ValueError(f\"Parameter '{current_path}' must be a list.\")\n",
        "            # The schema should specify the type of elements in the list.\n",
        "            if not expected_type:\n",
        "                # Raise an error if the schema is not configured correctly.\n",
        "                raise TypeError(\n",
        "                    f\"Schema for '{current_path}' is an empty list; \"\n",
        "                    \"cannot determine required element type.\"\n",
        "                )\n",
        "            # Get the required type for the list elements from the schema.\n",
        "            element_type = expected_type[0]\n",
        "            # Check each element in the user's list.\n",
        "            for element in value:\n",
        "                # Ensure each element has the correct type.\n",
        "                if not isinstance(element, element_type):\n",
        "                    # Raise an error if an element has the wrong type.\n",
        "                    raise ValueError(\n",
        "                        f\"All elements in list '{current_path}' must be of \"\n",
        "                        f\"type {element_type.__name__}.\"\n",
        "                    )\n",
        "        # For all other types (int, float, str), perform a simple isinstance check.\n",
        "        elif not isinstance(value, expected_type):\n",
        "            # Raise an error if the value's type does not match the expected type.\n",
        "            raise ValueError(\n",
        "                f\"Parameter '{current_path}' must be of type \"\n",
        "                f\"{expected_type.__name__}, but got {type(value).__name__}.\"\n",
        "            )\n",
        "\n",
        "def validate_and_cleanse_data(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Validates study parameters and input data, then cleanses the data.\n",
        "\n",
        "    This function serves as the first, critical step in the analysis pipeline.\n",
        "    It performs a series of rigorous checks on both the study configuration\n",
        "    dictionary and the input time series data. If all validations pass, it\n",
        "    proceeds to cleanse the data by interpolating missing values and\n",
        "    resampling from monthly to quarterly frequency.\n",
        "\n",
        "    The validation steps are:\n",
        "    1.  **Parameter Schema Validation**: Ensures the `study_params` dictionary\n",
        "        has the correct nested structure and data types for all keys.\n",
        "    2.  **Numerical Parameter Range Validation**: Checks that numerical\n",
        "        parameters (e.g., lags, alphas) are within their valid logical\n",
        "        ranges (e.g., positive, between 0 and 1).\n",
        "    3.  **DataFrame Structure Validation**: Verifies the input DataFrame has a\n",
        "        monthly DatetimeIndex and the two required columns.\n",
        "    4.  **Data Integrity Checks**: Scans for missing values (NaNs) and raises\n",
        "        an error if significant outliers are detected, as their handling is\n",
        "        highly context-dependent in bubble analysis.\n",
        "\n",
        "    The cleansing steps are:\n",
        "    1.  **Interpolation**: Fills any missing values using linear interpolation.\n",
        "    2.  **Resampling**: Converts the monthly time series to a quarterly\n",
        "        frequency by taking the last observation of each quarter.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame): A DataFrame with a monthly DatetimeIndex and\n",
        "                               two columns: \"real_oil_price\" and \"real_gdp\".\n",
        "        study_params (Dict[str, Any]): A nested dictionary containing all\n",
        "                                       study parameters.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `raw_df` is not a pandas DataFrame or `study_params`\n",
        "                   is not a dictionary.\n",
        "        ValueError: If any validation check fails, with a detailed message\n",
        "                    explaining the specific error.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleansed DataFrame, interpolated and resampled to a\n",
        "                      quarterly frequency, ready for further processing.\n",
        "    \"\"\"\n",
        "    # --- Input Type Validation ---\n",
        "    # Ensure the primary inputs are of the correct base type.\n",
        "    if not isinstance(raw_df, pd.DataFrame):\n",
        "        # Raise an error if the data is not a DataFrame.\n",
        "        raise TypeError(\"Input 'raw_df' must be a pandas DataFrame.\")\n",
        "    # Ensure the parameters are provided in a dictionary.\n",
        "    if not isinstance(study_params, dict):\n",
        "        # Raise an error if the parameters are not a dictionary.\n",
        "        raise TypeError(\"Input 'study_params' must be a dictionary.\")\n",
        "\n",
        "    # --- Task 1, Step 1: Parameter Dictionary Structure Validation ---\n",
        "    # Define the expected schema for the study parameters dictionary.\n",
        "    param_schema = {\n",
        "        \"estimation\": {\n",
        "            \"var_spec\": {\"p_lags\": int},\n",
        "            \"gcov_spec\": {\"H_moment_lags\": int, \"error_powers\": [int]}\n",
        "        },\n",
        "        \"forecasting\": {\n",
        "            \"intervals\": {\n",
        "                \"prediction_interval_alpha\": float,\n",
        "                \"confidence_set_alpha\": float\n",
        "            },\n",
        "            \"simulation\": {\n",
        "                \"S_bootstrap_replications\": int,\n",
        "                \"grid_points_per_dim\": int\n",
        "            }\n",
        "        },\n",
        "        \"structural_analysis\": {\n",
        "            \"irf\": {\n",
        "                \"H_horizon\": int,\n",
        "                \"delta_shock_sizes\": [float],\n",
        "                \"n_baseline_sims\": int\n",
        "            }\n",
        "        },\n",
        "        \"nonparametric_methods\": {\n",
        "            \"kde\": {\"kernel\": str, \"bandwidth_method\": str}\n",
        "        }\n",
        "    }\n",
        "    # Call the recursive helper to validate the entire parameter structure.\n",
        "    _validate_study_params_recursive(study_params, param_schema)\n",
        "\n",
        "    # --- Task 1, Step 2: Numerical Parameter Range Validation ---\n",
        "    # Extract parameters for convenient access.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    H_moment_lags = study_params[\"estimation\"][\"gcov_spec\"][\"H_moment_lags\"]\n",
        "    pi_alpha = study_params[\"forecasting\"][\"intervals\"][\"prediction_interval_alpha\"]\n",
        "    cs_alpha = study_params[\"forecasting\"][\"intervals\"][\"confidence_set_alpha\"]\n",
        "    S_reps = study_params[\"forecasting\"][\"simulation\"][\"S_bootstrap_replications\"]\n",
        "    grid_pts = study_params[\"forecasting\"][\"simulation\"][\"grid_points_per_dim\"]\n",
        "    H_horizon = study_params[\"structural_analysis\"][\"irf\"][\"H_horizon\"]\n",
        "    n_baseline = study_params[\"structural_analysis\"][\"irf\"][\"n_baseline_sims\"]\n",
        "\n",
        "    # Validate that integer parameters are positive.\n",
        "    if not all(p > 0 for p in [p_lags, H_moment_lags, S_reps, grid_pts, H_horizon, n_baseline]):\n",
        "        # Raise an error if any integer parameter is not strictly positive.\n",
        "        raise ValueError(\"Integer parameters (lags, replications, etc.) must be positive.\")\n",
        "\n",
        "    # Validate that alpha values for intervals are within the (0, 1) range.\n",
        "    if not all(0 < alpha < 1 for alpha in [pi_alpha, cs_alpha]):\n",
        "        # Raise an error if alpha is not a valid probability.\n",
        "        raise ValueError(\"Alpha values for intervals must be between 0 and 1.\")\n",
        "\n",
        "    # --- Task 1, Step 3: DataFrame Structure and Index Validation ---\n",
        "    # Check if the DataFrame index is a DatetimeIndex.\n",
        "    if not isinstance(raw_df.index, pd.DatetimeIndex):\n",
        "        # Raise an error if the index is not of the correct type.\n",
        "        raise ValueError(\"Input DataFrame index must be a DatetimeIndex.\")\n",
        "\n",
        "    # Infer the frequency of the time series.\n",
        "    freq = pd.infer_freq(raw_df.index)\n",
        "    # Check if the frequency is monthly (Month Start 'MS' or Month End 'M').\n",
        "    if freq not in ('M', 'MS'):\n",
        "        # Raise an error if the data is not at a monthly frequency.\n",
        "        raise ValueError(f\"Inferred frequency is '{freq}'. Expected monthly ('M' or 'MS').\")\n",
        "\n",
        "    # Check for the exact number of required columns.\n",
        "    if raw_df.shape[1] != 2:\n",
        "        # Raise an error if the column count is incorrect.\n",
        "        raise ValueError(f\"Input DataFrame must have exactly 2 columns, but found {raw_df.shape[1]}.\")\n",
        "\n",
        "    # Define the set of required column names.\n",
        "    required_cols: Set[str] = {\"real_oil_price\", \"real_gdp\"}\n",
        "    # Check if the DataFrame's columns match the required set.\n",
        "    if set(raw_df.columns) != required_cols:\n",
        "        # Raise an error if column names are incorrect.\n",
        "        raise ValueError(f\"DataFrame columns must be {required_cols}.\")\n",
        "\n",
        "    # --- Task 1, Step 4: Missing Value Detection and Outlier Identification ---\n",
        "    # Check for any missing values in the entire DataFrame.\n",
        "    if raw_df.isnull().values.any():\n",
        "        # Print a warning that NaNs were found and will be interpolated.\n",
        "        print(\"Warning: Missing values (NaNs) detected. Proceeding with linear interpolation.\")\n",
        "\n",
        "    # Calculate the first and third quartiles for outlier detection.\n",
        "    Q1 = raw_df.quantile(0.25)\n",
        "    Q3 = raw_df.quantile(0.75)\n",
        "    # Calculate the Interquartile Range (IQR).\n",
        "    IQR = Q3 - Q1\n",
        "    # Define the outlier bounds.\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    # Create a boolean mask to identify outliers.\n",
        "    outlier_mask = (raw_df < lower_bound) | (raw_df > upper_bound)\n",
        "    # Check if any outliers were detected.\n",
        "    if outlier_mask.values.any():\n",
        "        # Raise an error if outliers are found, as their handling is critical.\n",
        "        raise ValueError(\n",
        "            \"Outliers detected using the 1.5*IQR rule. \"\n",
        "            \"Please manually inspect and clean the data before proceeding, \"\n",
        "            \"as automatic removal may distort bubble dynamics.\"\n",
        "        )\n",
        "\n",
        "    # --- Task 1, Step 5: Data Interpolation and Frequency Conversion ---\n",
        "    # Create a copy to avoid modifying the original DataFrame.\n",
        "    cleansed_df = raw_df.copy()\n",
        "    # Perform linear interpolation to fill any gaps.\n",
        "    cleansed_df = cleansed_df.interpolate(method='linear')\n",
        "    # Resample the data from monthly to quarterly frequency, taking the last value in each quarter.\n",
        "    # This is a standard method for converting price/level data.\n",
        "    quarterly_df = cleansed_df.resample('Q').last()\n",
        "\n",
        "    # Return the fully validated, cleaned, and resampled DataFrame.\n",
        "    return quarterly_df\n"
      ],
      "metadata": {
        "id": "g7T2UoaCvr2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Preparation\n",
        "\n",
        "def prepare_var_data(\n",
        "    quarterly_df: pd.DataFrame\n",
        ") -> Tuple[pd.DataFrame, Dict[str, float]]:\n",
        "    \"\"\"\n",
        "    Prepares cleansed quarterly data for mixed VAR model estimation.\n",
        "\n",
        "    This function executes the essential data transformations required to convert\n",
        "    raw level data into stationary, demeaned series suitable for the GCov\n",
        "    estimator. It follows the specific preparation steps outlined in the\n",
        "    Gourieroux and Jasiak (2017, 2025) empirical application.\n",
        "\n",
        "    The sequence of transformations is:\n",
        "    1.  **Calculate GDP Growth Rate**: Converts the 'real_gdp' level series\n",
        "        into a quarter-over-quarter percentage growth rate.\n",
        "        Equation: growth_rate_t = 100 * (GDP_t - GDP_{t-1}) / GDP_{t-1}\n",
        "    2.  **Scale Oil Prices**: Divides the 'real_oil_price' series by a\n",
        "        factor of 10, as done in the paper's application section (p. 29).\n",
        "    3.  **Demean Series**: Calculates and subtracts the sample mean from both\n",
        "        the GDP growth rate and the scaled oil price series. The calculated\n",
        "        means are returned for later use (e.g., re-centering forecasts).\n",
        "        Equation: Y_t = X_t - mean(X)\n",
        "    4.  **Finalize Data**: Removes the first row, which contains a NaN value\n",
        "        resulting from the growth rate calculation, to produce a final,\n",
        "        analysis-ready DataFrame.\n",
        "\n",
        "    Args:\n",
        "        quarterly_df (pd.DataFrame): A cleansed DataFrame with a quarterly\n",
        "                                     DatetimeIndex or PeriodIndex and two\n",
        "                                     columns: \"real_oil_price\" and \"real_gdp\".\n",
        "                                     This should be the output of the\n",
        "                                     `validate_and_cleanse_data` function.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If `quarterly_df` is not a pandas DataFrame.\n",
        "        ValueError: If `quarterly_df` does not contain the required columns\n",
        "                    'real_oil_price' and 'real_gdp', or if it is empty.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, float]]:\n",
        "        - A pandas DataFrame containing the two prepared, demeaned time\n",
        "          series, ready for VAR estimation. The columns are renamed to\n",
        "          'gdp_growth_rate' and 'scaled_oil_price'.\n",
        "        - A dictionary containing the calculated sample means for both\n",
        "          series, which are needed to revert forecasts to their original scale.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(quarterly_df, pd.DataFrame):\n",
        "        # Raise an error for incorrect input type.\n",
        "        raise TypeError(\"Input 'quarterly_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Ensure the DataFrame is not empty.\n",
        "    if quarterly_df.empty:\n",
        "        # Raise an error if the DataFrame has no data.\n",
        "        raise ValueError(\"Input 'quarterly_df' cannot be empty.\")\n",
        "\n",
        "    # Define the required column names for this processing step.\n",
        "    required_cols = {\"real_oil_price\", \"real_gdp\"}\n",
        "    # Check if the required columns exist in the input DataFrame.\n",
        "    if not required_cols.issubset(quarterly_df.columns):\n",
        "        # Raise an error if columns are missing.\n",
        "        raise ValueError(f\"Input DataFrame must contain columns: {required_cols}\")\n",
        "\n",
        "    # --- Data Transformation ---\n",
        "    # Create a new DataFrame to store the transformed series.\n",
        "    # This avoids modifying the original DataFrame (good practice).\n",
        "    transformed_df = pd.DataFrame(index=quarterly_df.index)\n",
        "\n",
        "    # --- Task 2, Step 1: GDP Growth Rate Calculation ---\n",
        "    # Define the column name for real GDP for clarity.\n",
        "    gdp_col = 'real_gdp'\n",
        "    # Calculate the percentage change for the GDP series.\n",
        "    # Equation: growth_rate_t = 100 * (GDP_t - GDP_{t-1}) / GDP_{t-1}\n",
        "    transformed_df['gdp_growth_rate'] = quarterly_df[gdp_col].pct_change() * 100\n",
        "\n",
        "    # --- Task 2, Step 2: Oil Price Adjustment and Scaling ---\n",
        "    # Define the column name for real oil price.\n",
        "    oil_col = 'real_oil_price'\n",
        "    # Define the scaling factor as per the paper's methodology (p. 29).\n",
        "    OIL_PRICE_SCALING_FACTOR = 10.0\n",
        "    # Apply the scaling factor to the oil price series.\n",
        "    transformed_df['scaled_oil_price'] = quarterly_df[oil_col] / OIL_PRICE_SCALING_FACTOR\n",
        "\n",
        "    # --- Task 2, Step 3: Series Demeaning ---\n",
        "    # Calculate the sample means of the newly transformed series.\n",
        "    # The .mean() method in pandas correctly ignores the NaN in the first row.\n",
        "    series_means = {\n",
        "        'gdp_growth_rate_mean': transformed_df['gdp_growth_rate'].mean(),\n",
        "        'scaled_oil_price_mean': transformed_df['scaled_oil_price'].mean()\n",
        "    }\n",
        "\n",
        "    # Demean each series by subtracting its calculated sample mean.\n",
        "    # Equation: Y_t = X_t - mean(X)\n",
        "    prepared_df = transformed_df.copy()\n",
        "    prepared_df['gdp_growth_rate'] -= series_means['gdp_growth_rate_mean']\n",
        "    prepared_df['scaled_oil_price'] -= series_means['scaled_oil_price_mean']\n",
        "\n",
        "    # --- Task 2, Step 4: Data Structure Preparation for VAR Analysis ---\n",
        "    # Remove the first row, which contains NaN due to the pct_change() operation.\n",
        "    # This yields the final DataFrame ready for creating lagged variables for the VAR model.\n",
        "    final_df = prepared_df.dropna()\n",
        "\n",
        "    # Final validation to ensure the output is not empty after dropping NaNs.\n",
        "    if final_df.empty:\n",
        "        # This can happen if the input series was too short.\n",
        "        raise ValueError(\n",
        "            \"DataFrame is empty after transformations. \"\n",
        "            \"Input time series must have at least 2 observations.\"\n",
        "        )\n",
        "\n",
        "    # Return the final, prepared DataFrame and the dictionary of means.\n",
        "    return final_df, series_means\n"
      ],
      "metadata": {
        "id": "mFYthqO1v61R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Implement the Generalized Covariance (GCov) Estimator\n",
        "\n",
        "def _create_var_design_matrix(\n",
        "    data: np.ndarray,\n",
        "    p_lags: int\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Constructs the design matrix (X) and target matrix (Y) for VAR(p).\n",
        "\n",
        "    This helper function prepares the data for VAR model estimation (both OLS\n",
        "    and GCov). For a given time series `data` of shape (T, m), it creates:\n",
        "    - Y: The matrix of dependent variables, from t=p to T-1.\n",
        "      Shape: (T-p, m)\n",
        "    - X: The matrix of regressors, containing p lags of Y. For each row t,\n",
        "      X[t] = [Y_{t-1}', Y_{t-2}', ..., Y_{t-p}'].\n",
        "      Shape: (T-p, m * p)\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): The input time series data of shape (T, m), where\n",
        "                           T is the number of observations and m is the number\n",
        "                           of variables.\n",
        "        p_lags (int): The lag order 'p' of the VAR model.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]: A tuple containing (Y, X).\n",
        "    \"\"\"\n",
        "    # Get the number of observations T and variables m from the data shape.\n",
        "    T, m = data.shape\n",
        "    # The effective number of observations for regression is T - p_lags.\n",
        "    num_obs_eff = T - p_lags\n",
        "\n",
        "    # Create the target matrix Y by taking observations from p_lags to the end.\n",
        "    Y = data[p_lags:, :]\n",
        "\n",
        "    # Create the design matrix X of regressors.\n",
        "    # Initialize an empty matrix with the correct final shape.\n",
        "    X = np.zeros((num_obs_eff, m * p_lags))\n",
        "\n",
        "    # Populate the design matrix X with lagged data.\n",
        "    for i in range(num_obs_eff):\n",
        "        # For each observation i (which corresponds to time t = i + p_lags),\n",
        "        # we construct the corresponding row of X.\n",
        "        # The row consists of the flattened lagged vectors.\n",
        "        # data[i+p_lags-1] is Y_{t-1}, data[i+p_lags-2] is Y_{t-2}, etc.\n",
        "        lagged_vectors = data[i : i + p_lags, :][::-1, :].ravel()\n",
        "        X[i, :] = lagged_vectors\n",
        "\n",
        "    return Y, X\n",
        "\n",
        "def _estimate_ols_var(\n",
        "    data: np.ndarray,\n",
        "    p_lags: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Estimates VAR(p) coefficients using Ordinary Least Squares (OLS).\n",
        "\n",
        "    This function provides initial parameter values for the GCov optimization.\n",
        "    It solves the multivariate least squares problem Y = X \\beta + E, where\n",
        "    \\beta contains the flattened VAR coefficient matrices.\n",
        "\n",
        "    Args:\n",
        "        data (np.ndarray): The input time series data of shape (T, m).\n",
        "        p_lags (int): The lag order 'p' of the VAR model.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A flattened 1D array of the estimated OLS coefficients,\n",
        "                    suitable for use as an initial guess `x0` in an optimizer.\n",
        "    \"\"\"\n",
        "    # Create the design and target matrices from the data.\n",
        "    Y, X = _create_var_design_matrix(data, p_lags)\n",
        "\n",
        "    # Solve the least squares problem to find the coefficient matrix.\n",
        "    # np.linalg.lstsq is numerically more stable than direct inversion.\n",
        "    # It returns the solution, residuals, rank, and singular values.\n",
        "    # The solution `beta_matrix` has shape (m * p, m).\n",
        "    beta_matrix, _, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n",
        "\n",
        "    # The optimizer expects a flattened 1D vector.\n",
        "    # We flatten the coefficient matrix to create the initial parameter vector theta_0.\n",
        "    # The transpose is needed to align with the [phi1, phi2,...] structure.\n",
        "    theta_0 = beta_matrix.T.ravel()\n",
        "\n",
        "    return theta_0\n",
        "\n",
        "def _gcov_objective_function(\n",
        "    theta: np.ndarray,\n",
        "    data: np.ndarray,\n",
        "    p_lags: int,\n",
        "    H_moment_lags: int,\n",
        "    error_powers: List[int]\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the GCov objective function value Q_T(theta).\n",
        "\n",
        "    This is the core function to be minimized by the optimizer. It takes a\n",
        "    parameter vector `theta`, computes the corresponding model residuals,\n",
        "    constructs the moment conditions based on the i.i.d. assumption of\n",
        "    true errors, and returns the quadratic form of the sample-averaged moments.\n",
        "\n",
        "    Equation: Q_T(theta) = g_bar(theta)' * W * g_bar(theta), where W=I.\n",
        "    g_bar is the time-average of the moment vectors g_t(theta).\n",
        "\n",
        "    Args:\n",
        "        theta (np.ndarray): A 1D array of flattened VAR coefficients.\n",
        "        data (np.ndarray): The input time series data of shape (T, m).\n",
        "        p_lags (int): The VAR lag order.\n",
        "        H_moment_lags (int): The number of lags H for moment conditions.\n",
        "        error_powers (List[int]): The powers of residuals to use in moments.\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar value of the GCov objective function.\n",
        "    \"\"\"\n",
        "    # Get dimensions from the data.\n",
        "    T, m = data.shape\n",
        "\n",
        "    # --- 1. Reconstruct Phi matrices and compute residuals ---\n",
        "    # Reshape the flat parameter vector `theta` into the VAR coefficient matrix.\n",
        "    # The shape is (m, m * p_lags) representing [Phi_1, Phi_2, ..., Phi_p].\n",
        "    phi_stacked = theta.reshape((m, m * p_lags))\n",
        "\n",
        "    # Create the design matrix of lagged variables.\n",
        "    _, X = _create_var_design_matrix(data, p_lags)\n",
        "\n",
        "    # Predict Y_hat using the current parameter guess `theta`.\n",
        "    # Y_hat = X * Phi'\n",
        "    Y_hat = X @ phi_stacked.T\n",
        "\n",
        "    # The residuals are the difference between actual Y and predicted Y.\n",
        "    # These are the estimated errors epsilon_hat_t(theta).\n",
        "    # Residuals start at time t=p, so there are T-p residuals.\n",
        "    residuals = data[p_lags:, :] - Y_hat\n",
        "\n",
        "    # --- 2. Construct moment conditions ---\n",
        "    # The moments are based on correlations of powers of residuals.\n",
        "    # We need H lags, so the effective sample for moments starts later.\n",
        "    # The number of observations available for moment calculation.\n",
        "    T_eff = residuals.shape[0] - H_moment_lags\n",
        "\n",
        "    # Isolate the \"current\" residuals (from t=p+H to T-1).\n",
        "    current_residuals = residuals[H_moment_lags:, :]\n",
        "\n",
        "    # Create a list to hold the moment condition matrices for each power.\n",
        "    all_moments = []\n",
        "\n",
        "    # Loop through the specified error powers (e.g., 1, 2).\n",
        "    for power in error_powers:\n",
        "        # Calculate the specified power of the current residuals.\n",
        "        # Shape: (T_eff, m)\n",
        "        powered_current_res = np.power(current_residuals, power)\n",
        "\n",
        "        # Loop through the required lags for the moment conditions (0 to H).\n",
        "        for h in range(H_moment_lags + 1):\n",
        "            # Get the h-lagged residuals.\n",
        "            # Shape: (T_eff, m)\n",
        "            lagged_res = residuals[H_moment_lags - h : -h if h > 0 else None, :]\n",
        "\n",
        "            # The moment condition is the element-wise product of the\n",
        "            # powered current residual and the lagged residual.\n",
        "            # This corresponds to E[eps_t^power * eps_{t-h}].\n",
        "            # Shape: (T_eff, m * m)\n",
        "            moment_h = np.einsum('ti,tj->tij', powered_current_res, lagged_res).reshape(T_eff, -1)\n",
        "            all_moments.append(moment_h)\n",
        "\n",
        "    # Concatenate all moment matrices into a single large matrix.\n",
        "    # Shape: (T_eff, K) where K is the total number of moments.\n",
        "    g_t = np.concatenate(all_moments, axis=1)\n",
        "\n",
        "    # --- 3. Calculate the quadratic objective function ---\n",
        "    # Calculate the time-average of the moment vectors.\n",
        "    # Shape: (K,)\n",
        "    g_bar = np.mean(g_t, axis=0)\n",
        "\n",
        "    # Calculate the quadratic form Q_T = g_bar' * W * g_bar.\n",
        "    # For one-step GCov, the weighting matrix W is the identity matrix.\n",
        "    # This simplifies to the squared L2 norm of the average moment vector.\n",
        "    objective_value = g_bar.T @ g_bar\n",
        "\n",
        "    return float(objective_value)\n",
        "\n",
        "def estimate_gcov_var(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, OptimizeResult]:\n",
        "    \"\"\"\n",
        "    Estimates the mixed Causal-Noncausal VAR(p) model using GCov.\n",
        "\n",
        "    This function orchestrates the entire estimation process. It first\n",
        "    computes OLS estimates to use as a robust starting point. It then\n",
        "    minimizes the Generalized Covariance (GCov) objective function using a\n",
        "    numerical optimizer to find the final parameter estimates.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The prepared, demeaned time series data.\n",
        "                                    This is the output of `prepare_var_data`.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters,\n",
        "                                       from which `p_lags`, `H_moment_lags`,\n",
        "                                       and `error_powers` are extracted.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If the numerical optimization fails to converge.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, OptimizeResult]:\n",
        "        - The estimated VAR coefficient matrix Phi, stacked as [Phi_1, ..., Phi_p],\n",
        "          with shape (m, m * p).\n",
        "        - The full result object from `scipy.optimize.minimize`, which\n",
        "          contains details about the optimization process (e.g., convergence\n",
        "          status, final objective value).\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Ensure the input is a pandas DataFrame.\n",
        "    if not isinstance(prepared_df, pd.DataFrame):\n",
        "        raise TypeError(\"Input 'prepared_df' must be a pandas DataFrame.\")\n",
        "    # Ensure the DataFrame is not empty.\n",
        "    if prepared_df.empty:\n",
        "        raise ValueError(\"Input 'prepared_df' cannot be empty.\")\n",
        "\n",
        "    # Extract necessary parameters from the study configuration.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    gcov_spec = study_params[\"estimation\"][\"gcov_spec\"]\n",
        "    H_moment_lags = gcov_spec[\"H_moment_lags\"]\n",
        "    error_powers = gcov_spec[\"error_powers\"]\n",
        "\n",
        "    # Convert the DataFrame to a NumPy array for numerical efficiency.\n",
        "    data_np = prepared_df.to_numpy()\n",
        "    m = data_np.shape[1]\n",
        "\n",
        "    # --- Task 3, Step 4: Initial Value Estimation ---\n",
        "    # Compute OLS estimates to use as the initial guess for the optimizer.\n",
        "    theta_initial = _estimate_ols_var(data_np, p_lags)\n",
        "\n",
        "    # --- Task 3, Step 5: Numerical Optimization Implementation ---\n",
        "    # Define the objective function with fixed data and parameters.\n",
        "    # Using a lambda function is a clean way to pass extra arguments to the optimizer.\n",
        "    objective_fn: Callable[[np.ndarray], float] = lambda theta: _gcov_objective_function(\n",
        "        theta,\n",
        "        data=data_np,\n",
        "        p_lags=p_lags,\n",
        "        H_moment_lags=H_moment_lags,\n",
        "        error_powers=error_powers\n",
        "    )\n",
        "\n",
        "    # Perform the optimization using scipy's minimize function.\n",
        "    # 'L-BFGS-B' is a robust quasi-Newton method suitable for this problem.\n",
        "    optimization_result = minimize(\n",
        "        fun=objective_fn,\n",
        "        x0=theta_initial,\n",
        "        method='L-BFGS-B',\n",
        "        options={'disp': False} # Set to True for verbose optimizer output.\n",
        "    )\n",
        "\n",
        "    # --- Post-Optimization Processing ---\n",
        "    # Check if the optimization was successful.\n",
        "    if not optimization_result.success:\n",
        "        # If not, raise a runtime error with the optimizer's message.\n",
        "        raise RuntimeError(\n",
        "            \"GCov estimation failed to converge. \"\n",
        "            f\"Optimizer message: {optimization_result.message}\"\n",
        "        )\n",
        "\n",
        "    # Extract the optimal parameter vector from the result.\n",
        "    theta_optimal = optimization_result.x\n",
        "\n",
        "    # Reshape the optimal flat vector back into the stacked Phi matrix.\n",
        "    # Shape: (m, m * p_lags)\n",
        "    phi_estimated = theta_optimal.reshape((m, m * p_lags))\n",
        "\n",
        "    # Return the estimated coefficient matrix and the full optimization result.\n",
        "    return phi_estimated, optimization_result\n"
      ],
      "metadata": {
        "id": "eGvDwB2VwmmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Compute the Real Jordan Decomposition\n",
        "\n",
        "def _construct_companion_matrix(\n",
        "    phi_estimated: np.ndarray,\n",
        "    m: int,\n",
        "    p_lags: int\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Constructs the VAR(p) companion matrix Psi.\n",
        "\n",
        "    For a VAR(p) model, this function creates the state transition matrix\n",
        "    for the equivalent VAR(1) representation.\n",
        "\n",
        "    Args:\n",
        "        phi_estimated (np.ndarray): The estimated VAR coefficient matrix,\n",
        "                                    stacked as [Phi_1, ..., Phi_p], with\n",
        "                                    shape (m, m * p_lags).\n",
        "        m (int): The number of variables in the time series.\n",
        "        p_lags (int): The lag order 'p' of the VAR model.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The companion matrix Psi of shape (m*p, m*p).\n",
        "    \"\"\"\n",
        "    # The total dimension of the state-space system.\n",
        "    n = m * p_lags\n",
        "\n",
        "    # Handle the simple VAR(1) case directly.\n",
        "    if p_lags == 1:\n",
        "        # For p=1, the companion matrix is just the Phi matrix itself.\n",
        "        return phi_estimated\n",
        "\n",
        "    # Initialize the companion matrix with zeros.\n",
        "    companion_matrix = np.zeros((n, n))\n",
        "\n",
        "    # The first block-row of the companion matrix contains the Phi coefficients.\n",
        "    companion_matrix[0:m, :] = phi_estimated\n",
        "\n",
        "    # The lower block of the companion matrix is a shifted identity matrix.\n",
        "    # This creates the lag structure Y_{t-1} = Y_{t-1}, Y_{t-2} = Y_{t-2}, etc.\n",
        "    # It's an identity matrix of size m*(p-1) placed at the correct offset.\n",
        "    identity_block = np.eye(m * (p_lags - 1))\n",
        "    companion_matrix[m:, 0:-m] = identity_block\n",
        "\n",
        "    return companion_matrix\n",
        "\n",
        "def compute_jordan_decomposition(\n",
        "    phi_estimated: np.ndarray,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Computes the Real Jordan Decomposition of the VAR(p) companion matrix.\n",
        "\n",
        "    This function is a cornerstone of the mixed causal-noncausal analysis.\n",
        "    It decomposes the system's dynamics into stable (causal) and unstable\n",
        "    (non-causal) components. The decomposition is Psi = A * J * A_inv, where:\n",
        "    - Psi: The VAR(p) companion matrix.\n",
        "    - J: The real Jordan form, a block-diagonal matrix with eigenvalues\n",
        "         or 2x2 real blocks for complex conjugate pairs.\n",
        "    - A: The transformation matrix whose columns are generalized eigenvectors.\n",
        "    - A_inv: The inverse of A.\n",
        "\n",
        "    The function performs the following steps:\n",
        "    1. Constructs the companion matrix Psi from the estimated Phi coefficients.\n",
        "    2. Computes the eigenvalues and eigenvectors of Psi.\n",
        "    3. Classifies eigenvalues as causal (|lambda|<1) or non-causal (|lambda|>1)\n",
        "       and raises an error if unit roots are found.\n",
        "    4. Sorts eigenvalues and eigenvectors to group causal and non-causal components.\n",
        "    5. Iteratively constructs the real Jordan matrix J and the transformation\n",
        "       matrix A, handling both real and complex conjugate eigenvalues.\n",
        "    6. Computes the inverse of A and validates the decomposition.\n",
        "\n",
        "    Args:\n",
        "        phi_estimated (np.ndarray): The estimated VAR coefficient matrix,\n",
        "                                    stacked as [Phi_1, ..., Phi_p], with\n",
        "                                    shape (m, m * p_lags).\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters,\n",
        "                                       from which `p_lags` is extracted.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the companion matrix has eigenvalues with modulus\n",
        "                    exactly 1 (unit roots), which violates model assumptions.\n",
        "        np.linalg.LinAlgError: If the constructed transformation matrix A is\n",
        "                               singular and cannot be inverted.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing all components of the\n",
        "                        decomposition:\n",
        "                        'J': The real Jordan matrix.\n",
        "                        'A': The transformation matrix.\n",
        "                        'A_inv': The inverse of A.\n",
        "                        'n1': The number of causal eigenvalues.\n",
        "                        'n2': The number of non-causal eigenvalues.\n",
        "                        'eigenvalues_sorted': The sorted eigenvalues.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Extract model dimensions from parameters and inputs.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    m = phi_estimated.shape[0]\n",
        "    n = m * p_lags\n",
        "\n",
        "    # --- Task 4, Step 1: Companion Matrix Construction ---\n",
        "    # Construct the companion matrix Psi from the estimated Phi coefficients.\n",
        "    psi_matrix = _construct_companion_matrix(phi_estimated, m, p_lags)\n",
        "\n",
        "    # --- Task 4, Step 2: Eigenvalue and Eigenvector Computation ---\n",
        "    # Compute eigenvalues and right eigenvectors of the companion matrix.\n",
        "    # Using scipy.linalg.eig is generally preferred for robustness.\n",
        "    eigenvalues, eigenvectors = linalg.eig(psi_matrix)\n",
        "\n",
        "    # --- Task 4, Step 3: Eigenvalue Classification and Sorting ---\n",
        "    # Calculate the modulus (absolute value) of each eigenvalue.\n",
        "    abs_eigenvalues = np.abs(eigenvalues)\n",
        "\n",
        "    # Check for unit roots, which are not allowed by the model's theory.\n",
        "    # A small tolerance is used for numerical stability.\n",
        "    if np.any(np.isclose(abs_eigenvalues, 1.0)):\n",
        "        raise ValueError(\n",
        "            \"Companion matrix has unit roots (eigenvalues with modulus 1). \"\n",
        "            \"This model is not supported.\"\n",
        "        )\n",
        "\n",
        "    # Create boolean masks to classify eigenvalues.\n",
        "    causal_mask = abs_eigenvalues < 1.0\n",
        "    non_causal_mask = abs_eigenvalues > 1.0\n",
        "\n",
        "    # Get the indices that would sort the eigenvalues by causality then magnitude.\n",
        "    # Causal roots are placed first, then non-causal.\n",
        "    sorting_indices = np.concatenate([\n",
        "        np.where(causal_mask)[0],\n",
        "        np.where(non_causal_mask)[0]\n",
        "    ])\n",
        "\n",
        "    # Apply the sorting to both eigenvalues and eigenvectors to maintain correspondence.\n",
        "    eigenvalues_sorted = eigenvalues[sorting_indices]\n",
        "    eigenvectors_sorted = eigenvectors[:, sorting_indices]\n",
        "\n",
        "    # Count the number of causal (n1) and non-causal (n2) roots.\n",
        "    n1 = np.sum(causal_mask)\n",
        "    n2 = np.sum(non_causal_mask)\n",
        "\n",
        "    # --- Task 4, Step 4: Real Jordan Form and Transformation Matrix Construction ---\n",
        "    # Initialize the Jordan matrix J and transformation matrix A with zeros.\n",
        "    J = np.zeros((n, n))\n",
        "    A = np.zeros((n, n), dtype=float) # A must be real.\n",
        "\n",
        "    # Iterate through the sorted eigenvalues to build J and A.\n",
        "    i = 0\n",
        "    col_ptr = 0\n",
        "    while i < n:\n",
        "        # Get the current eigenvalue and eigenvector.\n",
        "        eigval = eigenvalues_sorted[i]\n",
        "        eigvec = eigenvectors_sorted[:, i]\n",
        "\n",
        "        if np.isreal(eigval):\n",
        "            # Case 1: Real eigenvalue.\n",
        "            # Place the real eigenvalue on the diagonal of J.\n",
        "            J[col_ptr, col_ptr] = eigval.real\n",
        "            # The corresponding column of A is the real part of the eigenvector.\n",
        "            A[:, col_ptr] = eigvec.real\n",
        "            # Move to the next column.\n",
        "            i += 1\n",
        "            col_ptr += 1\n",
        "        else:\n",
        "            # Case 2: Complex conjugate pair of eigenvalues.\n",
        "            # The block in J is a 2x2 real matrix.\n",
        "            # J_block = [[a, b], [-b, a]] for eigenvalue a + bi.\n",
        "            J[col_ptr, col_ptr] = eigval.real\n",
        "            J[col_ptr, col_ptr + 1] = eigval.imag\n",
        "            J[col_ptr + 1, col_ptr] = -eigval.imag\n",
        "            J[col_ptr + 1, col_ptr + 1] = eigval.real\n",
        "\n",
        "            # The corresponding two columns of A are the real and imaginary\n",
        "            # parts of the complex eigenvector.\n",
        "            A[:, col_ptr] = eigvec.real\n",
        "            A[:, col_ptr + 1] = eigvec.imag\n",
        "\n",
        "            # Skip the next eigenvalue, as it's the conjugate pair.\n",
        "            i += 2\n",
        "            # Move the column pointer by two.\n",
        "            col_ptr += 2\n",
        "\n",
        "    # --- Task 4, Step 5: Matrix Inversion and Validation ---\n",
        "    try:\n",
        "        # Compute the inverse of the transformation matrix A.\n",
        "        A_inv = np.linalg.inv(A)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        # If A is singular, the decomposition is invalid.\n",
        "        raise np.linalg.LinAlgError(\n",
        "            \"Constructed transformation matrix A is singular.\"\n",
        "        ) from e\n",
        "\n",
        "    # Validate the decomposition by checking if Psi is close to A * J * A_inv.\n",
        "    reconstructed_psi = A @ J @ A_inv\n",
        "    # The Frobenius norm of the difference should be close to zero.\n",
        "    reconstruction_error = np.linalg.norm(psi_matrix - reconstructed_psi)\n",
        "    if not np.isclose(reconstruction_error, 0.0, atol=1e-8):\n",
        "        # Issue a warning if the decomposition is not numerically accurate.\n",
        "        print(\n",
        "            f\"Warning: Jordan decomposition reconstruction error is \"\n",
        "            f\"{reconstruction_error:.2e}. Results may be inaccurate.\"\n",
        "        )\n",
        "\n",
        "    # Package the results into a dictionary for clean output.\n",
        "    decomposition_results = {\n",
        "        'J': J,\n",
        "        'A': A,\n",
        "        'A_inv': A_inv,\n",
        "        'n1': n1,\n",
        "        'n2': n2,\n",
        "        'eigenvalues_sorted': eigenvalues_sorted\n",
        "    }\n",
        "\n",
        "    return decomposition_results\n"
      ],
      "metadata": {
        "id": "-AKDt79hxbBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Estimate Error Density and Noncausal Component Density\n",
        "\n",
        "def estimate_functional_components(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[gaussian_kde, gaussian_kde]:\n",
        "    \"\"\"\n",
        "    Estimates key functional components: error and non-causal state densities.\n",
        "\n",
        "    This function takes the estimated VAR model parameters and computes the two\n",
        "    time series essential for constructing the predictive density:\n",
        "    1.  Model Residuals (epsilon_hat): The one-step-ahead prediction errors.\n",
        "    2.  Filtered Non-Causal States (Z2_hat): The historical values of the\n",
        "        latent non-causal component of the system.\n",
        "\n",
        "    It then applies multivariate Kernel Density Estimation (KDE) to these\n",
        "    series to obtain non-parametric estimates of their respective probability\n",
        "    density functions, g(epsilon) and l2(z2).\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The prepared, demeaned time series data\n",
        "                                    from `prepare_var_data`.\n",
        "        phi_estimated (np.ndarray): The estimated VAR coefficient matrix,\n",
        "                                    stacked as [Phi_1, ..., Phi_p], with\n",
        "                                    shape (m, m * p).\n",
        "        decomposition_results (Dict[str, Any]): The dictionary containing the\n",
        "                                                Jordan decomposition results\n",
        "                                                from `compute_jordan_decomposition`.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the bandwidth method is unsupported or if a data\n",
        "                    series is degenerate (zero standard deviation).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[gaussian_kde, gaussian_kde]: A tuple containing two fitted\n",
        "                                           scipy.stats.gaussian_kde objects:\n",
        "                                           (g_hat, l2_hat).\n",
        "                                           - g_hat: The estimated density of\n",
        "                                             the error term.\n",
        "                                           - l2_hat: The estimated density of\n",
        "                                             the non-causal state component.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Extract model dimensions and parameters.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    data_np = prepared_df.to_numpy()\n",
        "    T, m = data_np.shape\n",
        "\n",
        "    # Extract decomposition components.\n",
        "    A_inv = decomposition_results['A_inv']\n",
        "    n1 = decomposition_results['n1']\n",
        "    n2 = decomposition_results['n2']\n",
        "    n = n1 + n2\n",
        "\n",
        "    # --- Task 5, Step 1: Residual Computation ---\n",
        "    # Equation: \\hat{\\epsilon}_t = Y_t - \\sum_{j=1}^{p} \\hat{\\Phi}_j Y_{t-j}\n",
        "    # Use the helper to create the standard VAR design matrices.\n",
        "    Y, X = _create_var_design_matrix(data_np, p_lags)\n",
        "    # Compute residuals in a single vectorized operation.\n",
        "    residuals = Y - X @ phi_estimated.T\n",
        "\n",
        "    # --- Task 5, Step 2: Non-causal Component Extraction ---\n",
        "    # Equation: \\hat{Z}_t = \\hat{A}^{-1} \\tilde{Y}_t\n",
        "    # We need the full state vector \\tilde{Y}_t = [Y_t', ..., Y_{t-p+1}']'\n",
        "    # This requires all T observations, not just the T-p for residuals.\n",
        "    # Initialize a matrix for the stacked state vectors.\n",
        "    Y_tilde = np.zeros((T - p_lags + 1, m * p_lags))\n",
        "    # Populate the matrix with stacked lagged data.\n",
        "    for t in range(T - p_lags + 1):\n",
        "        # The state at time t+p-1 is [Y_{t+p-1}', ..., Y_t']'\n",
        "        Y_tilde[t, :] = data_np[t : t + p_lags, :][::-1, :].ravel()\n",
        "\n",
        "    # Filter the full state vector Z_t using the inverse transformation matrix.\n",
        "    # Z_t' = \\tilde{Y}_t' * (A_inv)'  =>  Z_t = A_inv * \\tilde{Y}_t\n",
        "    Z_filtered = Y_tilde @ A_inv.T\n",
        "\n",
        "    # The non-causal component Z2_t corresponds to the last n2 columns of Z_t,\n",
        "    # because we sorted eigenvalues with causal first.\n",
        "    Z2_filtered = Z_filtered[:, n1:]\n",
        "\n",
        "    # --- Task 5, Step 3: Bandwidth Selection Implementation ---\n",
        "    # Extract the specified bandwidth selection method.\n",
        "    bw_method = study_params[\"nonparametric_methods\"][\"kde\"][\"bandwidth_method\"]\n",
        "\n",
        "    if bw_method == \"stdev_rule\":\n",
        "        # As per the paper's application (p.30), use component-wise standard deviations.\n",
        "        # This is a simple, robust rule of thumb.\n",
        "        # Bandwidth for the m-dimensional residual density g.\n",
        "        bw_g = np.std(residuals, axis=0)\n",
        "        # Bandwidth for the n2-dimensional non-causal state density l2.\n",
        "        bw_l2 = np.std(Z2_filtered, axis=0)\n",
        "\n",
        "        # Check for degenerate distributions.\n",
        "        if np.any(np.isclose(bw_g, 0.0)) or np.any(np.isclose(bw_l2, 0.0)):\n",
        "            raise ValueError(\n",
        "                \"Data component has zero standard deviation. \"\n",
        "                \"Cannot perform KDE on a degenerate distribution.\"\n",
        "            )\n",
        "    else:\n",
        "        # Raise an error if an unsupported bandwidth method is specified.\n",
        "        raise ValueError(f\"Unsupported bandwidth method: {bw_method}\")\n",
        "\n",
        "    # --- Task 5, Step 4: Multivariate Kernel Density Estimation ---\n",
        "    # Scipy's gaussian_kde requires data of shape (dims, n_samples).\n",
        "    # We must transpose our data arrays.\n",
        "\n",
        "    # Fit the KDE for the error density, g_hat.\n",
        "    # The input `residuals` has shape (T-p, m). Transpose to (m, T-p).\n",
        "    if residuals.shape[0] > 0:\n",
        "        g_hat = gaussian_kde(residuals.T)\n",
        "        # Manually set the bandwidth based on our rule.\n",
        "        # We adjust the default covariance factor by our desired bandwidths.\n",
        "        g_hat.set_bandwidth(bw_method=g_hat.scotts_factor() * bw_g)\n",
        "    else:\n",
        "        raise ValueError(\"Not enough data points to estimate residual density.\")\n",
        "\n",
        "    # Fit the KDE for the non-causal state density, l2_hat.\n",
        "    # The input `Z2_filtered` has shape (T-p+1, n2). Transpose to (n2, T-p+1).\n",
        "    if Z2_filtered.shape[0] > 0 and n2 > 0:\n",
        "        l2_hat = gaussian_kde(Z2_filtered.T)\n",
        "        # Manually set the bandwidth for the non-causal state density.\n",
        "        l2_hat.set_bandwidth(bw_method=l2_hat.scotts_factor() * bw_l2)\n",
        "    elif n2 == 0:\n",
        "        # If there is no non-causal component, return a placeholder.\n",
        "        # A lambda that always returns 1.0 is appropriate as l2 cancels out.\n",
        "        l2_hat = lambda x: 1.0\n",
        "    else:\n",
        "        raise ValueError(\"Not enough data points to estimate non-causal state density.\")\n",
        "\n",
        "    # --- Task 5, Step 5: Return Fitted KDE Objects ---\n",
        "    # The returned objects are callable and represent the estimated densities.\n",
        "    return g_hat, l2_hat\n"
      ],
      "metadata": {
        "id": "Tdm8UXKzyJWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Implement the Predictive Density Formula\n",
        "\n",
        "def compute_predictive_density(\n",
        "    y_candidates: np.ndarray,\n",
        "    history_df: pd.DataFrame,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    g_hat: gaussian_kde,\n",
        "    l2_hat: Union[gaussian_kde, Callable[[Any], float]],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the one-step-ahead predictive density l(y|Y_T).\n",
        "\n",
        "    This function implements the central formula (Eq. 3.1) of the paper,\n",
        "    calculating the probability density of a future value `y` given the\n",
        "    observed history `Y_T`. It combines the estimated parametric components\n",
        "    (Phi, J) and non-parametric components (g_hat, l2_hat) to produce a\n",
        "    state-dependent, non-Gaussian predictive density.\n",
        "\n",
        "    The computation is performed in log-space for numerical stability.\n",
        "    log l(y|Y_T) = log(l2(num)) - log(l2(den)) + log|det(J2)| + log(g(err))\n",
        "\n",
        "    The function is vectorized to efficiently evaluate the density over a\n",
        "    grid of candidate `y` values.\n",
        "\n",
        "    Args:\n",
        "        y_candidates (np.ndarray): An array of candidate future values `y` at\n",
        "                                   which to evaluate the density.\n",
        "                                   Shape: (N, m), where N is the number of\n",
        "                                   points and m is the number of variables.\n",
        "        history_df (pd.DataFrame): A DataFrame containing the historical data,\n",
        "                                   with the most recent observation at the\n",
        "                                   bottom. Must contain at least `p_lags` rows.\n",
        "        phi_estimated (np.ndarray): The estimated VAR coefficient matrix,\n",
        "                                    stacked as [Phi_1, ..., Phi_p].\n",
        "                                    Shape: (m, m * p).\n",
        "        decomposition_results (Dict[str, Any]): The dictionary from\n",
        "                                                `compute_jordan_decomposition`.\n",
        "        g_hat (gaussian_kde): The fitted KDE for the error density.\n",
        "        l2_hat (Union[gaussian_kde, Callable]): The fitted KDE for the non-causal\n",
        "                                                state density. Can be a\n",
        "                                                placeholder if n2=0.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the history DataFrame is too short or if input\n",
        "                    dimensions are inconsistent.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of density values corresponding to each row in\n",
        "                    `y_candidates`. Shape: (N,).\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Extract model dimensions and parameters.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    m = phi_estimated.shape[0]\n",
        "\n",
        "    # Ensure history is long enough to construct lagged regressors.\n",
        "    if len(history_df) < p_lags:\n",
        "        raise ValueError(f\"History must contain at least p_lags={p_lags} observations.\")\n",
        "\n",
        "    # Ensure candidate `y` has the correct number of variables (m).\n",
        "    if y_candidates.ndim != 2 or y_candidates.shape[1] != m:\n",
        "        raise ValueError(f\"y_candidates must have shape (N, {m}).\")\n",
        "\n",
        "    # Extract necessary components from the decomposition results.\n",
        "    J = decomposition_results['J']\n",
        "    A_inv = decomposition_results['A_inv']\n",
        "    n1 = decomposition_results['n1']\n",
        "    n2 = decomposition_results['n2']\n",
        "\n",
        "    # --- Handle the Purely Causal Case (n2 = 0) ---\n",
        "    if n2 == 0:\n",
        "        # If the model is purely causal, the predictive density simplifies.\n",
        "        # The l2 ratio and det(J2) terms disappear.\n",
        "        # l(y|Y_T) = g(y - Phi * Y_{T-1})\n",
        "\n",
        "        # Get the most recent `p_lags` observations for prediction.\n",
        "        Y_T_hist = history_df.iloc[-p_lags:].to_numpy()\n",
        "        # The regressor vector X_T = [Y_T', Y_{T-1}', ...]'\n",
        "        X_T = Y_T_hist[::-1, :].ravel().reshape(1, -1)\n",
        "\n",
        "        # The argument for g is the forecast error.\n",
        "        # error = y_candidate - X_T @ Phi'\n",
        "        error_term = y_candidates - X_T @ phi_estimated.T\n",
        "\n",
        "        # Return the density from the error distribution g_hat.\n",
        "        # The KDE object expects shape (m, N), so we transpose.\n",
        "        return g_hat.pdf(error_term.T)\n",
        "\n",
        "    # --- Full Mixed Causal-Noncausal Case (n2 > 0) ---\n",
        "    # --- Step 1: Compute the log|det(J2)| term ---\n",
        "    # Extract the non-causal block J2 from the Jordan matrix J.\n",
        "    J2 = J[n1:, n1:]\n",
        "    # Calculate the log of the absolute value of its determinant.\n",
        "    log_det_J2 = np.log(np.abs(np.linalg.det(J2)))\n",
        "\n",
        "    # --- Step 2: Compute the log(g(error)) term ---\n",
        "    # Get the most recent `p_lags` observations.\n",
        "    Y_T_hist = history_df.iloc[-p_lags:].to_numpy()\n",
        "    # Construct the regressor vector X_T for the forecast.\n",
        "    X_T = Y_T_hist[::-1, :].ravel().reshape(1, -1)\n",
        "    # Calculate the error term for all candidate points.\n",
        "    error_term = y_candidates - X_T @ phi_estimated.T\n",
        "    # Evaluate the log-pdf of the error term using the g_hat KDE.\n",
        "    # Transpose is required for the KDE's expected input shape (m, N).\n",
        "    log_g_val = g_hat.logpdf(error_term.T)\n",
        "\n",
        "    # --- Step 3: Compute the log(l2) ratio term ---\n",
        "    # This requires transforming both candidate and historical Y into Z2 states.\n",
        "\n",
        "    # a) Denominator term: l2(Z2_T)\n",
        "    # The state vector at time T is \\tilde{Y}_T = [Y_T', ..., Y_{T-p+1}']'\n",
        "    Y_tilde_T = Y_T_hist[::-1, :].ravel()\n",
        "    # Transform to the full state Z_T = A_inv * \\tilde{Y}_T\n",
        "    Z_T = A_inv @ Y_tilde_T\n",
        "    # Extract the non-causal part Z2_T.\n",
        "    Z2_T = Z_T[n1:]\n",
        "    # Evaluate the log-pdf for the denominator.\n",
        "    # The KDE expects shape (n2, 1), so we reshape.\n",
        "    log_l2_den = l2_hat.logpdf(Z2_T.reshape(-1, 1))\n",
        "\n",
        "    # b) Numerator term: l2(Z2_{T+1})\n",
        "    # The state vector at T+1 is \\tilde{Y}_{T+1} = [y', Y_T', ..., Y_{T-p+2}']'\n",
        "    # We need to construct this for all N candidates.\n",
        "    num_candidates = y_candidates.shape[0]\n",
        "    # The historical part of the state is common to all candidates.\n",
        "    hist_part_state = Y_T_hist[:-1, :][::-1, :].ravel() if p_lags > 1 else np.array([])\n",
        "    # Create N copies of the historical part.\n",
        "    hist_part_tiled = np.tile(hist_part_state, (num_candidates, 1))\n",
        "    # Concatenate candidate `y` with the history to form all \\tilde{Y}_{T+1}.\n",
        "    Y_tilde_Tplus1 = np.concatenate([y_candidates, hist_part_tiled], axis=1)\n",
        "\n",
        "    # Transform all candidate states to Z-space.\n",
        "    Z_Tplus1_candidates = Y_tilde_Tplus1 @ A_inv.T\n",
        "    # Extract the non-causal parts Z2_{T+1}.\n",
        "    Z2_Tplus1_candidates = Z_Tplus1_candidates[:, n1:]\n",
        "    # Evaluate the log-pdf for the numerator term.\n",
        "    # Transpose is required for the KDE's expected input shape (n2, N).\n",
        "    log_l2_num = l2_hat.logpdf(Z2_Tplus1_candidates.T)\n",
        "\n",
        "    # --- Step 4: Assemble the final log-density ---\n",
        "    # log l(y|Y_T) = log(l2_num) - log(l2_den) + log|det(J2)| + log(g_val)\n",
        "    log_predictive_density = log_l2_num - log_l2_den + log_det_J2 + log_g_val\n",
        "\n",
        "    # Convert back from log-space to get the final density value.\n",
        "    predictive_density = np.exp(log_predictive_density)\n",
        "\n",
        "    return predictive_density\n"
      ],
      "metadata": {
        "id": "mkgaUcDvzBsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: Implement Point Forecasting\n",
        "\n",
        "def compute_point_forecast(\n",
        "    history_df: pd.DataFrame,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    g_hat: gaussian_kde,\n",
        "    l2_hat: Union[gaussian_kde, Callable[[Any], float]],\n",
        "    study_params: Dict[str, Any],\n",
        "    data_stdevs: pd.Series\n",
        ") -> Tuple[np.ndarray, List[np.ndarray], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Computes the one-step-ahead point forecast by finding the mode of the\n",
        "    predictive density.\n",
        "\n",
        "    This function operationalizes the predictive density by searching for its\n",
        "    maximum value over a discrete numerical grid. The coordinates of this\n",
        "    maximum serve as the point forecast.\n",
        "\n",
        "    The process involves three main steps:\n",
        "    1.  **Grid Construction**: A multi-dimensional grid of candidate future\n",
        "        values (`y`) is created. The grid is centered around the last\n",
        "        observed value and its extent is determined by the historical\n",
        "        standard deviation of the series.\n",
        "    2.  **Density Evaluation**: The `compute_predictive_density` function is\n",
        "        called to evaluate the density at every point on the grid.\n",
        "    3.  **Mode Identification**: The grid point with the highest density value\n",
        "        is identified. Its coordinates are the modal point forecast.\n",
        "\n",
        "    Args:\n",
        "        history_df (pd.DataFrame): Historical data, with the most recent\n",
        "                                   observation at the bottom.\n",
        "        phi_estimated (np.ndarray): Estimated VAR coefficient matrix.\n",
        "        decomposition_results (Dict[str, Any]): Results from the Jordan\n",
        "                                                decomposition.\n",
        "        g_hat (gaussian_kde): Fitted KDE for the error density.\n",
        "        l2_hat (Union[gaussian_kde, Callable]): Fitted KDE for the non-causal\n",
        "                                                state density.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "        data_stdevs (pd.Series): The standard deviations of the original\n",
        "                                 (prepared) series, used to set grid bounds.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the number of variables `m` is greater than 2, as the\n",
        "                    grid search is implemented only for the bivariate case.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, List[np.ndarray], np.ndarray]:\n",
        "        - `point_forecast`: The modal forecast, an array of shape (m,).\n",
        "        - `grid_vectors`: A list of 1D arrays representing the axes of the\n",
        "                          evaluation grid.\n",
        "        - `density_on_grid`: A matrix of density values evaluated on the grid.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Extract model dimensions and parameters.\n",
        "    m = phi_estimated.shape[0]\n",
        "    grid_points_per_dim = study_params[\"forecasting\"][\"simulation\"][\"grid_points_per_dim\"]\n",
        "\n",
        "    # This implementation is specialized for the bivariate (m=2) case as in the paper.\n",
        "    if m != 2:\n",
        "        raise ValueError(\n",
        "            \"Point forecast via grid search is implemented for the bivariate (m=2) case only.\"\n",
        "        )\n",
        "\n",
        "    # --- Task 7, Step 1: Grid Construction ---\n",
        "    # Get the last observed data point, which will be the center of our grid.\n",
        "    last_observation = history_df.iloc[-1].to_numpy()\n",
        "\n",
        "    # Define the grid for each dimension.\n",
        "    # The grid is centered at the last observation and extends by a multiple\n",
        "    # of the standard deviation to ensure it covers the bulk of the density.\n",
        "    grid_stdev_multiplier = 4.0\n",
        "    grid_vectors = []\n",
        "    for i in range(m):\n",
        "        center = last_observation[i]\n",
        "        spread = data_stdevs.iloc[i] * grid_stdev_multiplier\n",
        "        # Create a 1D vector of points for this dimension's axis.\n",
        "        axis_vector = np.linspace(\n",
        "            center - spread, center + spread, grid_points_per_dim\n",
        "        )\n",
        "        grid_vectors.append(axis_vector)\n",
        "\n",
        "    # Use np.meshgrid to create coordinate matrices from the axis vectors.\n",
        "    # For m=2, grid_coords[0] is the x-grid, grid_coords[1] is the y-grid.\n",
        "    grid_coords = np.meshgrid(*grid_vectors)\n",
        "\n",
        "    # --- Task 7, Step 2: Density Evaluation on Grid ---\n",
        "    # To evaluate the density, we need a list of (x, y) points.\n",
        "    # We flatten the grid coordinate matrices and stack them.\n",
        "    # The result `y_candidates` has shape (grid_points^m, m).\n",
        "    y_candidates = np.stack([coords.ravel() for coords in grid_coords], axis=-1)\n",
        "\n",
        "    # Evaluate the predictive density at every point on the grid.\n",
        "    # This leverages the vectorized `compute_predictive_density` function.\n",
        "    density_values_flat = compute_predictive_density(\n",
        "        y_candidates=y_candidates,\n",
        "        history_df=history_df,\n",
        "        phi_estimated=phi_estimated,\n",
        "        decomposition_results=decomposition_results,\n",
        "        g_hat=g_hat,\n",
        "        l2_hat=l2_hat,\n",
        "        study_params=study_params\n",
        "    )\n",
        "\n",
        "    # Reshape the flat density values back into the grid's original shape.\n",
        "    density_on_grid = density_values_flat.reshape(grid_coords[0].shape)\n",
        "\n",
        "    # --- Task 7, Step 3: Mode Detection ---\n",
        "    # Find the index of the maximum value in the flattened density array.\n",
        "    max_density_flat_idx = np.argmax(density_on_grid)\n",
        "\n",
        "    # Convert the flat index into multi-dimensional grid coordinates (e.g., (row, col)).\n",
        "    max_density_grid_coords = np.unravel_index(\n",
        "        max_density_flat_idx, density_on_grid.shape\n",
        "    )\n",
        "\n",
        "    # Use the grid coordinates to find the corresponding values (the mode).\n",
        "    point_forecast = np.array(\n",
        "        [grid_vectors[i][max_density_grid_coords[i]] for i in range(m)]\n",
        "    )\n",
        "\n",
        "    # Check if the mode occurred on the boundary of the grid.\n",
        "    for i in range(m):\n",
        "        if max_density_grid_coords[i] == 0 or max_density_grid_coords[i] == grid_points_per_dim - 1:\n",
        "            print(\n",
        "                f\"Warning: Forecast mode for variable {i} was found on the \"\n",
        "                \"edge of the evaluation grid. The grid may be too small.\"\n",
        "            )\n",
        "\n",
        "    # Return the forecast and the grid data for potential reuse (e.g., plotting).\n",
        "    return point_forecast, grid_vectors, density_on_grid\n"
      ],
      "metadata": {
        "id": "2-Jib8cyz339"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Implement Prediction Interval Calculation\n",
        "\n",
        "def _find_quantile(\n",
        "    q: float,\n",
        "    grid_vector: np.ndarray,\n",
        "    cdf_values: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Finds the quantile for a given probability `q` from a numerical CDF.\n",
        "\n",
        "    This helper function uses linear interpolation to invert the cumulative\n",
        "    distribution function (CDF) that is defined numerically over a grid.\n",
        "\n",
        "    Args:\n",
        "        q (float): The target probability (e.g., 0.05 for the 5th percentile).\n",
        "        grid_vector (np.ndarray): The 1D array of values (the x-axis).\n",
        "        cdf_values (np.ndarray): The 1D array of corresponding CDF values\n",
        "                                 (the y-axis).\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated quantile value.\n",
        "    \"\"\"\n",
        "    # np.interp finds the value on the grid_vector that corresponds to the\n",
        "    # probability q on the cdf_values axis. It performs linear interpolation\n",
        "    # between the grid points. It is robust to non-monotonicity in cdf_values.\n",
        "    return float(np.interp(q, cdf_values, grid_vector))\n",
        "\n",
        "def compute_prediction_interval(\n",
        "    grid_vectors: List[np.ndarray],\n",
        "    density_on_grid: np.ndarray,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes prediction intervals from a grid-evaluated predictive density.\n",
        "\n",
        "    This function takes the output of `compute_point_forecast` (the grid and\n",
        "    the density values on it) and calculates the prediction interval for each\n",
        "    variable at a specified significance level alpha.\n",
        "\n",
        "    The process for each variable is:\n",
        "    1.  **Marginalization**: The joint density is numerically integrated over\n",
        "        all other variables to obtain the 1D marginal density.\n",
        "    2.  **Normalization**: The marginal density is re-normalized to ensure it\n",
        "        integrates to 1, correcting for any grid discretization errors.\n",
        "    3.  **CDF Calculation**: The marginal density is cumulatively integrated\n",
        "        to produce a numerical Cumulative Distribution Function (CDF).\n",
        "    4.  **Quantile Inversion**: The CDF is inverted via interpolation to find\n",
        "        the values corresponding to the lower (alpha/2) and upper\n",
        "        (1-alpha/2) probability bounds.\n",
        "\n",
        "    Args:\n",
        "        grid_vectors (List[np.ndarray]): A list of 1D arrays, where each\n",
        "                                         array defines the axis for one\n",
        "                                         dimension of the grid.\n",
        "        density_on_grid (np.ndarray): The matrix of density values evaluated\n",
        "                                      on the grid.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters, from\n",
        "                                       which `prediction_interval_alpha` is\n",
        "                                       extracted.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A 2D array of shape (m, 2) where each row contains the\n",
        "                    [lower_bound, upper_bound] of the prediction interval\n",
        "                    for the corresponding variable.\n",
        "    \"\"\"\n",
        "    # --- Input Validation and Setup ---\n",
        "    # Extract the significance level for the prediction interval.\n",
        "    alpha = study_params[\"forecasting\"][\"intervals\"][\"prediction_interval_alpha\"]\n",
        "    # Get the number of variables from the number of grid vectors.\n",
        "    m = len(grid_vectors)\n",
        "\n",
        "    # Define the target probability levels for the lower and upper bounds.\n",
        "    q_lower = alpha / 2.0\n",
        "    q_upper = 1.0 - (alpha / 2.0)\n",
        "\n",
        "    # Initialize a list to store the interval for each variable.\n",
        "    intervals = []\n",
        "\n",
        "    # --- Loop through each variable to compute its marginal interval ---\n",
        "    for i in range(m):\n",
        "        # --- Task 8, Step 1: Marginal Density Computation ---\n",
        "        # To get the marginal density for variable `i`, we integrate the\n",
        "        # joint density over all other variables.\n",
        "        # The axes to integrate over are all axes except `i`.\n",
        "        integration_axes = tuple(j for j in range(m) if j != i)\n",
        "\n",
        "        # Perform trapezoidal integration along the specified axes.\n",
        "        # This is more accurate than a simple sum.\n",
        "        # We start with the full density grid.\n",
        "        marginal_density = density_on_grid\n",
        "        # Iteratively integrate out each unwanted dimension.\n",
        "        for axis in sorted(integration_axes, reverse=True):\n",
        "            marginal_density = integrate.trapz(\n",
        "                marginal_density, x=grid_vectors[axis], axis=axis\n",
        "            )\n",
        "\n",
        "        # --- Task 8, Step 2: Normalization ---\n",
        "        # Due to grid discretization, the integral of the numerical marginal\n",
        "        # density might not be exactly 1. We re-normalize it.\n",
        "        # First, compute the total integral of the numerical density.\n",
        "        total_integral = integrate.trapz(marginal_density, x=grid_vectors[i])\n",
        "\n",
        "        # Avoid division by zero if the density is flat zero.\n",
        "        if np.isclose(total_integral, 0.0):\n",
        "            # If the integral is zero, we cannot compute an interval.\n",
        "            # This indicates a problem with the density estimation.\n",
        "            # We return a NaN interval as a failure signal.\n",
        "            print(f\"Warning: Marginal density for variable {i} has zero mass. Cannot compute interval.\")\n",
        "            intervals.append([np.nan, np.nan])\n",
        "            continue\n",
        "\n",
        "        # Divide by the total integral to ensure the area is 1.\n",
        "        marginal_density /= total_integral\n",
        "\n",
        "        # --- Task 8, Step 3: Cumulative Distribution Function (CDF) Construction ---\n",
        "        # Compute the CDF by cumulatively integrating the normalized marginal density.\n",
        "        # `cumtrapz` is the appropriate function for this. `initial=0` ensures\n",
        "        # the output array has the same length as the input.\n",
        "        cdf_values = integrate.cumtrapz(\n",
        "            marginal_density, x=grid_vectors[i], initial=0.0\n",
        "        )\n",
        "\n",
        "        # --- Task 8, Step 4: Quantile Computation and Interval Construction ---\n",
        "        # Find the lower and upper bounds by inverting the CDF at the\n",
        "        # target probability levels using our helper function.\n",
        "        lower_bound = _find_quantile(q_lower, grid_vectors[i], cdf_values)\n",
        "        upper_bound = _find_quantile(q_upper, grid_vectors[i], cdf_values)\n",
        "\n",
        "        # --- Task 8, Step 5: Interval Validation ---\n",
        "        # A basic sanity check.\n",
        "        if lower_bound > upper_bound:\n",
        "            print(\n",
        "                f\"Warning: Lower bound {lower_bound:.4f} is greater than \"\n",
        "                f\"upper bound {upper_bound:.4f} for variable {i}. \"\n",
        "                \"This may indicate issues with the density shape.\"\n",
        "            )\n",
        "            # Swap them to maintain logical consistency.\n",
        "            lower_bound, upper_bound = upper_bound, lower_bound\n",
        "\n",
        "        # Append the calculated interval to the list of results.\n",
        "        intervals.append([lower_bound, upper_bound])\n",
        "\n",
        "    # Convert the list of intervals into a single NumPy array.\n",
        "    return np.array(intervals)\n"
      ],
      "metadata": {
        "id": "jYNH0bK10c23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Implement Backward Bootstrap Procedure\n",
        "\n",
        "def _estimate_causal_density(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Union[gaussian_kde, Callable[[Any], float]]:\n",
        "    \"\"\"\n",
        "    Estimates the probability density of the causal state component, l1(z1).\n",
        "\n",
        "    This is a helper for the backward bootstrap procedure. It mirrors the\n",
        "    non-causal density estimation from Task 5.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The prepared, demeaned time series data.\n",
        "        decomposition_results (Dict[str, Any]): Jordan decomposition results.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        A fitted scipy.stats.gaussian_kde object for the causal density, or\n",
        "        a placeholder function if there are no causal components.\n",
        "    \"\"\"\n",
        "    # Extract parameters and decomposition results\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    data_np = prepared_df.to_numpy()\n",
        "    T, m = data_np.shape\n",
        "    A_inv = decomposition_results['A_inv']\n",
        "    n1 = decomposition_results['n1']\n",
        "\n",
        "    # If there are no causal components, return a placeholder.\n",
        "    if n1 == 0:\n",
        "        return lambda x: 1.0\n",
        "\n",
        "    # Construct the stacked state vectors \\tilde{Y}_t\n",
        "    Y_tilde = np.zeros((T - p_lags + 1, m * p_lags))\n",
        "    for t in range(T - p_lags + 1):\n",
        "        Y_tilde[t, :] = data_np[t: t + p_lags, :][::-1, :].ravel()\n",
        "\n",
        "    # Filter the full state vector Z_t\n",
        "    Z_filtered = Y_tilde @ A_inv.T\n",
        "\n",
        "    # The causal component Z1_t corresponds to the first n1 columns.\n",
        "    Z1_filtered = Z_filtered[:, :n1]\n",
        "\n",
        "    # Use the standard deviation rule for bandwidth selection.\n",
        "    bw_l1 = np.std(Z1_filtered, axis=0)\n",
        "    if np.any(np.isclose(bw_l1, 0.0)):\n",
        "        raise ValueError(\"Causal state component has zero standard deviation.\")\n",
        "\n",
        "    # Fit and return the KDE object.\n",
        "    l1_hat = gaussian_kde(Z1_filtered.T)\n",
        "    l1_hat.set_bandwidth(bw_method=l1_hat.scotts_factor() * bw_l1)\n",
        "\n",
        "    return l1_hat\n",
        "\n",
        "def _compute_backward_density(\n",
        "    y_candidates: np.ndarray,\n",
        "    history_block: np.ndarray,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    g_hat: gaussian_kde,\n",
        "    l1_hat: Union[gaussian_kde, Callable[[Any], float]],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the one-step-backward predictive density l_B(y_{T-1}|Y_T, ...).\n",
        "\n",
        "    This function implements the formula from Corollary 2, generalized for a\n",
        "    VAR(p) process. It is the engine for the backcasting simulation. The\n",
        "    logic is symmetric to the forward predictive density, but it relies on the\n",
        "    density of the causal state `l1` and the causal Jordan block `J1`.\n",
        "\n",
        "    The computation is performed in log-space for numerical stability.\n",
        "    log l_B = log(l1(num)) - log(l1(den)) + log|det(J1)| + log(g(err))\n",
        "\n",
        "    Args:\n",
        "        y_candidates (np.ndarray): Grid of candidate values for y_{T-1}.\n",
        "                                   Shape: (N, m).\n",
        "        history_block (np.ndarray): The conditioning history of `p` most\n",
        "                                    recent observations, [Y_T, ..., Y_{T-p+1}].\n",
        "                                    Shape: (p, m).\n",
        "        phi_estimated (np.ndarray): Estimated VAR coefficients [Phi_1, ..., Phi_p].\n",
        "        decomposition_results (Dict[str, Any]): Jordan decomposition results.\n",
        "        g_hat (gaussian_kde): Fitted KDE for the error density.\n",
        "        l1_hat (Union[gaussian_kde, Callable]): Fitted KDE for the causal state.\n",
        "        study_params (Dict[str, Any]): The study parameters.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of density values for each candidate. Shape (N,).\n",
        "    \"\"\"\n",
        "    # --- 1. Setup ---\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    m = phi_estimated.shape[0]\n",
        "    J, A_inv = decomposition_results['J'], decomposition_results['A_inv']\n",
        "    n1 = decomposition_results['n1']\n",
        "\n",
        "    # --- Handle the Purely Non-Causal Case (n1 = 0) ---\n",
        "    if n1 == 0:\n",
        "        # If the model is purely non-causal, the density simplifies.\n",
        "        Y_T = history_block[0, :]\n",
        "        Y_hist_for_err = history_block[1:, :]\n",
        "        X_T_hist = Y_hist_for_err[::-1, :].ravel()\n",
        "\n",
        "        phi1 = phi_estimated[:, :m]\n",
        "        phi_rest = phi_estimated[:, m:]\n",
        "\n",
        "        error_term = Y_T - (y_candidates @ phi1.T + X_T_hist @ phi_rest.T)\n",
        "        return g_hat.pdf(error_term.T)\n",
        "\n",
        "    # --- 2. Compute log|det(J1)| and log(g(error)) terms ---\n",
        "    # Extract the causal block J1 from the Jordan matrix J.\n",
        "    J1 = J[:n1, :n1]\n",
        "    # Calculate the log of the absolute value of its determinant.\n",
        "    log_det_J1 = np.log(np.abs(np.linalg.det(J1)))\n",
        "\n",
        "    # The error term is \\epsilon_T = Y_T - \\sum \\Phi_j Y_{T-j}\n",
        "    Y_T = history_block[0, :]\n",
        "    # The candidate y_{T-1} is the first part of the regressor.\n",
        "    # The rest of the history Y_{T-2},... is fixed.\n",
        "    X_hist_fixed = history_block[1:, :][::-1, :].ravel()\n",
        "\n",
        "    num_candidates = y_candidates.shape[0]\n",
        "    X_hist_tiled = np.tile(X_hist_fixed, (num_candidates, 1))\n",
        "    # Form the full regressor matrix for all candidates.\n",
        "    X_candidates = np.concatenate([y_candidates, X_hist_tiled], axis=1)\n",
        "\n",
        "    # Calculate the error term for all candidates.\n",
        "    error_term = Y_T - X_candidates @ phi_estimated.T\n",
        "    # Evaluate the log-pdf of the error term.\n",
        "    log_g_val = g_hat.logpdf(error_term.T)\n",
        "\n",
        "    # --- 3. Compute the log(l1) ratio term ---\n",
        "    # Extract the block of A_inv corresponding to the causal states.\n",
        "    A1_block = A_inv[:n1, :]\n",
        "\n",
        "    # a) Denominator term: l1(Z1_T)\n",
        "    # The state vector at time T is \\tilde{Y}_T = [Y_T', ..., Y_{T-p+1}']'\n",
        "    Y_tilde_T = history_block[::-1, :].ravel()\n",
        "    # Transform to the causal state Z1_T = A1 * \\tilde{Y}_T\n",
        "    Z1_T = A1_block @ Y_tilde_T\n",
        "    # Evaluate the log-pdf for the denominator.\n",
        "    log_l1_den = l1_hat.logpdf(Z1_T.reshape(-1, 1))\n",
        "\n",
        "    # b) Numerator term: l1(Z1_{T-1})\n",
        "    # The state vector at T-1 is \\tilde{Y}_{T-1} = [y_{T-1}', Y_{T-1}', ...]'\n",
        "    hist_part_state = history_block[:-1, :][::-1, :].ravel()\n",
        "    hist_part_tiled = np.tile(hist_part_state, (num_candidates, 1))\n",
        "    # Concatenate candidate `y` with the history to form all \\tilde{Y}_{T-1}.\n",
        "    Y_tilde_Tminus1 = np.concatenate([y_candidates, hist_part_tiled], axis=1)\n",
        "\n",
        "    # Transform all candidate states to Z1-space.\n",
        "    Z1_Tminus1_candidates = Y_tilde_Tminus1 @ A1_block.T\n",
        "    # Evaluate the log-pdf for the numerator term.\n",
        "    log_l1_num = l1_hat.logpdf(Z1_Tminus1_candidates.T)\n",
        "\n",
        "    # --- 4. Assemble the final log-density ---\n",
        "    log_backward_density = log_l1_num - log_l1_den + log_det_J1 + log_g_val\n",
        "\n",
        "    # Convert back from log-space to get the final density value.\n",
        "    return np.exp(log_backward_density)\n",
        "\n",
        "def _sample_from_grid_sir(\n",
        "    grid_vectors: List[np.ndarray],\n",
        "    density_on_grid: np.ndarray,\n",
        "    num_candidates: int = 100\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Draws a single sample from a multivariate distribution on a grid using\n",
        "    Sampling Importance Resampling (SIR).\n",
        "\n",
        "    This function provides a robust method for sampling from a multivariate\n",
        "    density, correctly accounting for cross-sectional dependence. It replaces\n",
        "    the naive method of sampling from marginals independently.\n",
        "\n",
        "    The SIR algorithm proceeds in three steps:\n",
        "    1.  **Proposal (Sample):** A large number of candidate samples are drawn\n",
        "        from a simpler proposal distribution, which is the product of the\n",
        "        marginals (i.e., assuming independence).\n",
        "    2.  **Weighting:** Each candidate is assigned a weight proportional to the\n",
        "        ratio of the true joint density to the proposal density at that point.\n",
        "        This corrects for the flawed independence assumption.\n",
        "    3.  **Resampling:** A single, final sample is drawn from the candidates,\n",
        "        with selection probabilities given by their calculated weights.\n",
        "\n",
        "    Args:\n",
        "        grid_vectors (List[np.ndarray]): List of 1D arrays for grid axes.\n",
        "        density_on_grid (np.ndarray): Matrix of density values on the grid.\n",
        "        num_candidates (int): The number of candidate samples to draw in the\n",
        "                              proposal step.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A single, high-fidelity sample of shape (1, m).\n",
        "    \"\"\"\n",
        "    # --- 1. Proposal Step: Sample from Marginals ---\n",
        "    m = len(grid_vectors)\n",
        "    candidate_samples = np.zeros((num_candidates, m))\n",
        "    marginal_pdfs = []\n",
        "\n",
        "    # Generate all random numbers needed at once for efficiency.\n",
        "    random_uniforms = np.random.rand(num_candidates, m)\n",
        "\n",
        "    # First, compute all marginal densities and CDFs.\n",
        "    for i in range(m):\n",
        "        integration_axes = tuple(j for j in range(m) if j != i)\n",
        "        marginal_density = density_on_grid\n",
        "        for axis in sorted(integration_axes, reverse=True):\n",
        "            marginal_density = integrate.trapz(\n",
        "                marginal_density, x=grid_vectors[axis], axis=axis\n",
        "            )\n",
        "\n",
        "        total_integral = integrate.trapz(marginal_density, x=grid_vectors[i])\n",
        "        if np.isclose(total_integral, 0.0):\n",
        "            # If a marginal is zero, we cannot proceed.\n",
        "            raise ValueError(f\"Marginal density for variable {i} has zero mass.\")\n",
        "\n",
        "        marginal_density /= total_integral\n",
        "        marginal_pdfs.append(marginal_density)\n",
        "\n",
        "        cdf_values = integrate.cumtrapz(\n",
        "            marginal_density, x=grid_vectors[i], initial=0.0\n",
        "        )\n",
        "\n",
        "        # Draw `num_candidates` samples for this variable via inverse transform.\n",
        "        candidate_samples[:, i] = np.interp(\n",
        "            random_uniforms[:, i], cdf_values, grid_vectors[i]\n",
        "        )\n",
        "\n",
        "    # --- 2. Weighting Step ---\n",
        "    # a) Evaluate the true joint density at each candidate point.\n",
        "    # Since candidates are off-grid, we must interpolate.\n",
        "    # `interpn` is the tool for multi-dimensional interpolation.\n",
        "    target_density_vals = interpolate.interpn(\n",
        "        points=tuple(grid_vectors),\n",
        "        values=density_on_grid,\n",
        "        xi=candidate_samples,\n",
        "        method=\"linear\",\n",
        "        bounds_error=False,\n",
        "        fill_value=0.0 # Density is zero outside the grid\n",
        "    )\n",
        "\n",
        "    # b) Evaluate the proposal density (product of marginals) at each point.\n",
        "    proposal_density_vals = np.ones(num_candidates)\n",
        "    for i in range(m):\n",
        "        # Interpolate the 1D marginal PDF to get the density at the sample point.\n",
        "        marginal_vals_at_candidates = np.interp(\n",
        "            candidate_samples[:, i], grid_vectors[i], marginal_pdfs[i]\n",
        "        )\n",
        "        proposal_density_vals *= marginal_vals_at_candidates\n",
        "\n",
        "    # c) Compute the importance weights.\n",
        "    # weight = target_density / proposal_density\n",
        "    epsilon = 1e-12 # To prevent division by zero\n",
        "    weights = target_density_vals / (proposal_density_vals + epsilon)\n",
        "\n",
        "    # --- 3. Resampling Step ---\n",
        "    # Normalize the weights to form a probability distribution.\n",
        "    total_weight = np.sum(weights)\n",
        "    if np.isclose(total_weight, 0.0):\n",
        "        # If all weights are zero, fall back to a uniform choice.\n",
        "        print(\"Warning: All SIR weights are zero. Falling back to uniform resampling.\")\n",
        "        probabilities = np.ones(num_candidates) / num_candidates\n",
        "    else:\n",
        "        probabilities = weights / total_weight\n",
        "\n",
        "    # Choose one candidate index based on the calculated probabilities.\n",
        "    chosen_index = np.random.choice(num_candidates, p=probabilities)\n",
        "\n",
        "    # The final sample is the chosen candidate.\n",
        "    final_sample = candidate_samples[chosen_index, :].reshape(1, m)\n",
        "\n",
        "    return final_sample\n",
        "\n",
        "def _generate_one_bootstrap_path(\n",
        "    terminal_observation: np.ndarray,\n",
        "    path_length: int,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    g_hat: \"gaussian_kde\",\n",
        "    l1_hat: \"gaussian_kde\",\n",
        "    study_params: Dict[str, Any],\n",
        "    data_stdevs: pd.Series\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates a single, full-length synthetic data path via backcasting,\n",
        "    using the rigorous Sampling Importance Resampling (SIR) method.\n",
        "    \"\"\"\n",
        "    # This function is now amended to call the new, rigorous sampler.\n",
        "    m = terminal_observation.shape[0]\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "\n",
        "    # Pre-compute the inverse of the Phi matrix for centering the grid.\n",
        "    phi_stacked = phi_estimated.reshape((m * p_lags, m)).T\n",
        "    phi1_inv = np.linalg.inv(phi_stacked[:, :m])\n",
        "\n",
        "    path = np.zeros((path_length, m))\n",
        "    path[-1, :] = terminal_observation\n",
        "\n",
        "    backcast_grid_points = 150\n",
        "    grid_stdev_multiplier = 4.0\n",
        "\n",
        "    for t in range(path_length - 2, -1, -1):\n",
        "        # The conditioning history is the last `p` points we have generated.\n",
        "        history_block = path[t + 1 : t + 1 + p_lags, :]\n",
        "        Y_t_plus_1 = history_block[0, :]\n",
        "\n",
        "        # Use a model-informed guess to center the grid.\n",
        "        center_guess = phi1_inv @ Y_t_plus_1 # Simplified for VAR(1) centering\n",
        "\n",
        "        grid_vectors = []\n",
        "        for i in range(m):\n",
        "            spread = data_stdevs.iloc[i] * grid_stdev_multiplier\n",
        "            axis_vector = np.linspace(\n",
        "                center_guess[i] - spread, center_guess[i] + spread, backcast_grid_points\n",
        "            )\n",
        "            grid_vectors.append(axis_vector)\n",
        "\n",
        "        grid_coords = np.meshgrid(*grid_vectors)\n",
        "        y_candidates = np.stack([coords.ravel() for coords in grid_coords], axis=-1)\n",
        "\n",
        "        # Evaluate the full, generalized backward density on the grid.\n",
        "        density_on_grid_flat = _compute_backward_density(\n",
        "            y_candidates, history_block, phi_estimated, decomposition_results,\n",
        "            g_hat, l1_hat, study_params\n",
        "        )\n",
        "        density_on_grid = density_on_grid_flat.reshape(grid_coords[0].shape)\n",
        "\n",
        "        # AMENDED: Call the rigorous SIR sampler.\n",
        "        sampled_y_t = _sample_from_grid_sir(\n",
        "            grid_vectors, density_on_grid, num_candidates=100\n",
        "        )\n",
        "\n",
        "        path[t, :] = sampled_y_t.flatten()\n",
        "\n",
        "    return pd.DataFrame(path, columns=data_stdevs.index)\n",
        "\n",
        "def _bootstrap_single_run(\n",
        "    bootstrap_path_df: pd.DataFrame,\n",
        "    terminal_history_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> List[Tuple[float, float]]:\n",
        "    \"\"\"\n",
        "    Performs a single full estimation run for one bootstrap path and returns\n",
        "    the interval parameters (m_s, sigma_s) for ALL variables.\n",
        "    \"\"\"\n",
        "    # This is the amended version of the helper function.\n",
        "    try:\n",
        "        # Steps 1-3: Estimation pipeline\n",
        "        phi_s, _ = estimate_gcov_var(bootstrap_path_df, study_params)\n",
        "        decomp_s = compute_jordan_decomposition(phi_s, study_params)\n",
        "        g_s, l2_s = estimate_functional_components(\n",
        "            bootstrap_path_df, phi_s, decomp_s, study_params\n",
        "        )\n",
        "\n",
        "        # Step 4: Compute prediction interval\n",
        "        data_stdevs_s = bootstrap_path_df.std()\n",
        "        _, grid_vec_s, density_s = compute_point_forecast(\n",
        "            history_df=terminal_history_df, phi_estimated=phi_s,\n",
        "            decomposition_results=decomp_s, g_hat=g_s, l2_hat=l2_s,\n",
        "            study_params=study_params, data_stdevs=data_stdevs_s\n",
        "        )\n",
        "        pi_s = compute_prediction_interval(grid_vecs, density_s, study_params)\n",
        "\n",
        "        # Step 5: Decompose interval for EACH variable\n",
        "        m = pi_s.shape[0]\n",
        "        results = []\n",
        "        alpha1 = study_params['forecasting']['intervals']['prediction_interval_alpha']\n",
        "        phi_inv_alpha1 = norm.ppf(alpha1 / 2.0)\n",
        "\n",
        "        if np.isclose(phi_inv_alpha1, 0.0):\n",
        "            return [(np.nan, np.nan)] * m\n",
        "\n",
        "        for i in range(m):\n",
        "            pi_s_vari = pi_s[i, :]\n",
        "            m_s = 0.5 * (pi_s_vari[0] + pi_s_vari[1])\n",
        "            sigma_s = - (pi_s_vari[1] - pi_s_vari[0]) / (2.0 * phi_inv_alpha1)\n",
        "            results.append((m_s, sigma_s))\n",
        "\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        m = terminal_history_df.shape[1]\n",
        "        print(f\"Bootstrap run failed with error: {e}. Skipping path.\")\n",
        "        return [(np.nan, np.nan)] * m\n",
        "\n",
        "def _find_bracket_for_root(\n",
        "    func: Callable[[float], float],\n",
        "    initial_a: float = 0.5,\n",
        "    initial_b: float = 2.5,\n",
        "    max_iter: int = 10\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Helper to find a bracketing interval [a, b] where f(a) and f(b) have opposite signs.\"\"\"\n",
        "    a, b = initial_a, initial_b\n",
        "    fa, fb = func(a), func(b)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        if np.sign(fa) != np.sign(fb):\n",
        "            return a, b\n",
        "        # Expand the interval\n",
        "        a *= 0.8\n",
        "        b *= 1.5\n",
        "        fa, fb = func(a), func(b)\n",
        "\n",
        "    raise ValueError(\"Could not find a bracketing interval for the root.\")\n",
        "\n",
        "def compute_bootstrap_confidence_set(\n",
        "    history_df: pd.DataFrame,\n",
        "    phi_estimated: np.ndarray,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    g_hat: \"gaussian_kde\",\n",
        "    l2_hat: Union[\"gaussian_kde\", Callable[[Any], float]],\n",
        "    original_pi: np.ndarray,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Implements the backward bootstrap to compute a generalized, robust\n",
        "    confidence set for the prediction interval.\n",
        "\n",
        "    This function is:\n",
        "    - **Generalized**: It computes a confidence set for all `m` variables.\n",
        "    - **Robust**: It uses an adaptive search to find a bracketing interval\n",
        "      for the root-finding algorithm that calibrates the quantile `q`.\n",
        "    - **Methodologically Rigorous**: It relies on the amended backcasting and\n",
        "      SIR sampling functions for path generation.\n",
        "\n",
        "    Args:\n",
        "        (Same as before)\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]:\n",
        "        - `confidence_set`: The final calibrated confidence set, shape (m, 2).\n",
        "        - `calibrated_quantiles`: The calibrated multipliers `q_hat` for each\n",
        "                                  variable, shape (m,).\n",
        "    \"\"\"\n",
        "    # --- 1. Setup ---\n",
        "    print(\"Starting generalized backward bootstrap procedure...\")\n",
        "    S = study_params['forecasting']['simulation']['S_bootstrap_replications']\n",
        "    alpha1 = study_params['forecasting']['intervals']['prediction_interval_alpha']\n",
        "    alpha2 = study_params['forecasting']['intervals']['confidence_set_alpha']\n",
        "    m = history_df.shape[1]\n",
        "\n",
        "    # --- 2. Path Generation (unchanged) ---\n",
        "    print(\"Estimating causal density for backcasting...\")\n",
        "    l1_hat = _estimate_causal_density(history_df, decomposition_results, study_params)\n",
        "    print(f\"Generating {S} bootstrap paths via backcasting with SIR...\")\n",
        "    terminal_observation = history_df.iloc[-1].to_numpy()\n",
        "    path_length = len(history_df)\n",
        "    data_stdevs = history_df.std()\n",
        "    bootstrap_paths = Parallel(n_jobs=-1)(\n",
        "        delayed(_generate_one_bootstrap_path)(\n",
        "            terminal_observation, path_length, phi_estimated,\n",
        "            decomposition_results, g_hat, l1_hat, study_params, data_stdevs\n",
        "        ) for _ in range(S)\n",
        "    )\n",
        "\n",
        "    # --- 3. Re-estimation (amended for m variables) ---\n",
        "    print(f\"Re-estimating model on {S} bootstrap paths...\")\n",
        "    bootstrap_results = Parallel(n_jobs=-1)(\n",
        "        delayed(_bootstrap_single_run)(path, history_df, study_params)\n",
        "        for path in bootstrap_paths\n",
        "    )\n",
        "\n",
        "    # --- 4. Calibration and Set Construction (amended for m variables and robust root-finding) ---\n",
        "    print(\"Calibrating confidence set quantiles for all variables...\")\n",
        "    calibrated_quantiles = np.zeros(m)\n",
        "    confidence_set = np.zeros((m, 2))\n",
        "    phi_inv_alpha1 = norm.ppf(alpha1 / 2.0)\n",
        "\n",
        "    # Unpack results into a more usable format\n",
        "    # bootstrap_params_per_var will be a list of m lists, each containing S tuples\n",
        "    bootstrap_params_per_var = list(zip(*bootstrap_results))\n",
        "\n",
        "    for i in range(m):\n",
        "        # Decompose the original PI for variable `i`\n",
        "        pi_vari = original_pi[i, :]\n",
        "        m_hat = 0.5 * (pi_vari[0] + pi_vari[1])\n",
        "        sigma_hat = - (pi_vari[1] - pi_vari[0]) / (2.0 * phi_inv_alpha1)\n",
        "\n",
        "        # Get the bootstrap distribution for variable `i`\n",
        "        valid_params = [p for p in bootstrap_params_per_var[i] if not np.isnan(p[0])]\n",
        "        if len(valid_params) < S * 0.5:\n",
        "            print(f\"Warning: High failure rate for var {i}. Using default quantile.\")\n",
        "            calibrated_quantiles[i] = norm.ppf(1.0 - alpha2 / 2.0)\n",
        "        else:\n",
        "            m_s_dist = np.array([p[0] for p in valid_params])\n",
        "            sigma_s_dist = np.array([p[1] for p in valid_params])\n",
        "\n",
        "            def coverage_error(q: float) -> float:\n",
        "                lower_covered = (m_hat - q * sigma_hat) < (m_s_dist + phi_inv_alpha1 * sigma_s_dist)\n",
        "                upper_covered = (m_hat + q * sigma_hat) > (m_s_dist - phi_inv_alpha1 * sigma_s_dist)\n",
        "                return np.mean(lower_covered & upper_covered) - (1.0 - alpha2)\n",
        "\n",
        "            try:\n",
        "                # Find a robust bracket for the root\n",
        "                bracket = _find_bracket_for_root(coverage_error)\n",
        "                # Find the root within the bracket\n",
        "                calibrated_quantiles[i] = optimize.brentq(coverage_error, a=bracket[0], b=bracket[1])\n",
        "            except ValueError:\n",
        "                print(f\"Warning: Could not find root for var {i}. Using default quantile.\")\n",
        "                calibrated_quantiles[i] = norm.ppf(1.0 - alpha2 / 2.0)\n",
        "\n",
        "        # Construct the final confidence set for variable `i`\n",
        "        confidence_set[i, 0] = m_hat - calibrated_quantiles[i] * sigma_hat\n",
        "        confidence_set[i, 1] = m_hat + calibrated_quantiles[i] * sigma_hat\n",
        "\n",
        "    print(\"Bootstrap procedure complete.\")\n",
        "    return confidence_set, calibrated_quantiles\n"
      ],
      "metadata": {
        "id": "tmrmTJ9t6pRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Implement Nonlinear Innovation Filtering\n",
        "\n",
        "def _nadaraya_watson_cdf(\n",
        "    y_points: np.ndarray,\n",
        "    x_points: np.ndarray,\n",
        "    y_eval: np.ndarray,\n",
        "    x_eval: np.ndarray,\n",
        "    bandwidth: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Computes the conditional CDF F(y|x) using the Nadaraya-Watson estimator.\n",
        "\n",
        "    This is a vectorized implementation for potentially multivariate conditioning\n",
        "    variables `x`.\n",
        "\n",
        "    Args:\n",
        "        y_points (np.ndarray): The historical data for the target variable Y. Shape (T,).\n",
        "        x_points (np.ndarray): The historical data for the conditioning variable X. Shape (T, d).\n",
        "        y_eval (np.ndarray): The points `y` at which to evaluate the CDF. Shape (N,).\n",
        "        x_eval (np.ndarray): The points `x` at which to evaluate the CDF. Shape (N, d).\n",
        "        bandwidth (np.ndarray): The bandwidth vector for the kernel. Shape (d,).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: The estimated conditional CDF values F(y|x). Shape (N,).\n",
        "    \"\"\"\n",
        "    # Get dimensions\n",
        "    T = x_points.shape[0]\n",
        "    N = x_eval.shape[0]\n",
        "\n",
        "    # --- Numerator Calculation ---\n",
        "    # Indicator matrix: 1 if y_points[t] <= y_eval[i], 0 otherwise.\n",
        "    # Broadcasting y_points (T, 1) and y_eval (1, N) gives a (T, N) matrix.\n",
        "    indicator = (y_points[:, np.newaxis] <= y_eval[np.newaxis, :]).astype(float)\n",
        "\n",
        "    # --- Kernel Weight Calculation ---\n",
        "    # Use broadcasting to compute kernel weights for all (T, N) pairs.\n",
        "    # diff shape: (T, N, d)\n",
        "    diff = x_points[:, np.newaxis, :] - x_eval[np.newaxis, :, :]\n",
        "    # Standardize the differences by the bandwidth.\n",
        "    u = diff / bandwidth\n",
        "\n",
        "    # Multivariate Gaussian kernel (product of univariate kernels for diagonal bandwidth)\n",
        "    # kernel_vals shape: (T, N, d)\n",
        "    kernel_vals = norm.pdf(u)\n",
        "    # Product across dimensions `d` to get the final kernel weight for each pair.\n",
        "    # kernel_weights shape: (T, N)\n",
        "    kernel_weights = np.prod(kernel_vals, axis=2)\n",
        "\n",
        "    # --- Final CDF Calculation ---\n",
        "    # Numerator: Weighted sum of indicators.\n",
        "    numerator = np.sum(indicator * kernel_weights, axis=0)\n",
        "\n",
        "    # Denominator: Sum of weights.\n",
        "    denominator = np.sum(kernel_weights, axis=0)\n",
        "\n",
        "    # Add a small epsilon to the denominator to prevent division by zero.\n",
        "    epsilon = 1e-9\n",
        "    cdf_values = numerator / (denominator + epsilon)\n",
        "\n",
        "    # Handle cases where the denominator was zero (point is too far from data).\n",
        "    cdf_values[np.isclose(denominator, 0.0)] = np.nan\n",
        "\n",
        "    return cdf_values\n",
        "\n",
        "def filter_nonlinear_innovations(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Filters the nonlinear causal innovations v_t from the data.\n",
        "\n",
        "    This function implements the structural analysis part of the paper,\n",
        "    extracting the underlying i.i.d. standard normal shocks that drive the\n",
        "    system. It uses the Probability Integral Transform (PIT) based on\n",
        "    non-parametrically estimated conditional CDFs.\n",
        "\n",
        "    The procedure follows the recursive identification scheme from Section 5.3:\n",
        "    1.  The latent states Z_t are filtered from the observed data Y_t.\n",
        "    2.  The non-causal innovation `v_{2,t}` is filtered first using the\n",
        "        conditional CDF F(Z_{2,t} | Z_{2,t-1}).\n",
        "        Eq 5.5: v_{2,t} = Phi^{-1}[F_2(Z_{2,t}|Z_{t-1})]\n",
        "    3.  The causal innovation `v_{1,t}` is filtered second, conditioning on\n",
        "        both the past and the contemporaneous non-causal state, using\n",
        "        F(Z_{1,t} | Z_{2,t}, Z_{1,t-1}).\n",
        "        Eq 5.7: v_{1,t} = Phi^{-1}[F_{1|2}(Z_{1,t}|Z_{2,t},Z_{t-1})]\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The prepared, demeaned time series data.\n",
        "        decomposition_results (Dict[str, Any]): Jordan decomposition results.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing the time series of filtered\n",
        "                      nonlinear innovations `v_t`, with columns for each\n",
        "                      causal and non-causal component.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup and State Filtering ---\n",
        "    print(\"Filtering nonlinear innovations...\")\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    if p_lags != 1:\n",
        "        raise NotImplementedError(\"Innovation filtering is implemented for VAR(1) only.\")\n",
        "\n",
        "    data_np = prepared_df.to_numpy()\n",
        "    T, m = data_np.shape\n",
        "    A_inv = decomposition_results['A_inv']\n",
        "    n1, n2 = decomposition_results['n1'], decomposition_results['n2']\n",
        "\n",
        "    # Filter the full history of latent states Z_t = A_inv * Y_t\n",
        "    Z_filtered = data_np @ A_inv.T\n",
        "    Z1_filtered = Z_filtered[:, :n1]\n",
        "    Z2_filtered = Z_filtered[:, n1:]\n",
        "\n",
        "    # Initialize the innovations DataFrame.\n",
        "    innovations = pd.DataFrame(\n",
        "        index=prepared_df.index[1:],\n",
        "        columns=[f'v1_{i+1}' for i in range(n1)] + [f'v2_{i+1}' for i in range(n2)]\n",
        "    )\n",
        "\n",
        "    # --- 2. Filter Non-Causal Innovations v_{2,t} ---\n",
        "    print(\"Filtering non-causal innovations (v2)...\")\n",
        "    if n2 > 0:\n",
        "        # We need Z_{2,t} and Z_{2,t-1} for F(Z_{2,t} | Z_{2,t-1})\n",
        "        y_points_z2 = Z2_filtered[1:, :]\n",
        "        x_points_z2 = Z2_filtered[:-1, :]\n",
        "\n",
        "        # Use Scott's rule for bandwidth selection on the conditioning variable.\n",
        "        bw_z2 = y_points_z2.std(axis=0) * (y_points_z2.shape[0])**(-1./(y_points_z2.shape[1]+4))\n",
        "\n",
        "        # The Nadaraya-Watson estimator is called for each component of Z2.\n",
        "        for i in range(n2):\n",
        "            cdf_vals_z2 = _nadaraya_watson_cdf(\n",
        "                y_points=y_points_z2[:, i],\n",
        "                x_points=x_points_z2,\n",
        "                y_eval=y_points_z2[:, i],\n",
        "                x_eval=x_points_z2,\n",
        "                bandwidth=bw_z2\n",
        "            )\n",
        "\n",
        "            # Clip values to avoid -inf/inf from norm.ppf\n",
        "            epsilon = 1e-9\n",
        "            cdf_vals_z2 = np.clip(cdf_vals_z2, epsilon, 1 - epsilon)\n",
        "\n",
        "            # Apply the Probability Integral Transform.\n",
        "            innovations[f'v2_{i+1}'] = norm.ppf(cdf_vals_z2)\n",
        "\n",
        "    # --- 3. Filter Causal Innovations v_{1,t} ---\n",
        "    print(\"Filtering causal innovations (v1)...\")\n",
        "    if n1 > 0:\n",
        "        # We need Z_{1,t} and the conditioning set [Z_{2,t}, Z_{1,t-1}]\n",
        "        y_points_z1 = Z1_filtered[1:, :]\n",
        "        # Conditioning set X_t = [Z_{2,t}, Z_{1,t-1}]\n",
        "        x_points_z1 = np.hstack([Z2_filtered[1:, :], Z1_filtered[:-1, :]])\n",
        "\n",
        "        # Bandwidth selection for the multivariate conditioning set.\n",
        "        bw_z1 = x_points_z1.std(axis=0) * (x_points_z1.shape[0])**(-1./(x_points_z1.shape[1]+4))\n",
        "\n",
        "        for i in range(n1):\n",
        "            cdf_vals_z1 = _nadaraya_watson_cdf(\n",
        "                y_points=y_points_z1[:, i],\n",
        "                x_points=x_points_z1,\n",
        "                y_eval=y_points_z1[:, i],\n",
        "                x_eval=x_points_z1,\n",
        "                bandwidth=bw_z1\n",
        "            )\n",
        "\n",
        "            # Clip values for numerical stability.\n",
        "            epsilon = 1e-9\n",
        "            cdf_vals_z1 = np.clip(cdf_vals_z1, epsilon, 1 - epsilon)\n",
        "\n",
        "            # Apply the Probability Integral Transform.\n",
        "            innovations[f'v1_{i+1}'] = norm.ppf(cdf_vals_z1)\n",
        "\n",
        "    print(\"Innovation filtering complete.\")\n",
        "    return innovations.dropna()\n"
      ],
      "metadata": {
        "id": "uJ-TYgNa97dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Implement Impulse Response Function (IRF) Simulation\n",
        "\n",
        "def _conditional_quantile_function(\n",
        "    q: float,\n",
        "    x_cond: np.ndarray,\n",
        "    y_points: np.ndarray,\n",
        "    x_points: np.ndarray,\n",
        "    bandwidth: np.ndarray\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the conditional quantile F^{-1}(q|x) via root-finding.\n",
        "\n",
        "    This function inverts the Nadaraya-Watson conditional CDF estimator. It\n",
        "    finds the value `y` such that F(y|x) = q.\n",
        "\n",
        "    Args:\n",
        "        q (float): The target probability (must be in (0, 1)).\n",
        "        x_cond (np.ndarray): The conditioning point `x`. Shape (d,).\n",
        "        y_points (np.ndarray): Historical data for the target variable Y.\n",
        "        x_points (np.ndarray): Historical data for the conditioning variable X.\n",
        "        bandwidth (np.ndarray): Bandwidth for the kernel estimator.\n",
        "\n",
        "    Returns:\n",
        "        float: The estimated quantile.\n",
        "    \"\"\"\n",
        "    # Define the function whose root we want to find: F(y|x) - q = 0\n",
        "    def objective(y: float) -> float:\n",
        "        # Reshape inputs for the estimator\n",
        "        y_eval = np.array([y])\n",
        "        x_eval = x_cond.reshape(1, -1)\n",
        "\n",
        "        # Calculate the conditional CDF at this `y`\n",
        "        cdf_val = _nadaraya_watson_cdf(\n",
        "            y_points, x_points, y_eval, x_eval, bandwidth\n",
        "        )\n",
        "        return cdf_val[0] - q\n",
        "\n",
        "    # Define a reasonable search interval for the root-finder.\n",
        "    search_min = np.min(y_points) - np.std(y_points)\n",
        "    search_max = np.max(y_points) + np.std(y_points)\n",
        "\n",
        "    try:\n",
        "        # Use Brent's method for robust and efficient root-finding.\n",
        "        quantile = optimize.brentq(objective, a=search_min, b=search_max)\n",
        "        return quantile\n",
        "    except ValueError:\n",
        "        # If the objective function has the same sign at both ends of the\n",
        "        # interval, a root may not exist or the interval is wrong.\n",
        "        return np.nan\n",
        "\n",
        "def _simulate_one_path(\n",
        "    initial_state: np.ndarray,\n",
        "    innovations: np.ndarray,\n",
        "    Z1_hist: np.ndarray,\n",
        "    Z2_hist: np.ndarray,\n",
        "    bw_z1: np.ndarray,\n",
        "    bw_z2: np.ndarray\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Simulates a single future path of the state variables Z_t.\n",
        "    \"\"\"\n",
        "    H, n = innovations.shape\n",
        "    n1 = Z1_hist.shape[1]\n",
        "\n",
        "    # Initialize the path with the starting state.\n",
        "    path = np.zeros((H + 1, n))\n",
        "    path[0, :] = initial_state\n",
        "\n",
        "    for h in range(H):\n",
        "        # Current state [Z1_t, Z2_t]\n",
        "        current_z1, current_z2 = path[h, :n1], path[h, n1:]\n",
        "\n",
        "        # --- 1. Simulate Z2_{t+1} ---\n",
        "        v2_next = innovations[h, n1:]\n",
        "        q2 = norm.cdf(v2_next)\n",
        "        # Conditioning variable is Z2_t\n",
        "        x_cond_z2 = current_z2\n",
        "\n",
        "        z2_next = np.array([\n",
        "            _conditional_quantile_function(\n",
        "                q2[i], x_cond_z2, Z2_hist[:, i], Z2_hist[:-1, :], bw_z2\n",
        "            ) for i in range(len(v2_next))\n",
        "        ])\n",
        "\n",
        "        # --- 2. Simulate Z1_{t+1} ---\n",
        "        v1_next = innovations[h, :n1]\n",
        "        q1 = norm.cdf(v1_next)\n",
        "        # Conditioning variable is [Z2_{t+1}, Z1_t]\n",
        "        x_cond_z1 = np.concatenate([z2_next, current_z1])\n",
        "\n",
        "        z1_next = np.array([\n",
        "            _conditional_quantile_function(\n",
        "                q1[i], x_cond_z1, Z1_hist[:, i],\n",
        "                np.hstack([Z2_hist[1:,:], Z1_hist[:-1,:]]), bw_z1\n",
        "            ) for i in range(len(v1_next))\n",
        "        ])\n",
        "\n",
        "        # Store the new state.\n",
        "        path[h + 1, :] = np.concatenate([z1_next, z2_next])\n",
        "\n",
        "    return path\n",
        "\n",
        "def simulate_irf(\n",
        "    history_df: pd.DataFrame,\n",
        "    decomposition_results: Dict[str, Any],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[pd.DataFrame, Dict[float, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Simulates state-dependent Impulse Response Functions (IRFs).\n",
        "\n",
        "    This function performs a complete IRF simulation experiment:\n",
        "    1.  It establishes an initial state Z_T based on the provided history.\n",
        "    2.  It simulates a set of `n_baseline_sims` future paths using randomly\n",
        "        drawn standard normal innovations to create an average baseline path.\n",
        "    3.  For each specified shock size, it applies the shock to the first\n",
        "        non-causal innovation and re-simulates `n_baseline_sims` paths.\n",
        "    4.  The IRF is computed as the difference between the average shocked\n",
        "        path and the average baseline path.\n",
        "    5.  Results are transformed back into the space of the original variables.\n",
        "\n",
        "    Args:\n",
        "        history_df (pd.DataFrame): Historical data, with the most recent\n",
        "                                   observation `Y_T` at the bottom.\n",
        "        decomposition_results (Dict[str, Any]): Jordan decomposition results.\n",
        "        study_params (Dict[str, Any]): The dictionary of study parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[float, pd.DataFrame]]:\n",
        "        - `baseline_path_df`: A DataFrame of the averaged baseline path in\n",
        "                              the original Y-space.\n",
        "        - `irf_results`: A dictionary where keys are shock sizes (float) and\n",
        "                         values are DataFrames containing the IRFs.\n",
        "    \"\"\"\n",
        "    # --- 1. Setup and State Filtering ---\n",
        "    print(\"Starting IRF simulation...\")\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    if p_lags != 1:\n",
        "        raise NotImplementedError(\"IRF simulation is implemented for VAR(1) only.\")\n",
        "\n",
        "    H = study_params[\"structural_analysis\"][\"irf\"][\"H_horizon\"]\n",
        "    n_sims = study_params[\"structural_analysis\"][\"irf\"][\"n_baseline_sims\"]\n",
        "    shocks = study_params[\"structural_analysis\"][\"irf\"][\"delta_shock_sizes\"]\n",
        "\n",
        "    data_np = history_df.to_numpy()\n",
        "    A, A_inv = decomposition_results['A'], decomposition_results['A_inv']\n",
        "    n1, n2 = decomposition_results['n1'], decomposition_results['n2']\n",
        "    n = n1 + n2\n",
        "\n",
        "    # Filter the full history of latent states Z_t = A_inv * Y_t\n",
        "    Z_filtered = data_np @ A_inv.T\n",
        "    Z1_hist, Z2_hist = Z_filtered[:, :n1], Z_filtered[:, n1:]\n",
        "\n",
        "    # Get the initial state Z_T for the simulation.\n",
        "    initial_state_z = Z_filtered[-1, :]\n",
        "\n",
        "    # Calculate bandwidths for the conditional quantile functions.\n",
        "    x_points_z2 = Z2_hist[:-1, :]\n",
        "    bw_z2 = x_points_z2.std(axis=0) * (x_points_z2.shape[0])**(-1./(x_points_z2.shape[1]+4))\n",
        "    x_points_z1 = np.hstack([Z2_hist[1:,:], Z1_hist[:-1,:]])\n",
        "    bw_z1 = x_points_z1.std(axis=0) * (x_points_z1.shape[0])**(-1./(x_points_z1.shape[1]+4))\n",
        "\n",
        "    # --- 2. Generate Baseline Paths ---\n",
        "    print(f\"Simulating {n_sims} baseline paths...\")\n",
        "    # Generate all random innovations for all baseline simulations at once.\n",
        "    base_innovations = np.random.randn(n_sims, H, n)\n",
        "\n",
        "    # Simulate paths in parallel.\n",
        "    baseline_paths_z = Parallel(n_jobs=-1)(\n",
        "        delayed(_simulate_one_path)(\n",
        "            initial_state_z, base_innovations[i], Z1_hist, Z2_hist, bw_z1, bw_z2\n",
        "        ) for i in range(n_sims)\n",
        "    )\n",
        "    # Average the paths to get the final baseline.\n",
        "    avg_baseline_path_z = np.mean(np.array(baseline_paths_z), axis=0)\n",
        "\n",
        "    # --- 3. Generate Shocked Paths for Each Shock Size ---\n",
        "    irf_results_z = {}\n",
        "    for shock_val in shocks:\n",
        "        print(f\"Simulating {n_sims} paths for shock = {shock_val}...\")\n",
        "        shocked_innovations = base_innovations.copy()\n",
        "        # Apply the shock to the first non-causal innovation at the first step.\n",
        "        if n2 > 0:\n",
        "            shocked_innovations[:, 0, n1] += shock_val\n",
        "\n",
        "        shocked_paths_z = Parallel(n_jobs=-1)(\n",
        "            delayed(_simulate_one_path)(\n",
        "                initial_state_z, shocked_innovations[i], Z1_hist, Z2_hist, bw_z1, bw_z2\n",
        "            ) for i in range(n_sims)\n",
        "        )\n",
        "        avg_shocked_path_z = np.mean(np.array(shocked_paths_z), axis=0)\n",
        "\n",
        "        # --- 4. Compute IRF in Z-space ---\n",
        "        irf_results_z[shock_val] = avg_shocked_path_z - avg_baseline_path_z\n",
        "\n",
        "    # --- 5. Transform Results to Y-space ---\n",
        "    # Transform the baseline path: Y_t = Z_t * A'\n",
        "    baseline_path_y = avg_baseline_path_z @ A.T\n",
        "    baseline_path_df = pd.DataFrame(\n",
        "        baseline_path_y,\n",
        "        columns=history_df.columns,\n",
        "        index=pd.RangeIndex(start=0, stop=H + 1, name=\"Horizon\")\n",
        "    )\n",
        "\n",
        "    irf_results_y = {}\n",
        "    for shock_val, irf_z in irf_results_z.items():\n",
        "        irf_y = irf_z @ A.T\n",
        "        irf_df = pd.DataFrame(\n",
        "            irf_y,\n",
        "            columns=history_df.columns,\n",
        "            index=pd.RangeIndex(start=0, stop=H + 1, name=\"Horizon\")\n",
        "        )\n",
        "        irf_results_y[shock_val] = irf_df\n",
        "\n",
        "    print(\"IRF simulation complete.\")\n",
        "    return baseline_path_df, irf_results_y\n"
      ],
      "metadata": {
        "id": "2g0a8dTW_M9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Conduct Empirical Analysis\n",
        "\n",
        "def run_empirical_analysis(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any],\n",
        "    forecast_dates: Optional[List[pd.Timestamp]] = None,\n",
        "    irf_dates: Optional[List[pd.Timestamp]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a full, end-to-end empirical analysis using the mixed\n",
        "    causal-noncausal VAR framework.\n",
        "\n",
        "    This master function serves as the primary entry point for conducting a\n",
        "    study. It executes the entire pipeline of data preparation, model\n",
        "    estimation, forecasting, and structural analysis.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame): The raw input DataFrame with a monthly\n",
        "                               DatetimeIndex and two columns: \"real_oil_price\"\n",
        "                               and \"real_gdp\".\n",
        "        study_params (Dict[str, Any]): A nested dictionary of study parameters.\n",
        "        forecast_dates (Optional[List[pd.Timestamp]]): A list of dates at\n",
        "            which to generate one-step-ahead forecasts.\n",
        "        irf_dates (Optional[List[pd.Timestamp]]): A list of dates to use as\n",
        "            initial conditions for IRF simulations.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive, nested dictionary containing all\n",
        "                        results from the analysis. **Crucially, this includes\n",
        "                        the full raw and prepared DataFrames under the\n",
        "                        'inputs' key for robust downstream use.**\n",
        "    \"\"\"\n",
        "    # --- 1. Data Preparation and Validation ---\n",
        "    print(\"--- Step 1: Data Validation and Preparation ---\")\n",
        "    # Cleanse and resample the raw data.\n",
        "    quarterly_df = validate_and_cleanse_data(raw_df, study_params)\n",
        "    # Prepare data for VAR (growth rates, scaling, demeaning).\n",
        "    prepared_df, series_means = prepare_var_data(quarterly_df)\n",
        "    # Store the standard deviations for use in grid construction.\n",
        "    data_stdevs = prepared_df.std()\n",
        "\n",
        "    # --- 2. Core Model Estimation ---\n",
        "    print(\"\\n--- Step 2: Core Model Estimation ---\")\n",
        "    # Estimate VAR parameters using the GCov estimator.\n",
        "    print(\"Estimating VAR parameters via GCov...\")\n",
        "    phi_estimated, opt_result = estimate_gcov_var(prepared_df, study_params)\n",
        "\n",
        "    # Compute the Real Jordan Decomposition.\n",
        "    print(\"Computing Real Jordan Decomposition...\")\n",
        "    decomposition_results = compute_jordan_decomposition(phi_estimated, study_params)\n",
        "\n",
        "    # Estimate the non-parametric functional components (densities).\n",
        "    print(\"Estimating non-parametric densities (g_hat, l2_hat)...\")\n",
        "    g_hat, l2_hat = estimate_functional_components(\n",
        "        prepared_df, phi_estimated, decomposition_results, study_params\n",
        "    )\n",
        "\n",
        "    # --- 3. Diagnostic Data Generation ---\n",
        "    print(\"\\n--- Step 3: Generating Diagnostic Data ---\")\n",
        "    # Compute final model residuals for diagnostic checks.\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    Y, X = _create_var_design_matrix(prepared_df.to_numpy(), p_lags)\n",
        "    residuals = Y - X @ phi_estimated.T\n",
        "    residuals_df = pd.DataFrame(\n",
        "        residuals,\n",
        "        index=prepared_df.index[p_lags:],\n",
        "        columns=[f\"resid_{col}\" for col in prepared_df.columns]\n",
        "    )\n",
        "\n",
        "    # --- 4. Targeted Forecasting ---\n",
        "    forecast_results = {}\n",
        "    if forecast_dates:\n",
        "        print(\"\\n--- Step 4: Generating Targeted Forecasts ---\")\n",
        "        for date in forecast_dates:\n",
        "            print(f\"Generating forecast for date: {date.strftime('%Y-%m-%d')}\")\n",
        "            if date not in prepared_df.index:\n",
        "                print(f\"Warning: Date {date} not in sample. Skipping forecast.\")\n",
        "                continue\n",
        "\n",
        "            history_to_date = prepared_df.loc[:date]\n",
        "\n",
        "            point_fc, grid_vecs, density_grid = compute_point_forecast(\n",
        "                history_df=history_to_date,\n",
        "                phi_estimated=phi_estimated,\n",
        "                decomposition_results=decomposition_results,\n",
        "                g_hat=g_hat,\n",
        "                l2_hat=l2_hat,\n",
        "                study_params=study_params,\n",
        "                data_stdevs=data_stdevs\n",
        "            )\n",
        "\n",
        "            pred_interval = compute_prediction_interval(\n",
        "                grid_vectors=grid_vecs,\n",
        "                density_on_grid=density_grid,\n",
        "                study_params=study_params\n",
        "            )\n",
        "\n",
        "            forecast_results[date] = {\n",
        "                \"point_forecast\": point_fc,\n",
        "                \"prediction_interval\": pred_interval,\n",
        "                \"grid_vectors\": grid_vecs,\n",
        "                \"density_on_grid\": density_grid\n",
        "            }\n",
        "\n",
        "    # --- 5. State-Dependent IRF Simulation ---\n",
        "    irf_results = {}\n",
        "    if irf_dates:\n",
        "        print(\"\\n--- Step 5: Simulating State-Dependent IRFs ---\")\n",
        "        for date in irf_dates:\n",
        "            print(f\"Simulating IRF from initial state at: {date.strftime('%Y-%m-%d')}\")\n",
        "            if date not in prepared_df.index:\n",
        "                print(f\"Warning: Date {date} not in sample. Skipping IRF.\")\n",
        "                continue\n",
        "\n",
        "            history_to_date = prepared_df.loc[:date]\n",
        "\n",
        "            baseline_path, irfs = simulate_irf(\n",
        "                history_df=history_to_date,\n",
        "                decomposition_results=decomposition_results,\n",
        "                study_params=study_params\n",
        "            )\n",
        "\n",
        "            irf_results[date] = {\n",
        "                \"baseline_path\": baseline_path,\n",
        "                \"irfs\": irfs\n",
        "            }\n",
        "\n",
        "    # --- 6. Assemble Final Results Dictionary---\n",
        "    print(\"\\n--- Analysis Complete ---\")\n",
        "    # This dictionary structure is now robust and self-contained.\n",
        "    # It provides all necessary data objects for any downstream task.\n",
        "    final_results = {\n",
        "        \"inputs\": {\n",
        "            \"study_params\": study_params,\n",
        "            # Store the full raw DataFrame.\n",
        "            \"raw_data\": raw_df,\n",
        "            # Store the full prepared DataFrame.\n",
        "            \"prepared_data\": prepared_df,\n",
        "            # Store the quarterly (but not prepared) DataFrame.\n",
        "            \"quarterly_data\": quarterly_df\n",
        "        },\n",
        "        \"estimation\": {\n",
        "            \"phi_estimated\": phi_estimated,\n",
        "            \"decomposition\": decomposition_results,\n",
        "            \"series_means\": series_means,\n",
        "            \"optimization_result\": opt_result\n",
        "        },\n",
        "        \"diagnostics\": {\n",
        "            \"residuals\": residuals_df\n",
        "        },\n",
        "        \"forecasts\": forecast_results,\n",
        "        \"irf_analysis\": irf_results\n",
        "    }\n",
        "\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "nAMMURdrAdBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Conduct Simulation Study\n",
        "\n",
        "def generate_mixed_var_data(\n",
        "    phi_true: np.ndarray,\n",
        "    n_obs: int,\n",
        "    error_df: int,\n",
        "    burn_in: int = 200\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generates artificial data from a mixed causal-noncausal VAR(p) process.\n",
        "\n",
        "    This function simulates a time series from a known DGP by:\n",
        "    1. Decomposing the VAR dynamics into causal and non-causal states.\n",
        "    2. Generating a long series of i.i.d. t-distributed errors.\n",
        "    3. Simulating the causal state forward in time.\n",
        "    4. Simulating the non-causal state backward in time for stability.\n",
        "    5. Transforming the latent states back to the observable variable space.\n",
        "    6. Removing a burn-in period to ensure the series is stationary.\n",
        "\n",
        "    Args:\n",
        "        phi_true (np.ndarray): The true VAR coefficient matrix [Phi_1, ..., Phi_p].\n",
        "        n_obs (int): The desired number of observations in the final series.\n",
        "        error_df (int): The degrees of freedom for the t-distributed errors.\n",
        "        burn_in (int): The number of observations to generate and discard at\n",
        "                       both ends of the simulation to mitigate initialization effects.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of shape (n_obs, m) containing the\n",
        "                      simulated time series.\n",
        "    \"\"\"\n",
        "    m, mp = phi_true.shape\n",
        "    p_lags = mp // m\n",
        "\n",
        "    # --- 1. Decompose the true dynamics ---\n",
        "    # We need the Jordan decomposition of the true process.\n",
        "    # This requires a dummy study_params dict for the helper function.\n",
        "    sim_study_params = {\"estimation\": {\"var_spec\": {\"p_lags\": p_lags}}}\n",
        "    decomp = compute_jordan_decomposition(phi_true, sim_study_params)\n",
        "    A, J, A_inv = decomp['A'], decomp['J'], decomp['A_inv']\n",
        "    n1, n2 = decomp['n1'], decomp['n2']\n",
        "    J1, J2 = J[:n1, :n1], J[n1:, n1:]\n",
        "\n",
        "    # --- 2. Generate Errors ---\n",
        "    sim_length = n_obs + 2 * burn_in\n",
        "    # Generate standardized t-distributed errors.\n",
        "    # Scale them to have variance = df/(df-2)\n",
        "    scale = np.sqrt((error_df - 2) / error_df) if error_df > 2 else 1.0\n",
        "    errors_eps = t_dist.rvs(df=error_df, size=(sim_length, m), scale=scale)\n",
        "\n",
        "    # Transform to state-space errors: eta = A_inv * [eps', 0, ...]'\n",
        "    eps_stacked = np.hstack([errors_eps, np.zeros((sim_length, m * p_lags - m))])\n",
        "    errors_eta = eps_stacked @ A_inv.T\n",
        "    eta1, eta2 = errors_eta[:, :n1], errors_eta[:, n1:]\n",
        "\n",
        "    # --- 3. & 4. Simulate Latent States ---\n",
        "    # Initialize state vectors.\n",
        "    Z1 = np.zeros((sim_length, n1))\n",
        "    Z2 = np.zeros((sim_length, n2))\n",
        "\n",
        "    # Simulate causal state forward.\n",
        "    for t in range(1, sim_length):\n",
        "        Z1[t, :] = J1 @ Z1[t - 1, :] + eta1[t, :]\n",
        "\n",
        "    # Simulate non-causal state backward for stability.\n",
        "    J2_inv = np.linalg.inv(J2)\n",
        "    for t in range(sim_length - 2, -1, -1):\n",
        "        Z2[t, :] = J2_inv @ (Z2[t + 1, :] - eta2[t + 1, :])\n",
        "\n",
        "    # --- 5. & 6. Combine, Transform, and Finalize ---\n",
        "    Z_path = np.hstack([Z1, Z2])\n",
        "    # Transform back to Y-space: Y_tilde = A * Z\n",
        "    Y_tilde_path = Z_path @ A.T\n",
        "    # The observable Y_t is the first m components of the state vector.\n",
        "    Y_path = Y_tilde_path[:, :m]\n",
        "\n",
        "    # Discard burn-in periods from both ends.\n",
        "    final_Y = Y_path[burn_in : -burn_in]\n",
        "\n",
        "    return pd.DataFrame(final_Y, columns=[f'Y{i+1}' for i in range(m)])\n",
        "\n",
        "def _simulation_single_replication(\n",
        "    dgp_params: Dict[str, Any],\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[bool, bool]:\n",
        "    \"\"\"\n",
        "    Executes a single replication of the Monte Carlo simulation.\n",
        "\n",
        "    Args:\n",
        "        dgp_params (Dict[str, Any]): Parameters for the data generating process.\n",
        "        study_params (Dict[str, Any]): Parameters for the estimation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[bool, bool]: A tuple of booleans indicating if the true value\n",
        "                           was covered by the prediction interval for each of the\n",
        "                           two variables.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Generate data\n",
        "        sim_df = generate_mixed_var_data(\n",
        "            phi_true=dgp_params['phi_true'],\n",
        "            n_obs=dgp_params['n_obs'],\n",
        "            error_df=dgp_params['error_df']\n",
        "        )\n",
        "\n",
        "        # 2. Split into train and test\n",
        "        train_df = sim_df.iloc[:-1]\n",
        "        true_future_val = sim_df.iloc[-1].to_numpy()\n",
        "\n",
        "        # 3. Run estimation and forecasting pipeline\n",
        "        # (Simplified version of run_empirical_analysis)\n",
        "        phi_est, _ = estimate_gcov_var(train_df, study_params)\n",
        "        decomp = compute_jordan_decomposition(phi_est, study_params)\n",
        "        g_hat, l2_hat = estimate_functional_components(\n",
        "            train_df, phi_est, decomp, study_params\n",
        "        )\n",
        "        _, grid_vecs, density = compute_point_forecast(\n",
        "            train_df, phi_est, decomp, g_hat, l2_hat, study_params, train_df.std()\n",
        "        )\n",
        "        pi = compute_prediction_interval(grid_vecs, density, study_params)\n",
        "\n",
        "        # 4. Check coverage\n",
        "        is_covered_y1 = (pi[0, 0] <= true_future_val[0] <= pi[0, 1])\n",
        "        is_covered_y2 = (pi[1, 0] <= true_future_val[1] <= pi[1, 1])\n",
        "\n",
        "        return is_covered_y1, is_covered_y2\n",
        "    except Exception as e:\n",
        "        # If any step fails, this replication is invalid.\n",
        "        # print(f\"Replication failed: {e}\")\n",
        "        return np.nan, np.nan\n",
        "\n",
        "def run_simulation_study(\n",
        "    dgp_configurations: List[Dict[str, Any]],\n",
        "    study_params: Dict[str, Any],\n",
        "    n_replications: int\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Conducts a Monte Carlo simulation study to evaluate the model.\n",
        "\n",
        "    This function systematically assesses the finite-sample properties of the\n",
        "    estimation and forecasting pipeline by running it on many artificially\n",
        "    generated datasets where the true process is known. The primary metric\n",
        "    evaluated is the empirical coverage rate of the prediction intervals.\n",
        "\n",
        "    Args:\n",
        "        dgp_configurations (List[Dict[str, Any]]): A list of dictionaries,\n",
        "            each defining a Data Generating Process (e.g., different sample\n",
        "            sizes, true parameters, or error distributions).\n",
        "        study_params (Dict[str, Any]): The study parameters for the estimation.\n",
        "        n_replications (int): The number of Monte Carlo replications to run\n",
        "                              for each DGP configuration.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame summarizing the simulation results, with\n",
        "                      rows for each DGP and columns for the coverage rates.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Simulation Study ---\")\n",
        "    all_results = []\n",
        "\n",
        "    for i, dgp_params in enumerate(dgp_configurations):\n",
        "        print(f\"\\nRunning DGP Configuration {i+1}/{len(dgp_configurations)}...\")\n",
        "        print(f\"  Parameters: {dgp_params}\")\n",
        "\n",
        "        # Run all replications for this DGP in parallel.\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(_simulation_single_replication)(dgp_params, study_params)\n",
        "            for _ in range(n_replications)\n",
        "        )\n",
        "\n",
        "        # Process the results.\n",
        "        valid_results = [res for res in results if not np.isnan(res[0])]\n",
        "        num_valid = len(valid_results)\n",
        "        if num_valid < n_replications * 0.8:\n",
        "            print(f\"Warning: High failure rate. Only {num_valid}/{n_replications} replications succeeded.\")\n",
        "\n",
        "        if num_valid > 0:\n",
        "            coverage_y1 = np.mean([res[0] for res in valid_results])\n",
        "            coverage_y2 = np.mean([res[1] for res in valid_results])\n",
        "        else:\n",
        "            coverage_y1, coverage_y2 = np.nan, np.nan\n",
        "\n",
        "        # Store summary.\n",
        "        summary = {\n",
        "            'dgp_config_id': i,\n",
        "            'n_obs': dgp_params['n_obs'],\n",
        "            'error_df': dgp_params['error_df'],\n",
        "            'coverage_Y1': coverage_y1,\n",
        "            'coverage_Y2': coverage_y2,\n",
        "            'successful_reps': num_valid\n",
        "        }\n",
        "        all_results.append(summary)\n",
        "        print(f\"  Results: Coverage Y1={coverage_y1:.3f}, Y2={coverage_y2:.3f}\")\n",
        "\n",
        "    print(\"\\n--- Simulation Study Complete ---\")\n",
        "    return pd.DataFrame(all_results)\n"
      ],
      "metadata": {
        "id": "OolWVq_6Bvee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks 14: Robustness Checks\n",
        "\n",
        "def run_robustness_checks(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any],\n",
        "    initial_guesses: Optional[List[np.ndarray]] = None,\n",
        "    bandwidth_multipliers: Optional[List[float]] = None,\n",
        "    forecast_date: Optional[pd.Timestamp] = None\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Conducts robustness checks on the model estimation and forecasting.\n",
        "\n",
        "    This function systematically assesses the sensitivity of the model's key\n",
        "    outputs to two critical specification choices:\n",
        "    1.  **Initialization Robustness**: It re-runs the GCov optimization from\n",
        "        multiple different starting points to check for convergence to a\n",
        "        unique global minimum.\n",
        "    2.  **Bandwidth Sensitivity**: It re-calculates the predictive density and\n",
        "        forecasts using different KDE bandwidths to assess the sensitivity\n",
        "        of forecasts to this non-parametric choice.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The prepared, demeaned time series data.\n",
        "        study_params (Dict[str, Any]): The base dictionary of study parameters.\n",
        "        initial_guesses (Optional[List[np.ndarray]]): A list of alternative\n",
        "            initial parameter vectors (`theta_0`) for the GCov optimizer.\n",
        "        bandwidth_multipliers (Optional[List[float]]): A list of multipliers\n",
        "            (e.g., [0.5, 1.0, 1.5]) to apply to the default KDE bandwidths.\n",
        "        forecast_date (Optional[pd.Timestamp]): The specific date for which\n",
        "            to perform the bandwidth sensitivity analysis on forecasts.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing DataFrames of results\n",
        "                                 for each robustness check performed.\n",
        "    \"\"\"\n",
        "    # --- Setup ---\n",
        "    print(\"--- Starting Robustness Checks ---\")\n",
        "    # Initialize the dictionary to hold all results.\n",
        "    results = {}\n",
        "\n",
        "    # --- 1. Initialization Robustness Check ---\n",
        "    # This check is performed if a list of initial guesses is provided.\n",
        "    if initial_guesses:\n",
        "        print(f\"\\nRunning Initialization Robustness Check with {len(initial_guesses)} starting points...\")\n",
        "        # Initialize a list to store the outcome of each optimization run.\n",
        "        init_results = []\n",
        "\n",
        "        # Iterate through each provided initial guess vector.\n",
        "        for i, guess in enumerate(initial_guesses):\n",
        "            print(f\"  Running optimization with initial guess #{i+1}...\")\n",
        "            try:\n",
        "                # Call the GCov estimator, providing the specific initial guess.\n",
        "                # This bypasses the default OLS-based initialization.\n",
        "                phi_est, opt_res = estimate_gcov_var(\n",
        "                    prepared_df,\n",
        "                    study_params,\n",
        "                    initial_guess=guess\n",
        "                )\n",
        "\n",
        "                # Store a dictionary of key results for this run.\n",
        "                init_results.append({\n",
        "                    \"guess_id\": i,\n",
        "                    \"initial_guess\": guess,\n",
        "                    \"final_phi_flat\": phi_est.ravel(),\n",
        "                    \"final_objective_value\": opt_res.fun,\n",
        "                    \"converged\": opt_res.success,\n",
        "                    \"message\": opt_res.message\n",
        "                })\n",
        "            except Exception as e:\n",
        "                # If any error occurs during optimization, log it as a failure.\n",
        "                print(f\"  Optimization with guess #{i+1} failed: {e}\")\n",
        "                init_results.append({\n",
        "                    \"guess_id\": i,\n",
        "                    \"initial_guess\": guess,\n",
        "                    \"final_phi_flat\": np.nan,\n",
        "                    \"final_objective_value\": np.nan,\n",
        "                    \"converged\": False,\n",
        "                    \"message\": str(e)\n",
        "                })\n",
        "\n",
        "        # Convert the list of results into a pandas DataFrame for easy analysis.\n",
        "        results['initialization_robustness'] = pd.DataFrame(init_results)\n",
        "        print(\"Initialization check complete.\")\n",
        "\n",
        "    # --- 2. Bandwidth Sensitivity Analysis ---\n",
        "    # This check is performed if bandwidth multipliers AND a forecast date are provided.\n",
        "    if bandwidth_multipliers and forecast_date:\n",
        "        print(f\"\\nRunning Bandwidth Sensitivity Check with multipliers: {bandwidth_multipliers}...\")\n",
        "\n",
        "        # a) Perform the main estimation once, as the Phi parameters are held constant.\n",
        "        print(\"  Performing base model estimation...\")\n",
        "        phi_estimated, _ = estimate_gcov_var(prepared_df, study_params)\n",
        "        decomposition_results = compute_jordan_decomposition(phi_estimated, study_params)\n",
        "        data_stdevs = prepared_df.std()\n",
        "        history_to_date = prepared_df.loc[:forecast_date]\n",
        "\n",
        "        # Initialize a list to store the results for each multiplier.\n",
        "        bw_results = []\n",
        "\n",
        "        # Iterate through each specified bandwidth multiplier.\n",
        "        for multiplier in bandwidth_multipliers:\n",
        "            print(f\"  Analyzing bandwidth multiplier: {multiplier}...\")\n",
        "            try:\n",
        "                # b) Re-estimate functional components with the modified bandwidth.\n",
        "                # The `bandwidth_multiplier` argument modifies the internal calculation.\n",
        "                g_hat_sens, l2_hat_sens = estimate_functional_components(\n",
        "                    prepared_df,\n",
        "                    phi_estimated,\n",
        "                    decomposition_results,\n",
        "                    study_params,\n",
        "                    bandwidth_multiplier=multiplier\n",
        "                )\n",
        "\n",
        "                # c) Compute the forecast and interval using the new densities.\n",
        "                point_fc, grid_vecs, density_grid = compute_point_forecast(\n",
        "                    history_df=history_to_date,\n",
        "                    phi_estimated=phi_estimated,\n",
        "                    decomposition_results=decomposition_results,\n",
        "                    g_hat=g_hat_sens,\n",
        "                    l2_hat=l2_hat_sens,\n",
        "                    study_params=study_params,\n",
        "                    data_stdevs=data_stdevs\n",
        "                )\n",
        "\n",
        "                # Use the captured density grid to compute the prediction interval\n",
        "                # without any redundant calculations.\n",
        "                pred_interval = compute_prediction_interval(\n",
        "                    grid_vectors=grid_vecs,\n",
        "                    density_on_grid=density_grid,\n",
        "                    study_params=study_params\n",
        "                )\n",
        "\n",
        "                # d) Store the results for this multiplier.\n",
        "                bw_results.append({\n",
        "                    \"multiplier\": multiplier,\n",
        "                    \"point_forecast_Y1\": point_fc[0],\n",
        "                    \"point_forecast_Y2\": point_fc[1],\n",
        "                    \"pi_lower_Y1\": pred_interval[0, 0],\n",
        "                    \"pi_upper_Y1\": pred_interval[0, 1],\n",
        "                    \"pi_lower_Y2\": pred_interval[1, 0],\n",
        "                    \"pi_upper_Y2\": pred_interval[1, 1]\n",
        "                })\n",
        "            except Exception as e:\n",
        "                # If any step fails for this multiplier, log it.\n",
        "                print(f\"  Bandwidth run with multiplier {multiplier} failed: {e}\")\n",
        "                bw_results.append({\"multiplier\": multiplier})\n",
        "\n",
        "        # Convert the list of results into a DataFrame, using the multiplier as the index.\n",
        "        results['bandwidth_sensitivity'] = pd.DataFrame(bw_results).set_index('multiplier')\n",
        "        print(\"Bandwidth check complete.\")\n",
        "\n",
        "    elif bandwidth_multipliers and not forecast_date:\n",
        "        # Provide a helpful message if parameters are incomplete.\n",
        "        print(\"Warning: `bandwidth_multipliers` provided but no `forecast_date`. Skipping sensitivity check.\")\n",
        "\n",
        "    print(\"\\n--- Robustness Checks Complete ---\")\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "hIBeSylLCIjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Visualization\n",
        "\n",
        "class ModelVisualizer:\n",
        "    \"\"\"\n",
        "    A class for generating publication-quality visualizations from the results\n",
        "    of the mixed causal-noncausal VAR analysis.\n",
        "\n",
        "    This class is designed to interface directly with the output dictionary\n",
        "    from the `run_empirical_analysis` function, providing a suite of methods\n",
        "    to plot key results and diagnostics in a clear and interpretable manner.\n",
        "    \"\"\"\n",
        "    def __init__(self, results: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the visualizer with the results from `run_empirical_analysis`.\n",
        "\n",
        "        This constructor validates the incoming results dictionary and stores\n",
        "        the necessary data components as instance attributes for easy access\n",
        "        by the plotting methods.\n",
        "\n",
        "        Args:\n",
        "            results (Dict[str, Any]): The comprehensive results dictionary.\n",
        "\n",
        "        Raises:\n",
        "            TypeError: If `results` is not a dictionary.\n",
        "            KeyError: If the results dictionary is missing essential keys\n",
        "                      like 'inputs', 'prepared_data', etc.\n",
        "        \"\"\"\n",
        "        # --- Input Validation ---\n",
        "        # Ensure the input is a dictionary.\n",
        "        if not isinstance(results, dict):\n",
        "            raise TypeError(\"`results` must be a dictionary.\")\n",
        "        # Check for the presence of essential top-level keys.\n",
        "        for key in ['inputs', 'diagnostics', 'estimation', 'forecasts', 'irf_analysis']:\n",
        "            if key not in results:\n",
        "                raise KeyError(f\"Results dictionary is missing required key: '{key}'\")\n",
        "        # Check for the presence of essential dataframes within the 'inputs' key.\n",
        "        for data_key in ['quarterly_data', 'prepared_data']:\n",
        "            if data_key not in results['inputs']:\n",
        "                raise KeyError(f\"Results dictionary is missing required data key: 'inputs.{data_key}'\")\n",
        "\n",
        "        # --- Direct Data Assignment (REMEDIED) ---\n",
        "        # Directly assign the full DataFrames from the results dictionary.\n",
        "        # This is robust and eliminates the fragile 'combine_first' workaround.\n",
        "        self.results = results\n",
        "        self.quarterly_df = results['inputs']['quarterly_data']\n",
        "        self.prepared_df = results['inputs']['prepared_data']\n",
        "\n",
        "        # Set a professional plot style for all methods in this class.\n",
        "        sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "    def plot_diagnostics(self):\n",
        "        \"\"\"\n",
        "        Generates diagnostic plots for the estimated model, including data\n",
        "        transformations and residual analysis.\n",
        "        \"\"\"\n",
        "        print(\"Generating diagnostic plots...\")\n",
        "\n",
        "        # --- 1. Data Transformation Plots (REMEDIED) ---\n",
        "        # This plot now accurately shows the \"before\" and \"after\" of the\n",
        "        # final preparation step (demeaning).\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
        "        fig.suptitle(\"Data Transformation Stages\", fontsize=16)\n",
        "\n",
        "        # Plot the quarterly data (post-resampling, pre-transformation).\n",
        "        # We reconstruct the pre-demeaned data for a clear visualization.\n",
        "        pre_demeaned_df = self.prepared_df + self.results['estimation']['series_means']\n",
        "        pre_demeaned_df.plot(\n",
        "            ax=axes[0],\n",
        "            title=\"Stage 1: Prepared Data (Quarterly, Scaled, Growth Rates)\"\n",
        "        )\n",
        "        axes[0].legend()\n",
        "        axes[0].set_ylabel(\"Value\")\n",
        "\n",
        "        # Plot the final, demeaned data used in the analysis.\n",
        "        self.prepared_df.plot(\n",
        "            ax=axes[1],\n",
        "            title=\"Stage 2: Final Analysis Data (Demeaned)\"\n",
        "        )\n",
        "        axes[1].legend()\n",
        "        axes[1].set_xlabel(\"Date\")\n",
        "        axes[1].set_ylabel(\"Value\")\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "        # --- 2. Residual Analysis Plots ---\n",
        "        residuals_df = self.results['diagnostics']['residuals']\n",
        "        m = residuals_df.shape[1]\n",
        "\n",
        "        # Create a figure with a grid of subplots for each variable's diagnostics.\n",
        "        fig, axes = plt.subplots(m, 3, figsize=(18, 4 * m), squeeze=False)\n",
        "        fig.suptitle(\"Residual Diagnostics\", fontsize=16)\n",
        "\n",
        "        for i, col in enumerate(residuals_df.columns):\n",
        "            # Plot the time series of residuals.\n",
        "            residuals_df[col].plot(ax=axes[i, 0], title=f\"Time Series of {col}\")\n",
        "            axes[i, 0].set_xlabel(\"Date\")\n",
        "            axes[i, 0].set_ylabel(\"Value\")\n",
        "\n",
        "            # Plot the kernel density estimate of the residuals.\n",
        "            sns.kdeplot(residuals_df[col], ax=axes[i, 1], fill=True)\n",
        "            axes[i, 1].set_title(f\"Density of {col}\")\n",
        "            axes[i, 1].set_xlabel(\"Value\")\n",
        "\n",
        "            # Plot the Autocorrelation Function (ACF) of the residuals.\n",
        "            plot_acf(residuals_df[col], ax=axes[i, 2], lags=20, title=f\"ACF of {col}\")\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "    def plot_predictive_density(self, forecast_date: pd.Timestamp):\n",
        "        \"\"\"\n",
        "        Generates 2D contour and 3D surface plots of the predictive density\n",
        "        for a specific forecast date.\n",
        "        \"\"\"\n",
        "        print(f\"Generating predictive density plots for {forecast_date.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "        # Safely access the forecast results for the given date.\n",
        "        fc_results = self.results['forecasts'].get(forecast_date)\n",
        "        if not fc_results:\n",
        "            print(f\"Error: No forecast results found for date {forecast_date}.\")\n",
        "            return\n",
        "\n",
        "        # Unpack the necessary components for plotting.\n",
        "        grid_vecs = fc_results['grid_vectors']\n",
        "        density = fc_results['density_on_grid']\n",
        "        point_fc = fc_results['point_forecast']\n",
        "        var_names = self.prepared_df.columns\n",
        "\n",
        "        # --- 2D Contour Plot ---\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        contour = ax.contourf(grid_vecs[0], grid_vecs[1], density.T, levels=20, cmap='viridis')\n",
        "        ax.plot(point_fc[0], point_fc[1], 'rX', markersize=12, markeredgecolor='k', label='Point Forecast (Mode)')\n",
        "        fig.colorbar(contour, ax=ax, label='Density')\n",
        "        ax.set_title(f\"Predictive Density Contour Plot ({forecast_date.strftime('%Y-%m-%d')})\", fontsize=16)\n",
        "        ax.set_xlabel(var_names[0])\n",
        "        ax.set_ylabel(var_names[1])\n",
        "        ax.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # --- 3D Surface Plot ---\n",
        "        fig = plt.figure(figsize=(12, 9))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        X, Y = np.meshgrid(grid_vecs[0], grid_vecs[1])\n",
        "        ax.plot_surface(X, Y, density.T, cmap='viridis', edgecolor='none', rstride=5, cstride=5)\n",
        "        ax.set_title(f\"Predictive Density 3D Surface ({forecast_date.strftime('%Y-%m-%d')})\", fontsize=16)\n",
        "        ax.set_xlabel(var_names[0])\n",
        "        ax.set_ylabel(var_names[1])\n",
        "        ax.set_zlabel(\"Density\")\n",
        "        ax.view_init(elev=30, azim=-60) # Set a good viewing angle.\n",
        "        plt.show()\n",
        "\n",
        "    def plot_irf(self, irf_date: pd.Timestamp):\n",
        "        \"\"\"\n",
        "        Generates plots of the state-dependent Impulse Response Functions.\n",
        "        \"\"\"\n",
        "        print(f\"Generating IRF plots for initial state at {irf_date.strftime('%Y-%m-%d')}...\")\n",
        "\n",
        "        # Safely access the IRF results for the given date.\n",
        "        irf_results = self.results['irf_analysis'].get(irf_date)\n",
        "        if not irf_results:\n",
        "            print(f\"Error: No IRF results found for date {irf_date}.\")\n",
        "            return\n",
        "\n",
        "        # Unpack the baseline and IRF dictionary.\n",
        "        baseline = irf_results['baseline_path']\n",
        "        irfs = irf_results['irfs']\n",
        "        m = baseline.shape[1]\n",
        "        var_names = baseline.columns\n",
        "\n",
        "        # Create a grid of subplots, one for each variable.\n",
        "        fig, axes = plt.subplots(m, 1, figsize=(12, 6 * m), sharex=True, squeeze=False)\n",
        "        fig.suptitle(f\"State-Dependent IRFs (Initial State: {irf_date.strftime('%Y-%m-%d')})\", fontsize=16)\n",
        "\n",
        "        # Define consistent styling for different shocks.\n",
        "        colors = {-2.0: 'darkblue', -1.0: 'green', 1.0: 'red', 2.0: 'cyan'}\n",
        "        styles = {-2.0: ':', -1.0: '--', 1.0: '--', 2.0: ':'}\n",
        "\n",
        "        for i in range(m):\n",
        "            ax = axes[i, 0]\n",
        "            # Plot the average baseline path.\n",
        "            ax.plot(baseline.index, baseline.iloc[:, i], 'k-', label='Baseline', linewidth=2.5)\n",
        "\n",
        "            # Plot the path for each shock scenario.\n",
        "            for shock_val, irf_df in irfs.items():\n",
        "                # The shocked path is the baseline plus the impulse response.\n",
        "                shocked_path = baseline.iloc[:, i] + irf_df.iloc[:, i]\n",
        "                ax.plot(\n",
        "                    shocked_path.index,\n",
        "                    shocked_path,\n",
        "                    label=f'Shock = {shock_val}',\n",
        "                    color=colors.get(shock_val, 'gray'),\n",
        "                    linestyle=styles.get(shock_val, '-')\n",
        "                )\n",
        "\n",
        "            ax.set_title(f\"Response of {var_names[i]}\")\n",
        "            ax.set_ylabel(\"Value\")\n",
        "            ax.legend()\n",
        "            ax.axhline(0, color='grey', linestyle='--', linewidth=0.8)\n",
        "\n",
        "        axes[-1, 0].set_xlabel(\"Horizon (Quarters)\")\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "dWv8UOIyGK4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Conduct Comparative Analysis\n",
        "\n",
        "def _estimate_and_forecast_linear_var(\n",
        "    train_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimates a standard linear VAR by OLS and computes a one-step-ahead\n",
        "    forecast with a standard prediction interval.\n",
        "\n",
        "    Args:\n",
        "        train_df (pd.DataFrame): The training data for estimation.\n",
        "        study_params (Dict[str, Any]): The study parameters.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[np.ndarray, np.ndarray]:\n",
        "        - `point_forecast`: The linear point forecast, shape (m,).\n",
        "        - `prediction_interval`: The symmetric prediction interval, shape (m, 2).\n",
        "    \"\"\"\n",
        "    p_lags = study_params[\"estimation\"][\"var_spec\"][\"p_lags\"]\n",
        "    alpha = study_params[\"forecasting\"][\"intervals\"][\"prediction_interval_alpha\"]\n",
        "    data_np = train_df.to_numpy()\n",
        "    m = data_np.shape[1]\n",
        "\n",
        "    # 1. Estimate Phi via OLS\n",
        "    phi_ols_flat = _estimate_ols_var(data_np, p_lags)\n",
        "    phi_ols = phi_ols_flat.reshape((m, m * p_lags))\n",
        "\n",
        "    # 2. Compute Point Forecast\n",
        "    Y_T_hist = data_np[-p_lags:, :]\n",
        "    X_T = Y_T_hist[::-1, :].ravel().reshape(1, -1)\n",
        "    point_forecast = (X_T @ phi_ols.T).flatten()\n",
        "\n",
        "    # 3. Compute Prediction Interval\n",
        "    Y, X = _create_var_design_matrix(data_np, p_lags)\n",
        "    residuals = Y - X @ phi_ols.T\n",
        "    # Forecast error variance is estimated by the residual variance\n",
        "    mse = np.var(residuals, axis=0)\n",
        "\n",
        "    # Interval is point forecast +/- z-score * sqrt(MSE)\n",
        "    z_score = norm.ppf(1.0 - alpha / 2.0)\n",
        "    half_width = z_score * np.sqrt(mse)\n",
        "\n",
        "    prediction_interval = np.array([\n",
        "        point_forecast - half_width,\n",
        "        point_forecast + half_width\n",
        "    ]).T\n",
        "\n",
        "    return point_forecast, prediction_interval\n",
        "\n",
        "def _run_one_oos_step(\n",
        "    oos_step_index: int,\n",
        "    full_data_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes one step of the out-of-sample forecasting experiment.\n",
        "    This function is the target for parallel execution.\n",
        "    \"\"\"\n",
        "    # 1. Split data for this step\n",
        "    train_df = full_data_df.iloc[:oos_step_index]\n",
        "    actual_value = full_data_df.iloc[oos_step_index].to_numpy()\n",
        "\n",
        "    try:\n",
        "        # 2. Run the Mixed VAR pipeline\n",
        "        phi_mixed, _ = estimate_gcov_var(train_df, study_params)\n",
        "        decomp_mixed = compute_jordan_decomposition(phi_mixed, study_params)\n",
        "        g_hat, l2_hat = estimate_functional_components(\n",
        "            train_df, phi_mixed, decomp_mixed, study_params\n",
        "        )\n",
        "        fc_mixed, grid_v, density_g = compute_point_forecast(\n",
        "            train_df, phi_mixed, decomp_mixed, g_hat, l2_hat, study_params, train_df.std()\n",
        "        )\n",
        "        pi_mixed = compute_prediction_interval(grid_v, density_g, study_params)\n",
        "    except Exception:\n",
        "        fc_mixed, pi_mixed = (np.full(2, np.nan), np.full((2, 2), np.nan))\n",
        "\n",
        "    try:\n",
        "        # 3. Run the Linear VAR pipeline\n",
        "        fc_linear, pi_linear = _estimate_and_forecast_linear_var(train_df, study_params)\n",
        "    except Exception:\n",
        "        fc_linear, pi_linear = (np.full(2, np.nan), np.full((2, 2), np.nan))\n",
        "\n",
        "    # 4. Package results for this time step\n",
        "    return {\n",
        "        'date': full_data_df.index[oos_step_index],\n",
        "        'actual': actual_value,\n",
        "        'fc_mixed': fc_mixed,\n",
        "        'pi_mixed': pi_mixed,\n",
        "        'fc_linear': fc_linear,\n",
        "        'pi_linear': pi_linear\n",
        "    }\n",
        "\n",
        "def run_comparative_analysis(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any],\n",
        "    oos_start_index: int\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Conducts a comparative out-of-sample (OOS) forecasting analysis.\n",
        "\n",
        "    This function performs a \"horse race\" between the mixed causal-noncausal\n",
        "    VAR model and a standard linear VAR benchmark. It uses an expanding\n",
        "    window approach: for each step in the OOS period, it re-estimates both\n",
        "    models on all available history and generates a one-step-ahead forecast.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The full prepared, demeaned time series.\n",
        "        study_params (Dict[str, Any]): The study parameters.\n",
        "        oos_start_index (int): The index location to start the OOS experiment.\n",
        "                               All data prior is used for the first estimation.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing:\n",
        "        - 'forecasts_df': A DataFrame with the time series of actuals,\n",
        "          forecasts, and interval bounds for both models.\n",
        "        - 'metrics_df': A DataFrame summarizing performance metrics (MSFE,\n",
        "          MAE, Coverage) for both models.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Comparative OOS Analysis ---\")\n",
        "\n",
        "    # --- 1. Run OOS Loop in Parallel ---\n",
        "    oos_indices = range(oos_start_index, len(prepared_df))\n",
        "    print(f\"Running OOS experiment for {len(oos_indices)} steps...\")\n",
        "\n",
        "    results_list = Parallel(n_jobs=-1)(\n",
        "        delayed(_run_one_oos_step)(i, prepared_df, study_params)\n",
        "        for i in oos_indices\n",
        "    )\n",
        "\n",
        "    # --- 2. Process and Structure Results ---\n",
        "    print(\"Processing OOS results...\")\n",
        "    # Unpack the list of dictionaries into a structured DataFrame\n",
        "    var_names = prepared_df.columns\n",
        "    data_to_plot = {\n",
        "        'date': [r['date'] for r in results_list],\n",
        "        f'actual_{var_names[0]}': [r['actual'][0] for r in results_list],\n",
        "        f'actual_{var_names[1]}': [r['actual'][1] for r in results_list],\n",
        "        f'fc_mixed_{var_names[0]}': [r['fc_mixed'][0] for r in results_list],\n",
        "        f'fc_mixed_{var_names[1]}': [r['fc_mixed'][1] for r in results_list],\n",
        "        f'pi_mixed_lower_{var_names[0]}': [r['pi_mixed'][0, 0] for r in results_list],\n",
        "        f'pi_mixed_upper_{var_names[0]}': [r['pi_mixed'][0, 1] for r in results_list],\n",
        "        f'pi_mixed_lower_{var_names[1]}': [r['pi_mixed'][1, 0] for r in results_list],\n",
        "        f'pi_mixed_upper_{var_names[1]}': [r['pi_mixed'][1, 1] for r in results_list],\n",
        "        f'fc_linear_{var_names[0]}': [r['fc_linear'][0] for r in results_list],\n",
        "        f'fc_linear_{var_names[1]}': [r['fc_linear'][1] for r in results_list],\n",
        "        f'pi_linear_lower_{var_names[0]}': [r['pi_linear'][0, 0] for r in results_list],\n",
        "        f'pi_linear_upper_{var_names[0]}': [r['pi_linear'][0, 1] for r in results_list],\n",
        "        f'pi_linear_lower_{var_names[1]}': [r['pi_linear'][1, 0] for r in results_list],\n",
        "        f'pi_linear_upper_{var_names[1]}': [r['pi_linear'][1, 1] for r in results_list],\n",
        "    }\n",
        "    forecasts_df = pd.DataFrame(data_to_plot).set_index('date').dropna()\n",
        "\n",
        "    # --- 3. Compute Performance Metrics ---\n",
        "    print(\"Computing performance metrics...\")\n",
        "    metrics = []\n",
        "    for model in ['mixed', 'linear']:\n",
        "        for i, var in enumerate(var_names):\n",
        "            actuals = forecasts_df[f'actual_{var}']\n",
        "            forecasts = forecasts_df[f'fc_{model}_{var}']\n",
        "\n",
        "            # Calculate MSFE and MAE\n",
        "            errors = actuals - forecasts\n",
        "            msfe = np.mean(errors**2)\n",
        "            mae = np.mean(np.abs(errors))\n",
        "\n",
        "            # Calculate Interval Coverage\n",
        "            lower = forecasts_df[f'pi_{model}_lower_{var}']\n",
        "            upper = forecasts_df[f'pi_{model}_upper_{var}']\n",
        "            is_covered = (actuals >= lower) & (actuals <= upper)\n",
        "            coverage = np.mean(is_covered)\n",
        "\n",
        "            metrics.append({\n",
        "                'model': model,\n",
        "                'variable': var,\n",
        "                'MSFE': msfe,\n",
        "                'MAE': mae,\n",
        "                'Coverage': coverage\n",
        "            })\n",
        "\n",
        "    metrics_df = pd.DataFrame(metrics).set_index(['model', 'variable'])\n",
        "\n",
        "    print(\"--- Comparative Analysis Complete ---\")\n",
        "    return {\n",
        "        'forecasts_df': forecasts_df,\n",
        "        'metrics_df': metrics_df\n",
        "    }\n"
      ],
      "metadata": {
        "id": "MeYTtve6JXnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pipeline Function\n",
        "\n",
        "def run_full_research_pipeline(\n",
        "    raw_df: pd.DataFrame,\n",
        "    study_params: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes a complete, end-to-end research pipeline for the mixed\n",
        "    causal-noncausal VAR model.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point for\n",
        "    the entire analytical system. Based on the configuration provided in\n",
        "    `study_params`, it can selectively run any or all of the following major\n",
        "    analytical tasks:\n",
        "\n",
        "    1.  **Empirical Analysis**: The core estimation and forecasting on the\n",
        "        provided data, including state-dependent IRFs.\n",
        "    2.  **Simulation Study**: A Monte Carlo experiment to assess the\n",
        "        finite-sample properties of the estimators.\n",
        "    3.  **Robustness Checks**: Sensitivity analysis with respect to optimizer\n",
        "        initial values and KDE bandwidths.\n",
        "    4.  **Comparative Analysis**: An out-of-sample forecasting horse race\n",
        "        against a standard linear VAR benchmark.\n",
        "\n",
        "    All results are collected and returned in a single, comprehensive, and\n",
        "    well-structured dictionary.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame): The raw input DataFrame with a monthly\n",
        "                               DatetimeIndex and two columns: \"real_oil_price\"\n",
        "                               and \"real_gdp\".\n",
        "        study_params (Dict[str, Any]): A comprehensive, nested dictionary\n",
        "            containing all parameters and control flags for the desired analyses.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A master dictionary containing the results of all\n",
        "                        analyses that were executed.\n",
        "    \"\"\"\n",
        "    # --- Master Results Dictionary ---\n",
        "    # Initialize the dictionary that will store all outputs.\n",
        "    pipeline_results: Dict[str, Any] = {\n",
        "        \"pipeline_configuration\": study_params\n",
        "    }\n",
        "    print(\"--- Starting Full Research Pipeline ---\")\n",
        "\n",
        "    # --- 1. Core Empirical Analysis (Task 12 & 15) ---\n",
        "    # Check the control flag in the parameters to see if this step should run.\n",
        "    empirical_config = study_params.get(\"run_empirical\", {})\n",
        "    if empirical_config.get(\"enabled\", False):\n",
        "        print(\"\\n>>> EXECUTING: Core Empirical Analysis <<<\")\n",
        "        # Execute the main analysis pipeline.\n",
        "        empirical_results = run_empirical_analysis(\n",
        "            raw_df=raw_df,\n",
        "            study_params=study_params,\n",
        "            forecast_dates=empirical_config.get(\"forecast_dates\"),\n",
        "            irf_dates=empirical_config.get(\"irf_dates\")\n",
        "        )\n",
        "        # Store the detailed results.\n",
        "        pipeline_results[\"empirical_analysis\"] = empirical_results\n",
        "\n",
        "        # Automatically generate and display visualizations for the results.\n",
        "        visualizer = ModelVisualizer(empirical_results)\n",
        "        visualizer.plot_diagnostics()\n",
        "        if empirical_config.get(\"forecast_dates\"):\n",
        "            for date in empirical_config[\"forecast_dates\"]:\n",
        "                visualizer.plot_predictive_density(date)\n",
        "        if empirical_config.get(\"irf_dates\"):\n",
        "            for date in empirical_config[\"irf_dates\"]:\n",
        "                visualizer.plot_irf(date)\n",
        "        print(\"--- Empirical Analysis Complete ---\")\n",
        "\n",
        "    # --- 2. Simulation Study (Task 13) ---\n",
        "    # Check the control flag for the simulation study.\n",
        "    sim_config = study_params.get(\"run_simulation\", {})\n",
        "    if sim_config.get(\"enabled\", False):\n",
        "        print(\"\\n>>> EXECUTING: Simulation Study <<<\")\n",
        "        # Run the Monte Carlo simulation study.\n",
        "        simulation_summary_df = run_simulation_study(\n",
        "            dgp_configurations=sim_config[\"dgp_configurations\"],\n",
        "            study_params=study_params,\n",
        "            n_replications=sim_config[\"n_replications\"]\n",
        "        )\n",
        "        # Store the summary DataFrame.\n",
        "        pipeline_results[\"simulation_study\"] = simulation_summary_df\n",
        "        print(\"--- Simulation Study Complete ---\")\n",
        "\n",
        "    # --- 3. Robustness Checks (Task 14) ---\n",
        "    # Check the control flag for robustness checks.\n",
        "    robust_config = study_params.get(\"run_robustness\", {})\n",
        "    if robust_config.get(\"enabled\", False):\n",
        "        print(\"\\n>>> EXECUTING: Robustness Checks <<<\")\n",
        "        # The robustness checks need the prepared data from the empirical run.\n",
        "        # We ensure the empirical analysis has been run or run a prep step.\n",
        "        if \"empirical_analysis\" in pipeline_results:\n",
        "            prepared_df = pipeline_results[\"empirical_analysis\"][\"inputs\"][\"prepared_data\"]\n",
        "        else:\n",
        "            # Run data prep steps if not already done.\n",
        "            quarterly_df = validate_and_cleanse_data(raw_df, study_params)\n",
        "            prepared_df, _ = prepare_var_data(quarterly_df)\n",
        "\n",
        "        robustness_results = run_robustness_checks(\n",
        "            prepared_df=prepared_df,\n",
        "            study_params=study_params,\n",
        "            initial_guesses=robust_config.get(\"initial_guesses\"),\n",
        "            bandwidth_multipliers=robust_config.get(\"bandwidth_multipliers\"),\n",
        "            forecast_date=robust_config.get(\"forecast_date\")\n",
        "        )\n",
        "        # Store the results dictionary.\n",
        "        pipeline_results[\"robustness_checks\"] = robustness_results\n",
        "        print(\"--- Robustness Checks Complete ---\")\n",
        "\n",
        "    # --- 4. Comparative Analysis (Task 16) ---\n",
        "    # Check the control flag for the comparative analysis.\n",
        "    comp_config = study_params.get(\"run_comparison\", {})\n",
        "    if comp_config.get(\"enabled\", False):\n",
        "        print(\"\\n>>> EXECUTING: Comparative Analysis <<<\")\n",
        "        # This also needs the prepared data.\n",
        "        if \"empirical_analysis\" in pipeline_results:\n",
        "            prepared_df = pipeline_results[\"empirical_analysis\"][\"inputs\"][\"prepared_data\"]\n",
        "        else:\n",
        "            quarterly_df = validate_and_cleanse_data(raw_df, study_params)\n",
        "            prepared_df, _ = prepare_var_data(quarterly_df)\n",
        "\n",
        "        comparison_results = run_comparative_analysis(\n",
        "            prepared_df=prepared_df,\n",
        "            study_params=study_params,\n",
        "            oos_start_index=comp_config[\"oos_start_index\"]\n",
        "        )\n",
        "        # Store the results dictionary.\n",
        "        pipeline_results[\"comparative_analysis\"] = comparison_results\n",
        "        print(\"--- Comparative Analysis Complete ---\")\n",
        "\n",
        "    print(\"\\n--- Full Research Pipeline Finished ---\")\n",
        "    return pipeline_results\n"
      ],
      "metadata": {
        "id": "kJ75EKYaT2Cb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}